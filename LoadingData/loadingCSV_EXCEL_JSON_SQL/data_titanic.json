[
 {
  "repo": "icons8/titanic",
  "language": "JavaScript",
  "readme_contents": "# Titanic\n\nA collection of animated icons + javascript library.\n\n![Preview](/docs/images/animated-icons-preview.gif)\n\n**[Preview all icons](https://rawgit.com/icons8/titanic/master/demo/index.html)**\n\n## Installation\n\nYou can install it either via CDN or npm.\n\n### CDN\n\nInsert this string to your HTML head:\n```html\n<script src=\"https://cdn.rawgit.com/icons8/titanic/master/dist/js/titanic.min.js\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/bodymovin/4.5.9/bodymovin.min.js\"></script>\n```\nAnd initialize it before the body closes:\n\n```html\n<script>\n    var titanic = new Titanic();\n</script>\n```\nThis way, you can add icons anywhere in your HTML using this tag:\n```html\n<div class='titanic titanic-chat'></div>\n```\nWhere chat can be any of these:\n* caps\n* chat\n* checkbox\n* expand\n* cheap\n* expensive\n* idea\n* mailbox\n* mic\n* no-mic\n* online\n* pause\n* power\n* shopping\n* smile\n* stop\n* unlock\n* zoom\n\n### Hosting your images\n\nIf you'd like to host your images on your server instead of rawgit, pass the base URL with the init() function:\n```javascript\ntitanic.begin('/my/base/directory/');\n```\nThen, if you have a div with id=\"chat\", Titanic will search for icons in /my/base/directory/chat.json.\n\n## npm\n\n```\nnpm install titanic-icons --save\n```\n\n## API\n\n* ```titanic.isInitialized()``` -- just true/false flag\n\n* ```titanic.items``` -- list of titanic items\n* ```titanic.items[index].on(), titanic.items[index].off(), titanic.items[index].play()``` -- play animations of the titanic item by index\n* ```titanic.on(token), titanic.off(token), titanic.play(token)``` -- play animations of the titanic item by token (name)\n\n## Example\n\n```html\n<head>\n    <!--Inserting the scripts once for the whole page-->\n    <script src=\"https://cdn.rawgit.com/icons8/titanic/master/dist/js/titanic.min.js\"></script>\n    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/bodymovin/4.5.9/bodymovin.min.js\"></script>\n</head>\n<body>\n    <!--Inserting an icon-->\n    <div class='titanic titanic-checkbox'></div>\n\n    <!--Initializing-->\n    <script>\n        var titanic = new Titanic({\n          hover: true, // auto animated on hover (default true)\n          click: true  // auto animated on click/tap (default false)\n        });\n    </script>\n\n    <!--Clicking turns this icon on-->\n    <button onclick=\"titanic.on(getElementById('checkbox').value)\">On</button>\n</body>\n```\n\n## Credits\nJavaScript is basically [bodymovin](https://github.com/bodymovin/bodymovin) plus few lines of my code. It's a solid library with an awesome name. Thank you, guys.\n\nIcons are created by [Margarita Ivanchikova](https://dribbble.com/imargarita) from [Icons8](https://icons8.com/). She has many more awesome animations in her portfolio.\n\nThe code rewritten by [Denis Alexanov](https://github.com/dhilt), my teacher and guru. Thank you!\n\nProject is produced by Icons8, author of the famous icon library, [IconPharm](https://iconpharm.com), and [Sleek Logos](https://sleeklogos.design).\n\nThe code is created by Icons8 \n\n![Magritte](/docs/images/magritte.gif)\n"
 },
 {
  "repo": "romainpiel/Titanic",
  "language": "Java",
  "readme_contents": "# Titanic for Android\n\nThis library is DEPRECATED, as I don't have time to mainatin it anymore. But feel free to go through the code and copy that into your project, it still does its job.\n\nTitanic is an Android experiment reproducing [this effect](http://codepen.io/lbebber/pen/xrwja).\n\n![ScreenShot](titanic.gif)\n\n## How to use\n\nAdd a `TitanicTextView` to your layout:\n```xml\n<com.romainpiel.titanic.TitanicTextView\n    android:id=\"@+id/titanic_tv\"\n    android:text=\"@string/loading\"\n    android:layout_width=\"wrap_content\"\n    android:layout_height=\"wrap_content\"\n    android:textColor=\"#212121\"\n    android:textSize=\"70sp\"/>\n```\n\nTo start the animation:\n```java\ntitanic = new Titanic();\ntitanic.start(myTitanicTextView);\n```\n\nYou may want to keep track of the titanic instance after the animation is started if you want to stop it.\n\nTo stop it:\n```java\ntitanic.cancel();\n```\n\n## How does it work?\n\n### Quick version\n\nTitanic is a simple illusion obtained by applying an animated translation on the TextView TextPaint Shader's matrix.\n\n### Less quick version\n\n#### What is a Shader?\n\nA [Shader](http://developer.android.com/reference/android/graphics/Shader.html) is a class defining spans of colors. It is installed in a [Paint](http://developer.android.com/reference/android/graphics/Paint.html). It's usually following a certain strategy, so you have LinearGradient shaders, RadialGradient shaders BitmapShader shaders, etc...\n\nShader attributes:\n- tile mode: how the shader color spans should be repeated on the x and y axis.\n- local matrix: can be used to apply transformations on the shader\n\n#### Why are you bugging me with these notions?\n\nWell because it is exaclty what we are using in this experiment.\n\nIn [`TitanicTextView`](https://github.com/RomainPiel/Titanic/blob/master/library/src/main/java/com/romainpiel/titanic/library/TitanicTextView.java), we create a BitmapShader containing a wave bitmap.\n\nWe set the tile mode to:\n- x: `TileMode.REPEAT`. The bitmap is repeated on the x-axis\n- y: `Tilemode.CLAMP`. The edge colors are repeated outside the bitmap on the y-axis\n\nWe have a `maskX` and a `maskY` variable that will define the position of the shader. So at every `onDraw()` we will take in account these values and translate the shader's local matrix at the right position.\n\nWe also have a variable `offsetY` to make the value maskY usable. So when maskY is equal to 0, the wave is at the center of the view.\n\n#### How is it animating?\n\nThe animation is based on Android Animator API. I am not going to go through that part. Go read [the documentation](http://developer.android.com/guide/topics/graphics/prop-animation.html) if you need some explanations.\n\nIn this experiment there are 2 animations.\n- One is moving the wave horizontally from 0 to 200 (the width of the wave bitmap).\n- The second one is moving the wave vertically from the bottom half to the top half.\n\nTo animate these translations, all we need is to apply an animator on `maskX` and `maskY`. The position of the shader's matrix will be updated automatically in `onDraw()`.\n\n#### I want more examples\n\nGlad you said that. Go check out [Shimmer-android](https://github.com/RomainPiel/Shimmer-android). It's based on the same concept with a `LinearGradient` shader.\n\n## Sample\n\nSee the [sample](https://github.com/RomainPiel/Titanic/tree/master/sample) for a common use of this library.\n\n## License\n```\nCopyright 2014 Romain Piel\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n"
 },
 {
  "repo": "agconti/kaggle-titanic",
  "language": "Jupyter Notebook",
  "readme_contents": "### Kaggle-titanic\nThis is a tutorial in an IPython Notebook for the Kaggle competition, Titanic Machine Learning From Disaster. The goal of this repository is to provide an example of a competitive analysis for those interested in getting into the field of data analytics or using python for Kaggle's Data Science competitions .\n\n**Quick Start:** [View](http://nbviewer.ipython.org/urls/raw.github.com/agconti/kaggle-titanic/master/Titanic.ipynb) a static version of the notebook in the comfort of your own web browser.\n\n### Installation:\n\nTo run this notebook interactively:\n\n1. Download this repository in a zip file by clicking on this [link](https://github.com/agconti/kaggle-titanic/archive/master.zip) or execute this from the terminal:\n`git clone https://github.com/agconti/kaggle-titanic.git`\n2. Install [virtualenv](http://virtualenv.readthedocs.org/en/latest/installation.html).\n3. Navigate to the directory where you unzipped or cloned the repo and create a virtual environment with `virtualenv env`.\n4. Activate the environment with `source env/bin/activate`\n5. Install the required dependencies with `pip install -r requirements.txt`.\n6. Execute `ipython notebook` from the command line or terminal.\n7. Click on `Titanic.ipynb` on the IPython Notebook dasboard and enjoy!\n8. When you're done deactivate the virtual environment with `deactivate`.\n\n\n#### Dependencies:\n* [NumPy](http://www.numpy.org/)\n* [IPython](http://ipython.org/)\n* [Pandas](http://pandas.pydata.org/)\n* [SciKit-Learn](http://scikit-learn.org/stable/)\n* [SciPy](http://www.scipy.org/)\n* [StatsModels](http://statsmodels.sourceforge.net/)\n* [Patsy](http://patsy.readthedocs.org/en/latest/)\n* [Matplotlib](http://matplotlib.org/)\n\n\n### Kaggle Competition | Titanic Machine Learning from Disaster\n\n>The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew.  This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\n>One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.  Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\n>In this contest, we ask you to complete the analysis of what sorts of people were likely to survive.  In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n\n>This Kaggle Getting Started Competition provides an ideal starting place for people who may not have a lot of experience in data science and machine learning.\"\n\nFrom the competition [homepage](http://www.kaggle.com/c/titanic-gettingStarted).\n\n### Goal for this Notebook:\nShow a simple example of an analysis of the Titanic disaster in Python using a full complement of PyData utilities. This is aimed for those looking to get into the field or those who are already in the field and looking to see an example of an analysis done with Python.\n\n#### This Notebook will show basic examples of:\n#### Data Handling\n*   Importing Data with Pandas\n*   Cleaning Data\n*   Exploring Data through Visualizations with Matplotlib\n\n#### Data Analysis\n*    Supervised Machine learning Techniques:\n    +   Logit Regression Model\n    +   Plotting results\n    +   Support Vector Machine (SVM) using 3 kernels\n    +   Basic Random Forest\n    +   Plotting results\n\n#### Valuation of the Analysis\n*   K-folds cross validation to valuate results locally\n*   Output the results from the IPython Notebook to Kaggle\n\n\n### Benchmark Scripts\nTo find the basic scripts for the competition benchmarks look in the \"Python Examples\" folder. These scripts are based on the originals provided by Astro Dave but have been reworked so that they are easier to understand for new comers.\n\nCompetition Website: http://www.kaggle.com/c/titanic-gettingStarted\n"
 },
 {
  "repo": "HanXiaoyang/Kaggle_Titanic",
  "language": "Jupyter Notebook",
  "readme_contents": "# Kaggle_Titanic\nthe data and ipython notebook of my attempt to solve the kaggle titanic problem\n\n\u6211\u81ea\u5df1\u5b9e\u9a8cKaggle\u4e0a\u7684[Titanic\u95ee\u9898](https://www.kaggle.com/c/titanic)\u7684ipython notebook\n\ntrain.csv\u548ctest.csv\u4e3a\u4f7f\u7528\u5230\u7684\u7684\u6570\u636e\n\n\u5bf9\u4e8e**\u8be5\u95ee\u9898\u7684\u8be6\u7ec6\u8bb2\u89e3**\u548c**\u673a\u5668\u5b66\u4e60\u89e3\u51b3\u95ee\u9898\u7684\u4e00\u822c\u601d\u8def**\u8bf7\u6233[\u5bd2\u5c0f\u9633\u7684\u535a\u5ba2](http://blog.csdn.net/han_xiaoyang/article/details/49797143)\n\n\u6709\u4efb\u4f55\u95ee\u9898\u6b22\u8fce\u8054\u7cfbhanxiaoyang.ml@gmail.com\n\n"
 },
 {
  "repo": "trevorstephens/titanic",
  "language": "R",
  "readme_contents": "Titanic: Machine Learning from Disaster - Getting Started With R\n================================================================\nTrevor Stephens - January 2014\n\n\nI will be dividing this series of tutorials into five parts:\n\n    Part 1: Booting Up R\n    Part 2: The Gender-Class Model\n    Part 3: Decision Trees\n    Part 4: Feature Engineering\n    Part 5: Random Forests\n\nThis repository contains all code used in the five part tutorial that can be found here:\nhttp://trevorstephens.com/post/72916401642/titanic-getting-started-with-r\n\nIf you have any questions about these lessons, I\u2019d encourage you to post them to the Kaggle forums:\nhttp://www.kaggle.com/c/titanic-gettingStarted/forums\nwhere many others may have already come across the issue before and can jump in to help you out.\n\nIf you notice any bugs or typos, or have any suggestions on making the tutorial easier to follow,\nplease send me a direct message through any of ...\n\n* Kaggle: www.kaggle.com/users/105374\n* Twitter: https://twitter.com/trevs\n* Tumblr: http://trevorstephens.com/ask\n\nKind regards,\n\nTrevor Stephens\n"
 },
 {
  "repo": "ahmedbesbes/How-to-score-0.8134-in-Titanic-Kaggle-Challenge",
  "language": "Jupyter Notebook",
  "readme_contents": "### How to score 0.8134 in Titanic Kaggle Challenge\n\nThe Titanic challenge on Kaggle is a competition in which the task is to predict the survival or the death of a given passenger based on a set of variables describing him such as his age, his sex, or his passenger class on the boat.\nI have been playing with the Titanic dataset for a while, and I have recently achieved an accuracy score of 0.8134 on the public leaderboard.\nAs I'm writing this post, I am ranked among the top 9% of all Kagglers: More than 4540 teams are currently competing.\n\nIn a form of a jupyter notebook, my solution goes through the basic steps of a data science pipeline: \n\n- Exploratory data analysis with visualizations\n- Data cleaning\n- Feature engineering\n- Modeling \n- Modelfine-tuning\n\n![energy](./images/article_1/score.png)\n\n"
 },
 {
  "repo": "minsuk-heo/kaggle-titanic",
  "language": "Jupyter Notebook",
  "readme_contents": "error: no README"
 },
 {
  "repo": "mrdbourke/your-first-kaggle-submission",
  "language": "Jupyter Notebook",
  "readme_contents": "# Make your first Kaggle submission!\n\nThe Jupyter notebook goes through the Kaggle Titanic dataset via an exploratory data analysis (EDA) with Python and finishes with making a submission.\n\nIf you're interested in hearing the topics covered in the code explained, I went through the notebook in a recent livestream on my YouTube channel: https://youtu.be/f1y9wDDxWnA\n\nYou can download the data from this repo or [directly from Kaggle](https://www.kaggle.com/c/titanic).\n\nIf you find any bugs, or make any improvements on the results in the notebook, I'd love to hear. \n\nEmail me anytime: daniel@mrdbourke.com\n\nA full blog post to go along with the video and code is available [here](https://towardsdatascience.com/a-gentle-introduction-to-exploratory-data-analysis-f11d843b8184).\n\n"
 },
 {
  "repo": "shantnu/Titanic-Machine-Learning",
  "language": "Jupyter Notebook",
  "readme_contents": "error: no README"
 },
 {
  "repo": "datacamp/datacamp_facebook_live_titanic",
  "language": "Jupyter Notebook",
  "readme_contents": "# IMPORTANT\n\n**If you're planning to code along, make sure to clone, download, or re-pull this repository on the morning of Friday 12/01. All edits will be completed by end of day ET Thursday 11/31.**\n\n\n\n# How to complete a Kaggle Competition with Machine Learning\n\nwith Hugo Bowne-Anderson. Follow him on twitter [@hugobowne](https://twitter.com/hugobowne)\n\nAfter a successful first Facebook Live Coding session, DataCamp's very own Hugo Bowne-Anderson is back in front of the camera! This time, Hugo will take you from zero to one with machine learning to make several submissions to Kaggle's (in)famous [Titanic machine learning competition](https://www.kaggle.com/c/titanic). The goal will be to build an algorithm that predicts whether any given passenger on the Titanic survived or not, given data on them such as the fare they paid, where they embarked and their age. You'll do so using the Python programming language, Jupyter notebooks and state-of-the-art packages such as `pandas`, `scikit-learn` and `seaborn`. Alongside Hugo, you'll dive into this rich dataset and build your chops in exploratory data analysis, data munging and cleaning, and machine learning. No previous experience with machine learning necessary. Join us for this live, interactive code along.\n\nJoin Hugo live on Friday 12/01 at 10:30am ET on Facebook!\n\n<p align=\"center\">\n<img src=\"img/nytimes.jpg\" width=\"600\">\n</p>\n\n\n## Prerequisites\n\nNot a lot. It would help if you knew\n\n* programming fundamentals and the basics of the Python programming language (e.g., variables, for loops);\n* a bit about `pandas` and DataFrames;\n* a bit about Jupyter Notebooks;\n* your way around the terminal/shell.\n\n\n**However, I have always found that the most important and beneficial prerequisite is a will to learn new things so if you have this quality, you'll definitely get something out of this code-along session.**\n\nAlso, if you'd like to watch and **not** code along, you'll also have a great time and these notebooks will be downloadable afterwards also.\n\nIf you are going to code along and use the [Anaconda distribution](https://www.anaconda.com/download/) of Python 3 (see below), I ask that you install it before the session.\n\n**Note:** We may be making some live submissions to [Kaggle](https://www.kaggle.com) so, if you want to do that, get yourself an account before the session.\n\n\n## Getting set up computationally\n\n### 1. Clone the repository\n\nTo get set up for this live coding session, clone this repository. You can do so by executing the following in your terminal:\n\n```\ngit clone https://github.com/datacamp/datacamp_facebook_live_titanic\n```\n\nAlternatively, you can download the zip file of the repository at the top of the main page of the repository. If you prefer not to use git or don't have experience with it, this a good option.\n\n### 2. Download Anaconda (if you haven't already)\n\nIf you do not already have the [Anaconda distribution](https://www.anaconda.com/download/) of Python 3, go get it (n.b., you can also do this w/out Anaconda using `pip` to install the required packages, however Anaconda is great for Data Science and I encourage you to use it).\n\n### 3. Create your conda environment for this session\n\nNavigate to the relevant directory `datacamp_facebook_live_titanic` and install required packages in a new conda environment:\n\n```\nconda env create -f environment.yml\n```\n\nThis will create a new environment called fb_live_titanic. To activate the environment on OSX/Linux, execute\n\n```\nsource activate fb_live_titanic\n```\nOn Windows, execute\n\n```\nactivate fb_live_titanic\n```\n\n\n### 4. Open your Jupyter notebook\n\nIn the terminal, execute `jupyter notebook`.\n\nThen open the notebook `1-titanic_EDA_first_models.ipynb` and we're ready to get coding. Enjoy.\n\n\n### Code\nThe code in this repository is released under the [MIT license](LICENSE). Read more at the [Open Source Initiative](https://opensource.org/licenses/MIT). All text remains the Intellectual Property of DataCamp. If you wish to reuse, adapt or remix, get in touch with me at hugo at datacamp com to request permission.\n"
 },
 {
  "repo": "UltravioletAnalytics/kaggle-titanic",
  "language": "Python",
  "readme_contents": "kaggle-titanic\n==============\n\nThis is the python/scikit-learn code I wrote during my stab at the Kaggle titanic competition. There is code for several different algorithms, but the primary and highest performing one is the RandomForest implemented in randomforest2.py.\n\nRequirements:\n- python (a 2.x release at least 2.6)\n- scikit-learn/NumPy/SciPy (http://scikit-learn.org/stable/install.html)\n- pandas (http://pandas.pydata.org/pandas-docs/stable/install.html)\n- matplotlib (http://matplotlib.org/faq/installing_faq.html)\n\nUsage:<br/>\n    > python randomforest2.py\n\nKey files:\n- loaddata.py: Contains all the feature engineering including options for generating different variable types, and performing PCA, clustering, and class balancing\n- randomforest2.py: The code that executes the pipeline\n- scorereport.py: Inspects and reports on the results of hyperparameter search\n- learningcurve.py: Includes code to generate a learning curve\n- roc_auc: Includes code to generate a ROC curve\n\nOther files contain other algorithms that were used during experimentation and are in various stages of completeness. Only randomforest2 is 100% up to date\n"
 },
 {
  "repo": "nd009/titanic_survival_exploration",
  "language": "Jupyter Notebook",
  "readme_contents": "# \u9879\u76ee 0: \u5165\u95e8\u4e0e\u57fa\u7840\n## \u9884\u6d4b\u6cf0\u5766\u5c3c\u514b\u53f7\u4e58\u5ba2\u5e78\u5b58\u7387\n\n### \u5b89\u88c5\u8981\u6c42\n\u8fd9\u4e2a\u9879\u76ee\u8981\u6c42\u4f7f\u7528 **Python 2.7** \u4ee5\u53ca\u5b89\u88c5\u4e0b\u5217python\u5e93\n\n- [NumPy](http://www.numpy.org/)\n- [Pandas](http://pandas.pydata.org)\n- [matplotlib](http://matplotlib.org/)\n- [scikit-learn](http://scikit-learn.org/stable/)\n  \u200b\n\n\u4f60\u8fd8\u9700\u8981\u5b89\u88c5\u548c\u8fd0\u884c [Jupyter Notebook](http://jupyter.readthedocs.io/en/latest/install.html#optional-for-experienced-python-developers-installing-jupyter-with-pip)\u3002\n\n\n\u4f18\u8fbe\u5b66\u57ce\u63a8\u8350\u5b66\u751f\u5b89\u88c5 [Anaconda](https://www.continuum.io/downloads)\uff0c\u4e00\u4e2a\u5305\u542b\u4e86\u9879\u76ee\u9700\u8981\u7684\u6240\u6709\u5e93\u548c\u8f6f\u4ef6\u7684 Python \u53d1\u884c\u7248\u672c\u3002[\u8fd9\u91cc](https://classroom.udacity.com/nanodegrees/nd002/parts/0021345403/modules/317671873575460/lessons/5430778793/concepts/54140889150923)\u4ecb\u7ecd\u4e86\u5982\u4f55\u5b89\u88c5Anaconda\u3002\n\n\u5982\u679c\u4f60\u4f7f\u7528macOS\u7cfb\u7edf\u5e76\u4e14\u5bf9\u547d\u4ee4\u884c\u6bd4\u8f83\u719f\u6089\uff0c\u53ef\u4ee5\u5b89\u88c5[homebrew](http://brew.sh/)\uff0c\u4ee5\u53cabrew\u7248python\n\n```bash\n$ brew install python\n```\n\n\u518d\u7528\u4e0b\u5217\u547d\u4ee4\u5b89\u88c5\u6240\u9700\u8981\u7684python\u5e93\n\n```bash\n$ pip install numpy pandas matplotlib scikit-learn scipy jupyter\n```\n\n### \u4ee3\u7801\n\u200b\n\u6838\u5fc3\u4ee3\u7801\u5728 `titanic_survival_exploration.ipynb` \u6587\u4ef6\u4e2d\uff0c\u8f85\u52a9\u4ee3\u7801\u5728 `titanic_visualizations.py` \u6587\u4ef6\u4e2d\u3002\u5c3d\u7ba1\u5df2\u7ecf\u63d0\u4f9b\u4e86\u4e00\u4e9b\u4ee3\u7801\u5e2e\u52a9\u4f60\u4e0a\u624b\uff0c\u4f60\u8fd8\u662f\u9700\u8981\u8865\u5145\u4e9b\u4ee3\u7801\u4f7f\u5f97\u9879\u76ee\u8981\u6c42\u7684\u529f\u80fd\u80fd\u591f\u6210\u529f\u5b9e\u73b0\u3002\n\n### \u8fd0\u884c\n\u200b\n\u5728\u547d\u4ee4\u884c\u4e2d\uff0c\u786e\u4fdd\u5f53\u524d\u76ee\u5f55\u4e3a `titanic_survival_exploration/` \u6587\u4ef6\u5939\u7684\u6700\u9876\u5c42\uff08\u76ee\u5f55\u5305\u542b\u672c README \u6587\u4ef6\uff09\uff0c\u8fd0\u884c\u4e0b\u5217\u547d\u4ee4\uff1a\n\n```bash\n$ jupyter notebook titanic_survival_exploration.ipynb\n```\n\u200b\n\u8fd9\u4f1a\u542f\u52a8 Jupyter Notebook \u628a\u9879\u76ee\u6587\u4ef6\u6253\u5f00\u5728\u4f60\u7684\u6d4f\u89c8\u5668\u4e2d\u3002\n\n\u5bf9jupyter\u4e0d\u719f\u6089\u7684\u540c\u5b66\u53ef\u4ee5\u770b\u4e00\u4e0b\u8fd9\u4e24\u4e2a\u94fe\u63a5\uff1a\n\n- [Jupyter\u4f7f\u7528\u89c6\u9891\u6559\u7a0b](http://cn-static.udacity.com/mlnd/how_to_use_jupyter.mp4)\n- [\u4e3a\u4ec0\u4e48\u4f7f\u7528jupyter\uff1f](https://www.zhihu.com/question/37490497)\n\u200b\n\u200b\n\u200b\n\u200b\n\u200b\n\u200b\n\u200b\n\u200b\n\u200b\n\u200b\n\u200b\n\u200b\n\u200b\n\u200b\n\n### \u6570\u636e\n\u200b\n\u8fd9\u4e2a\u9879\u76ee\u7684\u6570\u636e\u5305\u542b\u5728 `titanic_data.csv` \u6587\u4ef6\u4e2d\u3002\u6587\u4ef6\u5305\u542b\u4e0b\u5217\u7279\u5f81\uff1a\n\u200b\n- **Survived**\uff1a\u662f\u5426\u5b58\u6d3b\uff080\u4ee3\u8868\u5426\uff0c1\u4ee3\u8868\u662f\uff09\n- **Pclass**\uff1a\u793e\u4f1a\u9636\u7ea7\uff081\u4ee3\u8868\u4e0a\u5c42\u9636\u7ea7\uff0c2\u4ee3\u8868\u4e2d\u5c42\u9636\u7ea7\uff0c3\u4ee3\u8868\u5e95\u5c42\u9636\u7ea7\uff09\n- **Name**\uff1a\u8239\u4e0a\u4e58\u5ba2\u7684\u540d\u5b57\n- **Sex**\uff1a\u8239\u4e0a\u4e58\u5ba2\u7684\u6027\u522b\n- **Age**\uff1a\u8239\u4e0a\u4e58\u5ba2\u7684\u5e74\u9f84\uff08\u53ef\u80fd\u5b58\u5728 `NaN`\uff09\n- **SibSp**\uff1a\u4e58\u5ba2\u5728\u8239\u4e0a\u7684\u5144\u5f1f\u59d0\u59b9\u548c\u914d\u5076\u7684\u6570\u91cf\n- **Parch**\uff1a\u4e58\u5ba2\u5728\u8239\u4e0a\u7684\u7236\u6bcd\u4ee5\u53ca\u5c0f\u5b69\u7684\u6570\u91cf\n- **Ticket**\uff1a\u4e58\u5ba2\u8239\u7968\u7684\u7f16\u53f7\n- **Fare**\uff1a\u4e58\u5ba2\u4e3a\u8239\u7968\u652f\u4ed8\u7684\u8d39\u7528\n- **Cabin**\uff1a\u4e58\u5ba2\u6240\u5728\u8239\u8231\u7684\u7f16\u53f7\uff08\u53ef\u80fd\u5b58\u5728 `NaN`\uff09\n- **Embarked**\uff1a\u4e58\u5ba2\u4e0a\u8239\u7684\u6e2f\u53e3\uff08C \u4ee3\u8868\u4ece Cherbourg \u767b\u8239\uff0cQ \u4ee3\u8868\u4ece Queenstown \u767b\u8239\uff0cS \u4ee3\u8868\u4ece Southampton \u767b\u8239\uff09\n"
 },
 {
  "repo": "mdelhey/kaggle-titanic",
  "language": "Python",
  "readme_contents": "<h1>kaggle-titanic</h1>\r\n==============\r\nEntry in the Titanic: Machine Learning from Disaster competition @ <a href=\"http://www.kaggle.com\">kaggle.com</a>.\r\n<br />\r\n<br />\r\n<strong>Authors:</strong>\r\n<ul>\r\n<li>Matt Delhey; <a href=\"http://mattdelhey.com\">mattdelhey.com</a></li>\r\n<li>Frank Portman; <a href=\"http://fportman.com\">fportman.com</a></li>\r\n</ul>\r\n\r\n<h2>R files and submissions</h2>\r\n<ul>\r\n<li><code>1-clean.r</code>: \r\n  <ul>\r\n  <li>fixes data structures and missing values</li>\r\n  </ul>\r\n<li><code>2-randomForest</code>:\r\n  <ul>\r\n  <li>creates randomForest model</li>\r\n  </ul>\r\n<li><code>3-SVM</code>:\r\n  <ul>\r\n  <li>creates Support Vector Machine model</li>\r\n  </ul>\r\n<li><code>4-probit</code>:\r\n  <ul>\r\n  <li>creates probability unit model</li>\r\n  </ul>\r\n<li><code>5-model_testing</code>: \r\n  <ul>\r\n  <li>evaluates the accuracy of model</li>\r\n  </ul>\r\n</ul>\r\n"
 },
 {
  "repo": "upura/ml-competition-template-titanic",
  "language": "Python",
  "readme_contents": "ml-competition-template-titanic\n===\n- [Kaggle Titanic](https://www.kaggle.com/c/titanic) example of my own, inspired by [flowlight0's repo](https://github.com/flowlight0/talkingdata-adtracking-fraud-detection).\n- You can get the score = 0.76555 at the version of 2018-12-28.\n- Japanese article can be seen [here](https://upura.hatenablog.com/entry/2018/12/28/225234).\n\n# Structures\n```\n.\n\u251c\u2500\u2500 configs\n\u2502   \u2514\u2500\u2500 default.json\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 input\n\u2502   \u2502   \u251c\u2500\u2500 sample_submission.csv\n\u2502   \u2502   \u251c\u2500\u2500 train.csv\n\u2502   \u2502   \u2514\u2500\u2500 test.csv\n\u2502   \u2514\u2500\u2500 output\n\u251c\u2500\u2500 features\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 base.py\n\u2502   \u2514\u2500\u2500 create.py\n\u251c\u2500\u2500 logs\n\u2502   \u2514\u2500\u2500 logger.py\n\u251c\u2500\u2500 models\n\u2502   \u2514\u2500\u2500 lgbm.py\n\u251c\u2500\u2500 notebooks\n\u2502   \u2514\u2500\u2500 eda.ipynb\n\u251c\u2500\u2500 scripts\n\u2502   \u2514\u2500\u2500 convert_to_feather.py\n\u251c\u2500\u2500 utils\n\u2502   \u2514\u2500\u2500 __init__.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 .pylintrc\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 run.py\n\u2514\u2500\u2500 tox.ini\n```\n# Commands\n\n## Change data to feather format\n\n```\npython scripts/convert_to_feather.py\n```\n\n## Create features\n\n```\npython features/create.py\n```\n\n## Run LightGBM\n\n```\npython run.py\n```\n\n## flake8\n\n```\nflake8 .\n```\n"
 },
 {
  "repo": "wangchen1ren/Titanic",
  "language": "Python",
  "readme_contents": "# Titanic\n"
 },
 {
  "repo": "cindycindyhi/kaggle-Titanic",
  "language": "Python",
  "readme_contents": "# kaggle-Titanic\npython 2.7\nscikit learn 0.15\nnumpy && scipy && matplotlib\n\nTitanic is a competition in Kaggle for knowledge. \nFor detail infomation: http://www.cnblogs.com/north-north/p/4360121.html\n"
 },
 {
  "repo": "birchsport/titanic",
  "language": "Java",
  "readme_contents": "# titanic #\n\nExample code for solving the Titanic prediction problem (https://www.kaggle.com/c/titanic-gettingStarted) found on Kaggle.  This example uses the Weka Data Mining Libraries to perform our classifications and predictions. Note, we are using Weka version 3.6.9.\n\n## Data Cleanup/Initialization ##\n\nBefore we begin, we have to clean up the data files provided by Kaggle (these cleanup steps have already been performed on the committed files).  The first step is to remove the nested '\"\"' (quotation marks) from the files.  This was simply a straight search and replace operation in my editor.\n\nThe next step is to convert the CSV formatted files into the ARFF format.  The ARFF format provides more detailed information about the type of data in the CSV files.  To perform this conversion, you can use the CSVLoader from the Weka libraries.\n\n```\njava -cp lib/weka.jar weka.core.converters.CSVLoader test.csv > test.arff\njava -cp lib/weka.jar weka.core.converters.CSVLoader train.csv > train.arff\n```\n\nOnce we have created the ARFF files, we need to clean them up a little bit.  First, we identify any 'string' column to be of type string, and not nominal.  Then we ensure that nominal values are in the same order for both files (VERY IMPORTANT!).  Here is what the header section of the ARFF file should look like:\n\n```\n@attribute survived {0,1}\n@attribute pclass numeric\n@attribute name string\n@attribute sex {male,female}\n@attribute age numeric\n@attribute sibsp numeric\n@attribute parch numeric\n@attribute ticket string\n@attribute fare numeric\n@attribute cabin string\n@attribute embarked {Q,S,C}\n```\n\n## Training, Predicting, and Verifying the data ##\n\nNow that we have cleaned up our data, we are ready to run the code.  I have included the Eclipse project files to make it easy for anyone to import this project into Eclipse and go.  I have also included an Ant build file to compile and run everything as well.  If you don't have either of those options, you are on your own.\n\n### Training ###\n\nTo train the classifier, execute the 'titanic.weka.Train' class or run 'ant train' in a terminal.  This will load the training data, create and train a Classifier, and write the Classifier to disk.\n\n### Predicting ###\n\nTo create a prediction, execute the 'titanic.weka.Predict' class or run 'ant predict'.  This will load the test data, read the trained Classifier from disk, and produce a 'predict.csv'.  This CSV file is in a suitable format to submit to Kaggle.\n\n### Verifying ###\n\nTo verify our predictions, execute the 'titanic.weka.Verify' class or run 'ant verify'.  This will load our prediction results, read the trained Classifier from disk, then evaluate the classification performance.  You will see output similar to this:\n\n```\nCorrectly Classified Instances         418              100      %\nIncorrectly Classified Instances         0                0      %\nKappa statistic                          1     \nMean absolute error                      0.1409\nRoot mean squared error                  0.1986\nRelative absolute error                 30.3515 %\nRoot relative squared error             41.2246 %\nTotal Number of Instances              418     \n```\n"
 },
 {
  "repo": "Biedlin/Titanic84",
  "language": "Jupyter Notebook",
  "readme_contents": "# Titanic84\nKaggle:\u5165\u95e8\u8d5bTatanic\uff08\u6cf0\u5766\u5c3c\u514b\u53f7\uff0984.21%\u5e26\u4f60\u51b2\u8fdb\u524d2%\n"
 },
 {
  "repo": "ramansah/kaggle-titanic",
  "language": "Jupyter Notebook",
  "readme_contents": "# kaggle-titanic"
 },
 {
  "repo": "matthagy/titanic-families",
  "language": "OpenEdge ABL",
  "readme_contents": "Determine the families in the Kaggle Titanic competition\n------------------------------------------------------------\nHeuristic-based algorithm to find the family trees in the titanic data.\n\nOnly involves a few parameters, all of which are related to age\n(e.g. minimum age for marriage). See the constant defined at the\ntop for all of them. Neither iterative nor stochastic methods are\nused.\n\nFirst a graph of individuals is constructed where the edges represent\nshared last names. This includes any previous names such maiden names.\nEach edge represents as relationship that can be classified as one\nof the following:\n   * Spouse\n   * Parent/Child\n   * Sibling\n   * Extended (e.g. aunt, cousin, or distant relative)\n\nThe classification scheme is optimistic, i.e. we only ask\nwhether or not the relationship is possible. Much of information\ncan be directly inferred from the given attributes (e.g. two\nindividual cannot be siblings if one of them has sibsp==0).\n\nNext we prove spousal relationship. This fairly easy, epically\nas many spouses name pairs are of the form:\n\n    West, Mrs. Edwy Arthur (Ada Mary Worth)\n    West, Mr. Edwy Arthur\n\nWe don't require names of this style and can also use age differences,\nrequires Mrs title for the female, and other simple heuristics.\nThe only difficulty arises when one individual could be classified as\nmarried to multiple individuals. There are only a few such situations and\nthey can all be handled by assigning marriage to the couple in which the\nfemale has the males first name (e.g. Mrs. Edwy Arthur).\n\nWith spousal relationships found it is then straightforward to workout\nparent/child relationships. The only ambiguities at this point are\nchild vs. sibling and they can be resolved by checking for common\nparent(s). Lastly, parent/child relationships can be used to work out\nsibling relationships. We can then recover the structure of nuclear\nfamilies: families in which there is at least one parent and one or more\nchildren.\n\nOutside of the nuclear family structure, we still maintain the\nrelationship graph which allows for such classifications as:\n  * siblings traveling together without any parents\n  * extended relations\n  * families joined by extended relationships\n\nAt the moment there are still some edge cases. In particular, the largest\nrelationship graph component isn't separated into a family structure.\nAdditionally, it would be nice to remove or relax the few parameters.\n\nProject is licensed under the BSD (2-clause) license\n(see include LICENSE file)\n"
 },
 {
  "repo": "JohnCoene/titanicon",
  "language": "R",
  "readme_contents": "# titanicon\n\n<!-- badges: start -->\n[![Travis build status](https://travis-ci.org/JohnCoene/titanicon.svg?branch=master)](https://travis-ci.org/JohnCoene/titanicon)\n<!-- badges: end -->\n\nBrings [Titanic animated icons](https://icons8.com/c/animated-icons) to Shiny and Rmarkdown.\n\n## Installation\n\n``` r\n# install.packages(\"remotes\")\nremotes::install_github(\"johncoene/titanicon\")\n```\n\n## Example\n\nBrowse all icons with:\n\n```r\ntitanicon::titanicon_demo()\n```\n\nBelow is a self-explanatory example.\n\n```r\nlibrary(shiny)\nlibrary(titanicon)\n\nui <- fluidPage(\n  use_titanicon(), # imports dependencies\n  titanicon_theme(style = \"height:100px;\"), # applies to all subsequent icons\n  titanicon(\"idea\"),\n  titanicon(\"mic\"),\n  titanicon_opts(click = FALSE) # options\n)\n\nserver <- function(input, output){}\n\nshinyApp(ui, server)\n```\n"
 },
 {
  "repo": "prateekiiest/titanic_survival_exploration",
  "language": "Jupyter Notebook",
  "readme_contents": "# Titanic_Survival_Exploration\n\n[![Codacy Badge](https://api.codacy.com/project/badge/Grade/c73bee6015bf485d8ce4184cbb135b03)](https://www.codacy.com/app/prateekkol21/titanic_survival_exploration?utm_source=github.com&utm_medium=referral&utm_content=prateekiiest/titanic_survival_exploration&utm_campaign=badger)\n[![Build status](https://ci.appveyor.com/api/projects/status/vps8mifg5qyqqu7g?svg=true)](https://ci.appveyor.com/project/prateekiiest/titanic-survival-exploration)\n[![Maintainability](https://api.codeclimate.com/v1/badges/4c2f2473dae6c52f64a1/maintainability)](https://codeclimate.com/github/prateekiiest/titanic_survival_exploration/maintainability)\n\n[![chat on Slack](https://img.shields.io/badge/chat%20on%20-Slack-blue.svg)](https://join.slack.com/t/titanic-survival/shared_invite/enQtMjkyMzYxODE0MDk4LTdkZjgxYTFiZTdiYjQxYWNjMDM0MmNkYzIxOWU1ZDY2NDllM2Q1YThjZDUzMTI5ZmI0ODQwYjU2ZjNiODZkZmI)\n[![made with &hearts in Python](https://img.shields.io/badge/made%20with%20%E2%9D%A4%20in-Python-red.svg)](http://shields.io/#your-badge)\n\n\n\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1098228.svg)](https://doi.org/10.5281/zenodo.1098228)\n\n<a href=\"https://github.com/prateekiiest/boston_housing\"><img style=\"position: relative; top: 0; left: 0; border: 0;\" src=\"https://68.media.tumblr.com/38ae897f20630ef88e6484dea00db3b3/tumblr_mm8fhitR3u1rwwvg9o1_500.gif\" alt=\" Fork this repo\" data-canonical-></a>\n\n\n\n### This repository contains project file for Project 0 - Titanic Survival Exploration as part of Udacity's Machine Learning Nanodegree.\n\n---------------------------------------------------------------------------------\n\n### KWOC\n\nWe are glad to partner with IIT Kharagpur as a part of the Kharagpur Winter of Code. We are proud to host this Open Source event during the winter months and we hope you have a great winter this year.\n\n#### See Project Ideas [here](https://github.com/prateekiiest/titanic_survival_exploration/wiki/Winter-of-Code-Project)\n\n\n-------------------------------------------------------------------------\n\n\n### Description\n\n>The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew.  This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\n>One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.  Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\n>In this problem, we ask you to complete the analysis of what sorts of people were likely to survive.  In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n\n\n\n\nIn this optional project, you will create decision functions that attempt to predict survival outcomes from the 1912 Titanic disaster based on each passenger\u2019s features, such as sex and age. Start with a simple algorithm and increase its complexity until you are able to accurately predict the outcomes for at least 80% of the passengers in the provided data. This project will introduce you to some of the concepts of machine learning as you start the Nanodegree program.\n\n\n\n\n### Install\n\nThis project requires **Python 2.7** and the following Python libraries installed:\n\n- [NumPy](http://www.numpy.org/)\n- [Pandas](http://pandas.pydata.org)\n- [matplotlib](http://matplotlib.org/)\n- [scikit-learn](http://scikit-learn.org/stable/)\n\nYou will also need to have software installed to run and execute an [iPython Notebook](http://ipython.org/notebook.html)\n\nUdacity recommends our students install [Anaconda](https://www.continuum.io/downloads), i pre-packaged Python distribution that contains all of the necessary libraries and software for this project. \n\n### Code\n\nTemplate code is provided in the notebook `titanic_survival_exploration.ipynb` notebook file. Additional supporting code can be found in `titanic_visualizations.py`. While some code has already been implemented to get you started, you will need to implement additional functionality when requested to successfully complete the project.\n\n#### This Notebook will show basic examples of:\n#### Data Handling\n*   Importing Data with Pandas\n*   Cleaning Data\n*   Exploring Data through Visualizations with Matplotlib\n\n#### Data Analysis\n*    Supervised Machine learning Techniques:\n    +   Logit Regression Model\n    +   Plotting results\n    +   Support Vector Machine (SVM) using 3 kernels\n    +   Basic Random Forest\n    +   Plotting results\n\n#### Valuation of the Analysis\n*   K-folds cross validation to valuate results locally\n*   Output the results from the IPython Notebook to Kaggle\n\n\n\n### Run\n\nIn a terminal or command window, navigate to the top-level project directory `titanic_survival_exploration/` (that contains this README) and run one of the following commands:\n\n```ipython notebook titanic_survival_exploration.ipynb```\n```jupyter notebook titanic_survival_exploration.ipynb```\n\nThis will open the iPython Notebook software and project file in your browser.\n\n## Data\n\nThe dataset used in this project is included as `titanic_data.csv`. This dataset is provided by Udacity and contains the following attributes:\n\n- `survival` ? Survival (0 = No; 1 = Yes)\n- `pclass` ? Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n- `name` ? Name\n- `sex` ? Sex\n- `age` ? Age\n- `sibsp` ? Number of Siblings/Spouses Aboard\n- `parch` ? Number of Parents/Children Aboard\n- `ticket` ? Ticket Number\n- `fare` ? Passenger Fare\n- `cabin` ? Cabin\n- `embarked` ? Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n\n## Results\n Check here [Udacity Reviews](https://github.com/prateekiiest/titanic_survival_exploration/blob/master/Udacity_Reviews_titanic.pdf)\n\n## Contribution\n\nSee CONTRIBUTING.md\n\n### Some Video Resources\n- [ ] [Coursera Lectures by Andrew Ng](https://www.coursera.org/learn/machine-learning/lecture/zcAuT/welcome-to-machine-learning) are not very mathematically heavy and provide a good introduction to ML algorithms.\n- [ ] [Standford Lectures](https://www.youtube.com/watch?v=UzxYlbK2c7E)\n- [ ] [Unsupervised Learning](https://www.coursera.org/learn/machine-learning/lecture/olRZo/unsupervised-learning)\n- [ ] [Udacity Lectures (Intro to ML)](https://in.udacity.com/course/intro-to-machine-learning--ud120)\n- [ ] [Udacity Lectures (ML)](https://in.udacity.com/course/machine-learning--ud262)\n- [ ] [Sentdex Lectures on Introduction to ML](https://www.youtube.com/watch?v=OGxgnH8y2NM&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v)\n- [ ] [Udemy Lectures on ML using Python as well as R](https://www.udemy.com/machinelearning/)\n- [ ] [Udemy Course on various Data science and Machine Learning Techniques](https://www.udemy.com/python-for-data-science-and-machine-learning-bootcamp/)\n- [ ] [Machine Learning A-Z\u2122: Hands-On Python & R In Data Science](https://www.udemy.com/machinelearning/)\n- [ ] [EdX: Learning From Data (Introductory Machine Learning)](https://www.edx.org/course/learning-data-introductory-machine-caltechx-cs1156x-0)\n\n### Online Reading Material\n- [ ] [Advanced Introduction to Machine Learning ](http://www.cs.cmu.edu/~epxing/Class/10715/lecture.html)\n- [ ] [CS229: Machine Learning ](https://cs229.stanford.edu/)\n\n#### Happy Coding                                                                                           -- Prateek Chanda\n\n"
 },
 {
  "repo": "zainab-ali/titanic",
  "language": "Scala",
  "readme_contents": "# Titanic\n\n[![Build Status](https://travis-ci.org/zainab-ali/titanic.svg?branch=master)](https://travis-ci.org/zainab-ali/titanic)\n\n## Aim\n\nTo predict whether a person would have lived or died on the Titanic.\n\n## The dataset\n\nThis project contains a dataset of passengers on the Titanic.  It can be used to assess different predictive models of survival.\n\nThe dataset has been obtained from http://biostat.mc.vanderbilt.edu/DataSets.\nThe full dataset is located in `original.csv`. It has been randomly shuffled into training and validation data `train.csv` and test data `test.csv`.\n\nPredictive models should be trained and validated on the training data before being tested on the test data.\n\n\n## The problem\n\nThis problem is stochastic in nature, so is *inconsistent*.  We cannot expect to find a model which correctly predicts all of training points.\n\n## Hypotheses\n\nThe project currently contains four hypothesis classes:\n\n1. Everyone Dies\n2. Females survive\n3. Greedy decision trees\n4. Pruned decision trees\n\nThese hypotheses can be run using the `titanic.app.HypothesisRunner`.\n"
 },
 {
  "repo": "Andrewnetwork/NeuralTitanic",
  "language": "JavaScript",
  "readme_contents": "# Neural Titanic\nThis visualization uses [TensorFlow.js](https://js.tensorflow.org/) to train a neural network on the titanic dataset and visualize how the predictions of the neural network evolve after every training epoch. The colors of each row indicate the predicted survival probability for each passenger. Red indicates a prediction that a passenger died. Green indicates a prediction that a passenger survived. The intensity of the color indicates the magnitude of the prediction probability. As an example, a bright green passenger represents a strong predicted probability for survival. We also plot the loss of our objective function on the left of the table with [D3.js](https://d3js.org/). \n\nView live: https://andrewnetwork.github.io/NeuralTitanic/dist/\n\n## Setup \n1. Install [Node.js](https://nodejs.org/en/). \n2. Clone or download this repo. \n3. Open a terminal and cd into this repo. \n4. In your terminal type: ```npm install```\n5. Launch the dev server by typing: ```npm run dev```\n6. Click on the url shown in the terminal or enter ```http://localhost:8080/``` in your web browser. \n\nIf you wish to bundle the code for production run: \n```npm run build```. This will produce the flat files you can serve on the web in the ```/dist/``` directory. I have not optimized this code for real production use and the dependencies are quite extraneously large. "
 },
 {
  "repo": "cystanford/Titanic_Data",
  "language": "Python",
  "readme_contents": "# Titanic_Data\n\u6570\u636e\u96c6\u4e2d\u7684\u5b57\u6bb5\u63cf\u8ff0\uff1a\nPassengerId\t\u4e58\u5ba2\u7f16\u53f7\nSurvived\t\u662f\u5426\u5e78\u5b58\nPclass\t\u8239\u7968\u7b49\u7ea7\nName\t\u4e58\u5ba2\u59d3\u540d\nSex\t\u4e58\u5ba2\u6027\u522b\nSibSp\t\u4eb2\u621a\u6570\u91cf\uff08\u5144\u59b9\u3001\u914d\u5076\u6570\uff09\nParch\t\u4eb2\u621a\u6570\u91cf\uff08\u7236\u6bcd\u3001\u5b50\u5973\u6570\uff09\nTicket\t\u8239\u7968\u53f7\u7801\nFare\t\u8239\u7968\u4ef7\u683c\nCabin\t\u8239\u8231\nEmbarked\t\u767b\u5f55\u6e2f\u53e3\n"
 },
 {
  "repo": "dataworkshop/webinar-titanic",
  "language": "Jupyter Notebook",
  "readme_contents": "# Titanic \n\n1. [Understand Business and Data](https://github.com/dataworkshop/webinar-titanic/blob/master/notebooks/understand_business_data.ipynb)\n2. [Basic Model](https://github.com/dataworkshop/webinar-titanic/blob/master/notebooks/basic_model.ipynb)\n3. [Feature Engineering](https://github.com/dataworkshop/webinar-titanic/blob/master/notebooks/feature_engineering.ipynb)\n\n## Video (in Polish) \n[![webinar](https://img.youtube.com/vi/GvuxM3z4xU4/0.jpg)](https://www.youtube.com/watch?v=GvuxM3z4xU4)\n"
 },
 {
  "repo": "mazurkin/titanic",
  "language": "Python",
  "readme_contents": "error: no README"
 },
 {
  "repo": "rladiestaipei/Azureml-shiny-app",
  "language": "R",
  "readme_contents": "# Azureml-shiny-app\nTitanic prediction workshop - shiny on Azure via ML studio\n\n\u5b78\u7fd2\u5982\u4f55\u7528\u9435\u9054\u5c3c\u865f\u6578\u64da\u505a\u51fa\u4e00\u500bweb service\uff0c\u4e0a\u624bMachine Learning Studio \u4ee5 Web \u670d\u52d9\u65b9\u5f0f\u767c\u4f48\u6a21\u578b\uff0c\u900f\u904eR Shiny\u505a\u51fa\u53ef\u4e92\u52d5\u5f0f\u9810\u6e2c\u7d50\u679c\u4e26\u5728 Azure \u865b\u64ec\u6a5f\u5668\u57f7\u884c\n\n\u8ab2\u7a0b\u5167\u5bb9\u5206\u6210\u4e09\u90e8\u5206\n1. Azure Machine Learning Studio\u4ecb\u7d39\n2. \u9023\u7d50\u9810\u8a2d\u6a21\u578b(AzureML\u88e1)\u5230Shiny\n3. \u5982\u4f55\u7528Python\u5728Azure ML\u523b\u6f14\u7b97\u6cd5\n\n![\u5716](https://www.evernote.com/l/ANzHJ9AEGuBJ76lgy3taL1uELsbMe353i28B/image.png)\n\n\n\n\n\u6bcf\u500b\u90e8\u5206\u6703\u5305\u542b Code/\u793a\u7bc4\u3001\u6559\u5b78\u6587\u4ef6\u3001\u88dc\u5145\u6559\u6750\n\n\u9435\u9054\u5c3c\u865f\u8a13\u7df4\u8cc7\u6599\u96c6 [link](https://goo.gl/S3Nz1H)\n## Azure Machine Learning Studio\u4ecb\u7d39 @Ning\n### Code/\u793a\u7bc4\n[link](https://gallery.cortanaintelligence.com/Experiment/AzureMLPredictModelforteaching)\n\n### \u6559\u5b78\u6587\u4ef6\n[Slide](https://docs.google.com/presentation/d/121feaGbLOVaX6-4C1HOoz6_5mGKAaac9XrWk8Ico1qo/edit?usp=sharing)\n[\u5b98\u65b9\u6587\u4ef6](https://docs.microsoft.com/en-us/azure/machine-learning/studio/what-is-ml-studio)\n\n### \u88dc\u5145\u6559\u6750\nAzureML\u74b0\u5883\u4ecb\u7d39 [Video](https://www.facebook.com/chiehningchen/videos/10154231877932471/)\n\u5982\u4f55\u4e0a\u50b3\u8cc7\u6599 [Video](https://www.facebook.com/chiehningchen/videos/10154322903962471/)\n\u4e94\u5206\u9418\u5feb\u901f\u4e0a\u624b [link](https://docs.microsoft.com/en-us/azure/machine-learning/preview/)\n\n\n## \u9023\u7d50\u9810\u8a2d\u6a21\u578b(AzureML\u88e1)\u5230Shiny @Kristen\n### Code/\u793a\u7bc4\n[link](https://github.com/rladiestaipei/Azureml-shiny-app/tree/master/Shiny_Titanic)\n\n### \u6559\u5b78\u6587\u4ef6\n[Slide](https://drive.google.com/open?id=1v6OqqdPpJJwnGUq6Hexe78lx0m8lIxII)\n\n### \u88dc\u5145\u6559\u6750\n+ [Request Response API Documentation for TitanicWS](https://studio.azureml.net/apihelp/workspaces/852a506a05ab41868939caa8f97d3a57/webservices/cc53c7743e5b4abbbeb417fa807c4fbc/endpoints/c052c781636540b4a2530c5b753cb947/score#sampleCode)\n+ [Shiny Cheat sheet](https://shiny.rstudio.com/articles/cheatsheet.html)\n\n\n\n## \u5982\u4f55\u7528Python\u5728Azure ML\u523b\u6f14\u7b97\u6cd5 @Mia\n\n### Code/\u793a\u7bc4\n[link](https://github.com/rladiestaipei/Azureml-shiny-app/tree/master/python)\n### \u6559\u5b78\u6587\u4ef6\n[Slides - Tips](https://docs.google.com/presentation/d/17VgbMwYibq5E3zQSulG8eq5Z3_0uFuCsMdRrlyZNRo8/)\n[Slides - with python code](https://docs.google.com/presentation/d/1ny_dCwszjw5kwoeIcgzBJVSlZNw_DmNMAYx92HxxtAI/)\n### \u88dc\u5145\u6559\u6750\n\u53c3\u8003\u8cc7\u65991.Training and operationalization of Scikit-Learn models\nhttps://gallery.cortanaintelligence.com/Experiment/Training-and-operationalization-of-Scikit-Learn-models-1\n\n\n## FAQ\n[link](https://onedrive.live.com/?cid=cf211efc6ea69b1a&id=CF211EFC6EA69B1A%21131&authkey=%21AGt6Ehbd7FREV2Y)\n"
 },
 {
  "repo": "ehsanmok/sparkling-titanic",
  "language": "Python",
  "readme_contents": "Sparkling Titanic\n=================\n\n### Introduction\n\n`titanic_logReg.py` trains a Logistic Regression and makes prediction for [Titanic dataset](http://kaggle.com/c/titanic/data) as part of Kaggle competition using Apache-Spark [spark-1.3.1-bin-hadoop2.4](http://spark.apache.org/downloads.html) with its Python API on a local machine. I used `pyspark_csv.py` to load data as Spark DataFrame, for more instructions see [this](http://github.com/seahboonsiew/pyspark-csv). \n\nThe following will be added later\n\n*   Imputing NAs in train and test sets\n*   Cross-validation\n*   Using more features and feature engineering\n*   RandomForest classifier, SVM, etc.\n\n### Running PySpark Script in Shell\n\nUse `$SPARK_HOME/bin/spark-submit scriptDirectoryPath/titanic_logReg.py`. For multithreading, you can add the option `--master local[N]` where N is the number of threads.\n\n\n\n\n"
 },
 {
  "repo": "jaza10/AppliedNeuralNetworkTitanicSurvival",
  "language": "Jupyter Notebook",
  "readme_contents": "# AppliedNeuralNetworkTitanicSurvival\nThis notebook applies the architecture from Andrew Ng's Deep Learning Specialization from Coursera on the Titanic Survival data set from kaggle.\nPlease refer to the Medium post for instructions how to adjust the network to suit your binary classification problem.\nhttps://medium.com/p/b77edbc83816/edit\n\nIn order to use the github repo, please enroll into the Coursera course on Deep Learning and Neural Networks.\nhttps://www.coursera.org/learn/neural-networks-deep-learning\nYou can find the \"Deep Neural Network Application - v3\" and \"dnn_utils_v2.py\" in the programming assignments from week 4.\nDownload both notebooks and save them locally. If you're using the code from this repo, please copy and paste the code from the L_layer_model function into the corresponding cell.\n\nYou can download the Titanic Survival data sets from kaggle here:\nhttps://www.kaggle.com/c/titanic/data\n\nAll rights of this course belong to Coursera and deeplearning.ai.\n"
 },
 {
  "repo": "carlaprv/workshop-machine-learning-titanic",
  "language": "Jupyter Notebook",
  "readme_contents": "# ML Workshop: exploring survival on the Titanic\n\nThis Jupyter Notebook was used to give a workshop about Machine Learning with Python in Brazil with the support of [WoMakersCode Community](https://github.com/WoMakersCode).\n\nThe Notebook is a Machine Learning Solution to a Kaggle Competition. You can read more about the challenge here: [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data)\n\nYou can access the slides used in the workshop here: [Workshop Slides Presentation](https://docs.google.com/presentation/d/1eCrsY8pIv_QuNHgESFZ19uwhmPx1EcADDQRdX-3mWv8/edit?usp=sharing)\n\n## Getting Started\n\nThese instructions will get you a copy of the project up and running on your local machine for development and learning purposes. \n\n### Prerequisites\n\n* Python 3.7\n* Jupyter Notebook (I recommend using [Anaconda](https://www.anaconda.com/download/))\n\n### Download/Clone repository\n\nDownload or clone this repository to your machine (it includes jupyter notebook and titanic)\n\n### Start your own ```Jupyter Notebook```\n\n**Windows**\n\nInside your project folder with all the files from this repository, open cmd and run the command ```jupyter notebook```. Your cmd should looke like the image below:\n\n![Image of cmd running Jupyter](https://i.imgur.com/imfl23W.png)\n\nNow, you are all started and it should open a new tab on your browser running Jupyter.\n\n## Authors\n\n* **Carla Vieira** - [@carlaprvieira](https://twitter.com/carlaprvieira)\n\n## Acknowledgments\n\n* [Megan Risdal notebook on Kaggle](https://www.kaggle.com/mrisdal/exploring-survival-on-the-titanic)\n* [Classificando M\u00fasicas do Spotify com SVM (Com C\u00f3digos Python)](http://minerandodados.com.br/index.php/2018/04/04/spotify-svm-python/)\n* [Train/Test Split and Cross Validation in Python](https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6)\n* [Cross-validation: evaluating estimator performance](https://scikit-learn.org/stable/modules/cross_validation.html)\n\n"
 },
 {
  "repo": "nezwulf/Logistic-titanic",
  "language": "R",
  "readme_contents": "# Logistic-titanic\nLogistic regression using CARET package for a class assignment\n"
 },
 {
  "repo": "TarekDib03/titanic-EDA",
  "language": null,
  "readme_contents": "# titanic-EDA\nThe notebook performs exploratory data analysis on the titanic data set. The data set was downloaded from kaggle \nwebsite. Please [click here][id] to download the train data set if you want to \nfollow along. \n\nMultiple packages were used in the notenook. These packages were imported into python 2.7.3. The packages used are:\n\n* numpy\n* pandas\n* matplotlib\n* seaborn\n\nThe first two packages are for combutation and data analysis, and the other two packages are for data visulaization.\n\n[id]: https://www.kaggle.com/c/titanic/data\n\nPredicting the likelikelihood of survival using scikit learn package will be discussed in future posts.\n"
 },
 {
  "repo": "lucko515/clustering-python",
  "language": "Jupyter Notebook",
  "readme_contents": "# Clustering using Python\n\nIn this repository you can find mini-projects that explains clustering Machine Learning tecnihuqes. All projects are done in Python programming languange.\n\n\n## More information\n\nEach mini project has its own README file where you can find more information about project itself.\n\n### Custom algorithms\n\nPlease find folder called **algorithms_custom** where you can inspect KMeans, Means Shift.. algorithms wrote from scratch using only numpy. This can help you to understand intuition behind blackbox version of algorithm in Scikit-learn library. \n\n## Lincese\n\nEach project has MIT Lincese.\n"
 },
 {
  "repo": "Chinmayrane16/Titanic-Survival-In-Depth-Analysis",
  "language": "Jupyter Notebook",
  "readme_contents": "# Titanic-Survival-In-Depth-Analysis\n**Problem Statement**\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n\n**The Titanic: Machine Learning from Disaster competiton (Overview)**\nThe purpose of this project was to use DL and ML implementations on a dataset to help and analyise a titanic survival scenario.\n* With data being provided of varoius passengers traveling on the ship I have used libraries like _Numpy , Pandas_ to manipulate , explore and analyze the data.\n* Also used Libraries like _Matplotlib and Seaborn_ to Visualise the data. \n* Lastly I have used various machine learning models to make predictions on the formerly cleaned and preprocessed data. \n* Then I used _GridSearchCV_ to optimise the parameters of the various models.\n"
 },
 {
  "repo": "Swalloow/pyspark-ml-examples",
  "language": "Jupyter Notebook",
  "readme_contents": "# pyspark-ml-examples\nSpark ML Tutorial and Examples for Beginners\n\n## How to start\nUse Docker Images : https://hub.docker.com/r/jupyter/pyspark-notebook/\n\n```\ndocker run -it --rm -p 8888:8888 --name jupyter \\\n-v /YOUR_DOWNLOAD_PATH/pyspark-ml-examples:/home/jovyan jupyter/pyspark-notebook start-notebook.sh\n```\n\n## Index\n- **spark-ml-starter**: EDA, Preprocessing, Modeling, Evaluation, Tuning\n- **spark-ml-gbt-pipeline**: GBTClassifier, Pipeline\n- **spark-ml-recommendation-explicit**: Movie recommendation with Explicit Collaborative Filtering\n- **spark-ml-recommendation-implicit**: Music recommendation with Implicit Collaborative Filtering\n- **spark-ml-clustering**: Anomaly Detection in Network Trac with K-means Clustering\n\n## Dataset\n- Kaggle Titanic Dataset: https://www.kaggle.com/c/titanic/data\n- MovieLens Dataset: https://grouplens.org/datasets/movielens/100k/\n- Last.fm Music Dataset: http://www.dtic.upf.edu/~ocelma/MusicRecommendationDataset/lastfm-1K.html\n- KDD Cup 1999 Dataset: http://www.kdd.org/kdd-cup/view/kdd-cup-1999/Data\n"
 },
 {
  "repo": "whydna/Deep-Learning-Movie-Scene-Detection",
  "language": "Python",
  "readme_contents": "# Deep-Learning-Movie-Scene-Detection\nA project that trains a neural network to recognize certain types of movie scenes (i.e: every drinking scene from Mad Men, every kissing scene from the Titanic, etc.)\n"
 },
 {
  "repo": "Arctanxy/Titanic_Voting_Classifier",
  "language": "Python",
  "readme_contents": "# Titanic_Voting_Classifier\n\n\u6cf0\u5766\u5c3c\u514b\u751f\u5b58\u9884\u6d4b\u7684\u5c0f\u4f8b\u5b50\n\n\u4f7f\u7528\u4e86\u591a\u4e2a\u6a21\u578b\u6295\u7968\u8868\u51b3\u7684\u601d\u8def\u6784\u5efa\n\n\u8be6\u60c5\u8bf7\u53c2\u8003\uff1a[\u521d\u5b66\u8005\u5982\u4f55\u73a9\u8f6cKaggle\u2014\u2014Titanic\u751f\u5b58\u9884\u6d4b](https://www.jianshu.com/p/3e54fd56e78f)\n"
 },
 {
  "repo": "abhinavsagar/Kaggle-Titanic-Solution",
  "language": "Jupyter Notebook",
  "readme_contents": "# Kaggle-Titanic-Solution\nHow I scored in the top 1% of Kaggle's Titanic Machine Learning Challenge\n\nDownload the data from [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic/data).\n\nCheck out the corresponding medium blog post [https://towardsdatascience.com/how-i-scored-in-the-top-1-of-kaggles-titanic-machine-learning-challenge-7716386ba298](https://towardsdatascience.com/how-i-scored-in-the-top-1-of-kaggles-titanic-machine-learning-challenge-7716386ba298).\n"
 },
 {
  "repo": "Rudo-erek/decision-tree",
  "language": "Python",
  "readme_contents": "# decision-tree\n\u57fa\u4e8ekaggle\u4e0aTitanic\u6570\u636e\u96c6\u5b9e\u73b0\u7684ID3\u3001C4.5\u3001CART\u548cCART\u526a\u679d\u7b97\u6cd5\n"
 },
 {
  "repo": "pcsanwald/kaggle-titanic",
  "language": "Clojure",
  "readme_contents": "# titanic\n\nA Clojure library that generates the same CSV file as done by excel, in Kaggle's \ntitanic example: http://www.kaggle.com/c/titanic-gettingStarted/details/getting-started-with-excel\n\n## Usage\n\nlein repl\n(use 'titanic.core)\n(spit \"prediction.csv\" csv)\n\n## License\n\nCopyright \u00a9 2012 Paul Sanwald\n\nDistributed under the Eclipse Public License, the same as Clojure.\n"
 },
 {
  "repo": "wehrley/Kaggle_Titanic",
  "language": null,
  "readme_contents": "Kaggle Contest: Predicting Survival on the Titanic\n=========\n[From Kaggle's competition details]:\n\n> The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew.  This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\n> One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.  Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\n**Objective**: Apply the tools of machine learning to predict which passengers survived the tragedy.\n\nVariable Descriptions\n----\n\nvariable name | description\n--- | ---\nsurvival | Survival (0 = No; 1 = Yes)\npclass | Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd) \nname | Name \nsex | Sex\nage | Age\nsibsb | Number of Siblings/Spouses Aboard \nparch | Number of Parents/Children Aboard \nticket | Ticket Number \nfare | Passenger Fare \ncabin | Cabin \nembarked | Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n\nSpecial Notes\n----\n*Pclass* is a proxy for socio-economic status (SES) [1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower]\n\n*Age* is in years; fractional if age is less than one (1).  If age is estimated, it is in the form xx.5\n\nWith respect to the family relation variables (i.e. sibsp and parch) some relations were ignored.  Here are the definitions used for *sibsp* and *parch*.\n\n**Sibling**:  Brother, Sister, Stepbrother, or Stepsister of Passenger Aboard Titanic\n\n**Spouse**:   Husband or Wife of Passenger Aboard Titanic (Mistresses and Fiances Ignored)\n\n**Parent**:   Mother or Father of Passenger Aboard Titanic\n\n**Child**:    Son, Daughter, Stepson, or Stepdaughter of Passenger Aboard Titanic\n\nOther family relatives excluded from this study include cousins, nephews/nieces, aunts/uncle, and in-laws.  Some children traveled only with a nanny, therefore parch=0 for them.  Some traveled with very close friends or neighbors in a village; however, the definitions do not support such relations.\n\n[From Kaggle's competition details]:http://www.kaggle.com/c/titanic-gettingStarted\n    "
 },
 {
  "repo": "masumrumi/Titanic_Kaggle",
  "language": "Jupyter Notebook",
  "readme_contents": "\n<img src=\"http://data.freehdw.com/ships-titanic-vehicles-best.jpg\"  Width=\"800\">\n\n<a id=\"introduction\" ></a><br>\nThis kernel is for all aspiring data scientists to learn from and to review their knowledge. We will have a detailed statistical analysis of Titanic data set along with Machine learning model implementation. I am super excited to share my first kernel with the Kaggle community. As I go on in this journey and learn new topics, I will incorporate them with each new updates. So, check for them and please <b>leave a comment</b> if you have any suggestions to make them better!! Going back to the topics of this kernel, I will do more in-depth visualizations to explain the data, and the machine learning classifiers will be used to predict passenger survival status. So, let's get started.\n\n\n\n<div style=\"text-align: left\">This notebook goes indepth in classifier models since we are trying to solve a classifier problem here. If you want to learn more about Advanced Regression models, please check out <a href=\"https://www.kaggle.com/masumrumi/a-stats-analysis-and-ml-workflow-of-house-pricing\">this</a> kernel.</div>\n\n\n# Kernel Goals\n<a id=\"aboutthiskernel\"></a>\n***\nThere are three primary goals of this kernel.\n- <b>Do a statistical analysis</b> of how some group of people was survived more than others. \n- <b>Do an exploratory data analysis(EDA)</b> of titanic with visualizations and storytelling.  \n- <b>Predict</b>: Use machine learning classification models to predict the chances of passengers survival.\n\nP.S. If you want to learn more about regression models, try this [kernel](https://www.kaggle.com/masumrumi/a-stats-analysis-and-ml-workflow-of-house-pricing/edit/run/9585160). \n\n# Part 1: Importing Necessary Libraries and datasets\n***\n<a id=\"import_libraries**\"></a>\n## 1a. Loading libraries\n\nPython is a fantastic language with a vibrant community that produces many amazing libraries. I am not a big fan of importing everything at once for the newcomers. So, I am going to introduce a few necessary libraries for now, and as we go on, we will keep unboxing new libraries when it seems appropriate. \n\n\n```python\n# Import necessary modules for data analysis and data visualization. \n# Data analysis modules\n# Pandas is probably the most popular and important modules for any work related to data management. \nimport pandas as pd\n\n\"\"\"# numpy is a great library for doing mathmetical operations. \nimport numpy as np\n\n# Some visualization libraries\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n## Some other snippit of codes to get the setting right \n## This is so that the chart created by matplotlib can be shown in the jupyter notebook. \n%matplotlib inline \n%config InlineBackend.figure_format = 'retina' ## This is preferable for retina display. \n\"\"\"\nimport warnings ## importing warnings library. \nwarnings.filterwarnings('ignore') ## Ignore warning\n\nimport os ## imporing os\nprint(os.listdir(\"../input/\")) \n```\n\n    ['gender_submission.csv', 'train.csv', 'test.csv']\n\n\n## 1b. Loading Datasets\n<a id=\"load_data\"></a>\n***\n\nAfter loading the necessary modules, we need to import the datasets. Many of the business problems usually come with a tremendous amount of messy data. We extract those data from many sources. I am hoping to write about that in a different kernel. For now, we are going to work with a less complicated and quite popular machine learning dataset.\n\n\n```python\n## Importing the datasets\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n```\n\nYou are probably wondering why two datasets? Also, Why have I named it \"train\" and \"test\"?  To explain that I am going to give you an overall picture of the supervised machine learning process. \n\n\"Machine Learning\" is simply \"Machine\" and \"Learning\". Nothing more and nothing less. In a supervised machine learning process, we are giving machine/computer/models specific inputs or data(text/number/image/audio) to learn from aka we are training the machine to learn certain thing based on the data and the output. Now, how do we know what we are teaching is what they are learning? That is where the test set comes to play. We withhold part of the data where we know the output/result of the algorithms, and we use this data to test the trained machine learning model.  We then compare the outcomes to determine machines performance. If you are a bit confused thats okay. I will explain more as we keep reading. Let's take a look at sample datasets.\n\n## 1c. A Glimpse of the Datasets. \n<a id=\"glimpse\"></a>\n***\n\n**> Sample train dataset**\n\n\n```python\n## Take a look at the overview of the dataset. \ntrain.sample(5)\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>720</td>\n      <td>721</td>\n      <td>1</td>\n      <td>2</td>\n      <td>Harper, Miss. Annie Jessie \"Nina\"</td>\n      <td>female</td>\n      <td>6.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>248727</td>\n      <td>33.0000</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <td>331</td>\n      <td>332</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Partner, Mr. Austen</td>\n      <td>male</td>\n      <td>45.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>113043</td>\n      <td>28.5000</td>\n      <td>C124</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <td>621</td>\n      <td>622</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Kimball, Mr. Edwin Nelson Jr</td>\n      <td>male</td>\n      <td>42.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>11753</td>\n      <td>52.5542</td>\n      <td>D19</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <td>138</td>\n      <td>139</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Osen, Mr. Olaf Elon</td>\n      <td>male</td>\n      <td>16.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7534</td>\n      <td>9.2167</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <td>353</td>\n      <td>354</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Arnold-Franchi, Mr. Josef</td>\n      <td>male</td>\n      <td>25.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>349237</td>\n      <td>17.8000</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n**> Sample test dataset**\n\n\n```python\ntest.sample(5)\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>23</td>\n      <td>915</td>\n      <td>1</td>\n      <td>Williams, Mr. Richard Norris II</td>\n      <td>male</td>\n      <td>21.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>PC 17597</td>\n      <td>61.3792</td>\n      <td>NaN</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <td>257</td>\n      <td>1149</td>\n      <td>3</td>\n      <td>Niklasson, Mr. Samuel</td>\n      <td>male</td>\n      <td>28.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>363611</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>916</td>\n      <td>1</td>\n      <td>Ryerson, Mrs. Arthur Larned (Emily Maria Borie)</td>\n      <td>female</td>\n      <td>48.0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>PC 17608</td>\n      <td>262.3750</td>\n      <td>B57 B59 B63 B66</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <td>226</td>\n      <td>1118</td>\n      <td>3</td>\n      <td>Asplund, Mr. Johan Charles</td>\n      <td>male</td>\n      <td>23.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>350054</td>\n      <td>7.7958</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>912</td>\n      <td>1</td>\n      <td>Rothschild, Mr. Martin</td>\n      <td>male</td>\n      <td>55.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17603</td>\n      <td>59.4000</td>\n      <td>NaN</td>\n      <td>C</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\nThis is a sample of train and test dataset. Lets find out a bit more about the train and test dataset. \n\n\n```python\nprint (\"The shape of the train data is (row, column):\"+ str(train.shape))\nprint (train.info())\nprint (\"The shape of the test data is (row, column):\"+ str(test.shape))\nprint (test.info())\n```\n\n    The shape of the train data is (row, column):(891, 12)\n    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 891 entries, 0 to 890\n    Data columns (total 12 columns):\n    PassengerId    891 non-null int64\n    Survived       891 non-null int64\n    Pclass         891 non-null int64\n    Name           891 non-null object\n    Sex            891 non-null object\n    Age            714 non-null float64\n    SibSp          891 non-null int64\n    Parch          891 non-null int64\n    Ticket         891 non-null object\n    Fare           891 non-null float64\n    Cabin          204 non-null object\n    Embarked       889 non-null object\n    dtypes: float64(2), int64(5), object(5)\n    memory usage: 83.7+ KB\n    None\n    The shape of the test data is (row, column):(418, 11)\n    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 418 entries, 0 to 417\n    Data columns (total 11 columns):\n    PassengerId    418 non-null int64\n    Pclass         418 non-null int64\n    Name           418 non-null object\n    Sex            418 non-null object\n    Age            332 non-null float64\n    SibSp          418 non-null int64\n    Parch          418 non-null int64\n    Ticket         418 non-null object\n    Fare           417 non-null float64\n    Cabin          91 non-null object\n    Embarked       418 non-null object\n    dtypes: float64(2), int64(4), object(5)\n    memory usage: 36.0+ KB\n    None\n\n\n ## 1d. About This Dataset\n<a id=\"aboutthisdataset\"></a>\n***\nThe data has split into two groups:\n\n- training set (train.csv)\n- test set (test.csv)\n\n***The training set includes our target variable(dependent variable), passenger survival status***(also known as the ground truth from the Titanic tragedy) along with other independent features like gender, class, fare, and Pclass. \n\nThe test set should be used to see how well our model performs on unseen data. When we say unseen data, we mean that the algorithm or machine learning models have no relation to the test data. We do not want to use any part of the test data in any way to modify our algorithms; Which are the reasons why we clean our test data and train data separately. ***The test set does not provide passengers survival status***. We are going to use our model to predict passenger survival status.\n\nNow let's go through the features and describe a little. There is a couple of different type of variables, They are...\n\n***\n**Categorical:**\n- **Nominal**(variables that have two or more categories, but which do not have an intrinsic order.)\n   > - **Cabin**\n   > - **Embarked**(Port of Embarkation)\n            C(Cherbourg)\n            Q(Queenstown) \n            S(Southampton)\n        \n- **Dichotomous**(Nominal variable with only two categories)\n   > - **Sex**\n            Female\n            Male\n- **Ordinal**(variables that have two or more categories just like nominal variables. Only the categories can also be ordered or ranked.)\n   > - **Pclass** (A proxy for socio-economic status (SES)) \n            1(Upper)\n            2(Middle) \n            3(Lower)\n***\n**Numeric:**\n- **Discrete**\n  >  - **Passenger ID**(Unique identifing # for each passenger)\n  >  - **SibSp**\n  >  - **Parch**\n  >  - **Survived** (Our outcome or dependent variable)\n            0\n            1\n- **Continous**\n>  - **Age**\n>  - **Fare**\n***\n**Text Variable**\n> - **Ticket** (Ticket number for passenger.)\n> - **Name**(  Name of the passenger.) \n\n\n\n## 1e. Tableau Visualization of the Data\n<a id='tableau_visualization'></a>\n***\nI have incorporated a tableau visualization below of the training data. This visualization... \n* is for us to have an overview and play around with the dataset. \n* is done without making any changes(including Null values) to any features of the dataset.\n***\nLet's get a better perspective of the dataset through this visualization.\n\n\n\n```python\n%%HTML\n<div class='tableauPlaceholder' id='viz1516349898238' style='position: relative'><noscript><a href='#'><img alt='An Overview of Titanic Training Dataset ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ti&#47;Titanic_data_mining&#47;Dashboard1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='Titanic_data_mining&#47;Dashboard1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ti&#47;Titanic_data_mining&#47;Dashboard1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1516349898238');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>\n```\n\n\n<div class='tableauPlaceholder' id='viz1516349898238' style='position: relative'><noscript><a href='#'><img alt='An Overview of Titanic Training Dataset ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ti&#47;Titanic_data_mining&#47;Dashboard1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='Titanic_data_mining&#47;Dashboard1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ti&#47;Titanic_data_mining&#47;Dashboard1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1516349898238');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>\n\n\n\nWe want to see how the left bar(with green and red) changes when we filter out unique values of a feature. We can use multiple filters to see if there are any correlations among them. For example, if we click on **upper** and **Female** tab, we would see that green color dominates the bar with a ratio of 91:3 survived and non survived female passengers; a 97% survival rate for females. We can reset the filters by clicking anywhere in the whilte space. The age distribution chart on top provides us with some more info such as, what was the age range of those three unlucky females as the red color give away the unsurvived once. If you would like to check out some of my other tableau charts, please click [here.](https://public.tableau.com/profile/masum.rumi#!/)\n\n# Part 2: Overview and Cleaning the Data\n<a id=\"cleaningthedata\"></a>\n***\n## 2a. Overview\n\nDatasets in the real world are often messy, However, this dataset is almost clean. Lets analyze and see what we have here.\n\n\n```python\n## saving passenger id in advance in order to submit later. \npassengerid = test.PassengerId\n## We will drop PassengerID and Ticket since it will be useless for our data. \n#train.drop(['PassengerId'], axis=1, inplace=True)\n#test.drop(['PassengerId'], axis=1, inplace=True)\n\nprint (train.info())\nprint (\"*\"*40)\nprint (test.info())\n```\n\n    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 891 entries, 0 to 890\n    Data columns (total 12 columns):\n    PassengerId    891 non-null int64\n    Survived       891 non-null int64\n    Pclass         891 non-null int64\n    Name           891 non-null object\n    Sex            891 non-null object\n    Age            714 non-null float64\n    SibSp          891 non-null int64\n    Parch          891 non-null int64\n    Ticket         891 non-null object\n    Fare           891 non-null float64\n    Cabin          204 non-null object\n    Embarked       889 non-null object\n    dtypes: float64(2), int64(5), object(5)\n    memory usage: 83.7+ KB\n    None\n    ****************************************\n    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 418 entries, 0 to 417\n    Data columns (total 11 columns):\n    PassengerId    418 non-null int64\n    Pclass         418 non-null int64\n    Name           418 non-null object\n    Sex            418 non-null object\n    Age            332 non-null float64\n    SibSp          418 non-null int64\n    Parch          418 non-null int64\n    Ticket         418 non-null object\n    Fare           417 non-null float64\n    Cabin          91 non-null object\n    Embarked       418 non-null object\n    dtypes: float64(2), int64(4), object(5)\n    memory usage: 36.0+ KB\n    None\n\n\nIt looks like, the features have unequal amount of data entries for every column and they have many different types of variables. This can happen for the following reasons...\n* We may have missing values in our features.\n* We may have categorical features. \n* We may have alphanumerical or/and text features. \n\n\n## 2b. Dealing with Missing values\n<a id=\"dealwithnullvalues\"></a>\n***\n**Missing values in *train* dataset.**\n\n\n```python\n# Let's write a functin to print the total percentage of the missing values.(this can be a good exercise for beginners to try to write simple functions like this.)\ndef missing_percentage(df):\n    \"\"\"This function takes a DataFrame(df) as input and returns two columns, total missing values and total missing values percentage\"\"\"\n    total = df.isnull().sum().sort_values(ascending = False)\n    percent = round(df.isnull().sum().sort_values(ascending = False)/len(df)*100,2)\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])\n```\n\n\n```python\nmissing_percentage(train)\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Total</th>\n      <th>Percent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Cabin</td>\n      <td>687</td>\n      <td>77.10</td>\n    </tr>\n    <tr>\n      <td>Age</td>\n      <td>177</td>\n      <td>19.87</td>\n    </tr>\n    <tr>\n      <td>Embarked</td>\n      <td>2</td>\n      <td>0.22</td>\n    </tr>\n    <tr>\n      <td>Fare</td>\n      <td>0</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <td>Ticket</td>\n      <td>0</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <td>Parch</td>\n      <td>0</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <td>SibSp</td>\n      <td>0</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <td>Sex</td>\n      <td>0</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <td>Name</td>\n      <td>0</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <td>Pclass</td>\n      <td>0</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <td>Survived</td>\n      <td>0</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <td>PassengerId</td>\n      <td>0</td>\n      <td>0.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n**Missing values in *test* set.**\n\n\n```python\nmissing_percentage(test)\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Total</th>\n      <th>Percent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Cabin</td>\n      <td>327</td>\n      <td>78.23</td>\n    </tr>\n    <tr>\n      <td>Age</td>\n      <td>86</td>\n      <td>20.57</td>\n    </tr>\n    <tr>\n      <td>Fare</td>\n      <td>1</td>\n      <td>0.24</td>\n    </tr>\n    <tr>\n      <td>Embarked</td>\n      <td>0</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <td>Ticket</td>\n      <td>0</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <td>Parch</td>\n      <td>0</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <td>SibSp</td>\n      <td>0</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <td>Sex</td>\n      <td>0</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <td>Name</td>\n      <td>0</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <td>Pclass</td>\n      <td>0</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <td>PassengerId</td>\n      <td>0</td>\n      <td>0.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\nWe see that in both **train**, and **test** dataset have missing values. Let's make an effort to fill these missing values starting with \"Embarked\" feature. \n\n### Embarked feature\n***\n\n\n```python\ndef percent_value_counts(df, feature):\n    \"\"\"This function takes in a dataframe and a column and finds the percentage of the value_counts\"\"\"\n    percent = pd.DataFrame(round(df.loc[:,feature].value_counts(dropna=False, normalize=True)*100,2))\n    ## creating a df with th\n    total = pd.DataFrame(df.loc[:,feature].value_counts(dropna=False))\n    ## concating percent and total dataframe\n\n    total.columns = [\"Total\"]\n    percent.columns = ['Percent']\n    return pd.concat([total, percent], axis = 1)\n    \n```\n\n\n```python\npercent_value_counts(train, 'Embarked')\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Total</th>\n      <th>Percent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>S</td>\n      <td>644</td>\n      <td>72.28</td>\n    </tr>\n    <tr>\n      <td>C</td>\n      <td>168</td>\n      <td>18.86</td>\n    </tr>\n    <tr>\n      <td>Q</td>\n      <td>77</td>\n      <td>8.64</td>\n    </tr>\n    <tr>\n      <td>NaN</td>\n      <td>2</td>\n      <td>0.22</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\nIt looks like there are only two null values( ~ 0.22 %) in the Embarked feature, we can replace these with the mode value \"S\". However, let's dig a little deeper. \n\n**Let's see what are those two null values**\n\n\n```python\ntrain[train.Embarked.isnull()]\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>61</td>\n      <td>62</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Icard, Miss. Amelie</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>113572</td>\n      <td>80.0</td>\n      <td>B28</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <td>829</td>\n      <td>830</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Stone, Mrs. George Nelson (Martha Evelyn)</td>\n      <td>female</td>\n      <td>62.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>113572</td>\n      <td>80.0</td>\n      <td>B28</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\nWe may be able to solve these two missing values by looking at other independent variables of the two raws. Both passengers paid a fare of $80, are of Pclass 1 and female Sex. Let's see how the **Fare** is distributed among all **Pclass** and **Embarked** feature values\n\n\n```python\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style('darkgrid')\nfig, ax = plt.subplots(figsize=(16,12),ncols=2)\nax1 = sns.boxplot(x=\"Embarked\", y=\"Fare\", hue=\"Pclass\", data=train, ax = ax[0]);\nax2 = sns.boxplot(x=\"Embarked\", y=\"Fare\", hue=\"Pclass\", data=test, ax = ax[1]);\nax1.set_title(\"Training Set\", fontsize = 18)\nax2.set_title('Test Set',  fontsize = 18)\n\n\n# ## Fixing legends\n# leg_1 = ax1.get_legend()\n# leg_1.set_title(\"PClass\")\n# legs = leg_1.texts\n# legs[0].set_text('Upper')\n# legs[1].set_text('Middle')\n# legs[2].set_text('Lower')\n\nfig.show()\n```\n\n\n![png](kernel_files/kernel_36_0.png)\n\n\nHere, in both training set and test set, the average fare closest to $80 are in the <b>C</b> Embarked values. So, let's fill in the missing values as \"C\" \n\n\n```python\n## Replacing the null values in the Embarked column with the mode. \ntrain.Embarked.fillna(\"C\", inplace=True)\n```\n\n### Cabin Feature\n***\n\n\n```python\nprint(\"Train Cabin missing: \" + str(train.Cabin.isnull().sum()/len(train.Cabin)))\nprint(\"Test Cabin missing: \" + str(test.Cabin.isnull().sum()/len(test.Cabin)))\n```\n\n    Train Cabin missing: 0.7710437710437711\n    Test Cabin missing: 0.7822966507177034\n\n\nApproximately 77% of Cabin feature is missing in the training data and 78% missing on the test data. \nWe have two choices, \n* we can either get rid of the whole feature, or \n* we can brainstorm a little and find an appropriate way to put them in use. For example, We may say passengers with cabin record had a higher socio-economic-status then others. We may also say passengers with cabin record were more likely to be taken into consideration when loading into the boat.\n\nLet's combine train and test data first and for now, will assign all the null values as **\"N\"**\n\n\n```python\n## Concat train and test into a variable \"all_data\"\nsurvivers = train.Survived\n\ntrain.drop([\"Survived\"],axis=1, inplace=True)\n\nall_data = pd.concat([train,test], ignore_index=False)\n\n## Assign all the null values to N\nall_data.Cabin.fillna(\"N\", inplace=True)\n```\n\nAll the cabin names start with an English alphabet following by multiple digits. It seems like there are some passengers that had booked multiple cabin rooms in their name. This is because many of them travelled with family. However, they all seem to book under the same letter followed by different numbers. It seems like there is a significance with the letters rather than the numbers. Therefore, we can group these cabins according to the letter of the cabin name. \n\n\n```python\nall_data.Cabin = [i[0] for i in all_data.Cabin]\n```\n\nNow let's look at the value counts of the cabin features and see how it looks. \n\n\n```python\npercent_value_counts(all_data, \"Cabin\")\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Total</th>\n      <th>Percent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>N</td>\n      <td>1014</td>\n      <td>77.46</td>\n    </tr>\n    <tr>\n      <td>C</td>\n      <td>94</td>\n      <td>7.18</td>\n    </tr>\n    <tr>\n      <td>B</td>\n      <td>65</td>\n      <td>4.97</td>\n    </tr>\n    <tr>\n      <td>D</td>\n      <td>46</td>\n      <td>3.51</td>\n    </tr>\n    <tr>\n      <td>E</td>\n      <td>41</td>\n      <td>3.13</td>\n    </tr>\n    <tr>\n      <td>A</td>\n      <td>22</td>\n      <td>1.68</td>\n    </tr>\n    <tr>\n      <td>F</td>\n      <td>21</td>\n      <td>1.60</td>\n    </tr>\n    <tr>\n      <td>G</td>\n      <td>5</td>\n      <td>0.38</td>\n    </tr>\n    <tr>\n      <td>T</td>\n      <td>1</td>\n      <td>0.08</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\nSo, We still haven't done any effective work to replace the null values. Let's stop for a second here and think through how we can take advantage of some of the other features here.  \n* We can use the average of the fare column We can use pythons ***groupby*** function to get the mean fare of each cabin letter. \n\n\n```python\nall_data.groupby(\"Cabin\")['Fare'].mean().sort_values()\n```\n\n\n\n\n    Cabin\n    G     14.205000\n    F     18.079367\n    N     19.132707\n    T     35.500000\n    A     41.244314\n    D     53.007339\n    E     54.564634\n    C    107.926598\n    B    122.383078\n    Name: Fare, dtype: float64\n\n\n\n\n```python\nwith_N = all_data[all_data.Cabin == \"N\"]\n\nwithout_N = all_data[all_data.Cabin != \"N\"]\n\nall_data.groupby(\"Cabin\")['Fare'].mean().sort_values()\n```\n\n\n\n\n    Cabin\n    G     14.205000\n    F     18.079367\n    N     19.132707\n    T     35.500000\n    A     41.244314\n    D     53.007339\n    E     54.564634\n    C    107.926598\n    B    122.383078\n    Name: Fare, dtype: float64\n\n\n\nNow, these means can help us determine the unknown cabins, if we compare each unknown cabin rows with the given mean's above. Let's write a simple function so that we can give cabin names based on the means. \n\n\n```python\ndef cabin_estimator(i):\n    \"\"\"Grouping cabin feature by the first letter\"\"\"\n    a = 0\n    if i<16:\n        a = \"G\"\n    elif i>=16 and i<27:\n        a = \"F\"\n    elif i>=27 and i<38:\n        a = \"T\"\n    elif i>=38 and i<47:\n        a = \"A\"\n    elif i>= 47 and i<53:\n        a = \"E\"\n    elif i>= 53 and i<54:\n        a = \"D\"\n    elif i>=54 and i<116:\n        a = 'C'\n    else:\n        a = \"B\"\n    return a\n    \n```\n\nLet's apply <b>cabin_estimator</b> function in each unknown cabins(cabin with <b>null</b> values). Once that is done we will separate our train and test to continue towards machine learning modeling. \n\n\n```python\n##applying cabin estimator function. \nwith_N['Cabin'] = with_N.Fare.apply(lambda x: cabin_estimator(x))\n\n## getting back train. \nall_data = pd.concat([with_N, without_N], axis=0)\n\n## PassengerId helps us separate train and test. \nall_data.sort_values(by = 'PassengerId', inplace=True)\n\n## Separating train and test from all_data. \ntrain = all_data[:891]\n\ntest = all_data[891:]\n\n# adding saved target variable with train. \ntrain['Survived'] = survivers\n```\n\n### Fare Feature\n***\nIf you have paid attention so far, you know that there is only one missing value in the fare column. Let's have it. \n\n\n```python\ntest[test.Fare.isnull()]\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>152</td>\n      <td>1044</td>\n      <td>3</td>\n      <td>Storey, Mr. Thomas</td>\n      <td>male</td>\n      <td>60.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3701</td>\n      <td>NaN</td>\n      <td>B</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\nHere, We can take the average of the **Fare** column to fill in the NaN value. However, that might not be the best way to fill in this value. We can be a little more specific and take the average of the values where**Pclass** is ***3***, **Sex** is ***male*** and **Embarked** is ***S***\n\n\n```python\nmissing_value = test[(test.Pclass == 3) & (test.Embarked == \"S\") & (test.Sex == \"male\")].Fare.mean()\n## replace the test.fare null values with test.fare mean\ntest.Fare.fillna(missing_value, inplace=True)\n```\n\n### Age Feature\n***\nWe know that the feature \"Age\" is the one with most missing values, let's see it in terms of percentage. \n\n\n```python\nprint (\"Train age missing value: \" + str((train.Age.isnull().sum()/len(train))*100)+str(\"%\"))\nprint (\"Test age missing value: \" + str((test.Age.isnull().sum()/len(test))*100)+str(\"%\"))\n```\n\n    Train age missing value: 19.865319865319865%\n    Test age missing value: 20.574162679425836%\n\n\nWe will take a different approach since **~20% data in the Age column is missing** in both train and test dataset. The age variable seems to be promising for determining survival rate. Therefore, It would be unwise to replace the missing values with median, mean or mode. We will use machine learning model Random Forest Regressor to impute missing value instead of  Null value. We will keep the age column unchanged for now and work on that in the feature engineering section. \n\n# Part 3. Visualization and Feature Relations\n<a id=\"visualization_and_feature_relations\" ></a>\n***\nBefore we dive into finding relations between different features and our dependent variable(survivor) let us create some assumptions about how the relations may turnout among features.\n\n**Assumptions:**\n- Gender: More female survived than male\n- Pclass: Higher socio-economic status passenger survived more than others. \n- Age: Younger passenger survived more than other passengers. \n- Fare: Passenget with higher fare survived more that other passengers. This can be quite correlated with Pclass. \n\n\nNow, let's see how the features are related to each other by creating some visualizations. \n\n\n\n## 3a. Gender and Survived\n<a id=\"gender_and_survived\"></a>\n***\n\n\n```python\nimport seaborn as sns\npal = {'male':\"green\", 'female':\"Pink\"}\nsns.set(style=\"darkgrid\")\nplt.subplots(figsize = (15,8))\nax = sns.barplot(x = \"Sex\", \n                 y = \"Survived\", \n                 data=train, \n                 palette = pal,\n                 linewidth=5,\n                 order = ['female','male'],\n                 capsize = .05,\n\n                )\n\nplt.title(\"Survived/Non-Survived Passenger Gender Distribution\", fontsize = 25,loc = 'center', pad = 40)\nplt.ylabel(\"% of passenger survived\", fontsize = 15, )\nplt.xlabel(\"Sex\",fontsize = 15);\n\n\n```\n\n\n![png](kernel_files/kernel_63_0.png)\n\n\nThis bar plot above shows the distribution of female and male survived. The ***x_label*** represents **Sex** feature while the ***y_label*** represents the % of **passenger survived**. This bar plot shows that ~74% female passenger survived while only ~19% male passenger survived.\n\n\n```python\npal = {1:\"seagreen\", 0:\"gray\"}\nsns.set(style=\"darkgrid\")\nplt.subplots(figsize = (15,8))\nax = sns.countplot(x = \"Sex\", \n                   hue=\"Survived\",\n                   data = train, \n                   linewidth=4, \n                   palette = pal\n)\n\n## Fixing title, xlabel and ylabel\nplt.title(\"Passenger Gender Distribution - Survived vs Not-survived\", fontsize = 25, pad=40)\nplt.xlabel(\"Sex\", fontsize = 15);\nplt.ylabel(\"# of Passenger Survived\", fontsize = 15)\n\n## Fixing xticks\n#labels = ['Female', 'Male']\n#plt.xticks(sorted(train.Sex.unique()), labels)\n\n## Fixing legends\nleg = ax.get_legend()\nleg.set_title(\"Survived\")\nlegs = leg.texts\nlegs[0].set_text(\"No\")\nlegs[1].set_text(\"Yes\")\nplt.show()\n```\n\n\n![png](kernel_files/kernel_65_0.png)\n\n\nThis count plot shows the actual distribution of male and female passengers that survived and did not survive. It shows that among all the females ~ 230 survived and ~ 70 did not survive. While among male passengers ~110 survived and ~480 did not survive. \n\n**Summary**\n***\n- As we suspected, female passengers have survived at a much better rate than male passengers. \n- It seems about right since females and children were the priority. \n\n## 3b. Pclass and Survived\n<a id=\"pcalss_and_survived\"></a>\n***\n\n\n```python\n\nplt.subplots(figsize = (15,10))\nsns.barplot(x = \"Pclass\", \n            y = \"Survived\", \n            data=train, \n            linewidth=5,\n            capsize = .1\n\n           )\nplt.title(\"Passenger Class Distribution - Survived vs Non-Survived\", fontsize = 25, pad=40)\nplt.xlabel(\"Socio-Economic class\", fontsize = 15);\nplt.ylabel(\"% of Passenger Survived\", fontsize = 15);\nlabels = ['Upper', 'Middle', 'Lower']\n#val = sorted(train.Pclass.unique())\nval = [0,1,2] ## this is just a temporary trick to get the label right. \nplt.xticks(val, labels);\n```\n\n\n![png](kernel_files/kernel_68_0.png)\n\n\n- It looks like ...\n    - ~ 63% first class passenger survived titanic tragedy, while \n    - ~ 48% second class and \n    - ~ only  24% third class passenger survived. \n\n\n\n\n```python\n# Kernel Density Plot\nfig = plt.figure(figsize=(15,8),)\n## I have included to different ways to code a plot below, choose the one that suites you. \nax=sns.kdeplot(train.Pclass[train.Survived == 0] , \n               color='gray',\n               shade=True,\n               label='not survived')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Pclass'] , \n               color='g',\n               shade=True, \n               label='survived', \n              )\nplt.title('Passenger Class Distribution - Survived vs Non-Survived', fontsize = 25, pad = 40)\nplt.ylabel(\"Frequency of Passenger Survived\", fontsize = 15, labelpad = 20)\nplt.xlabel(\"Passenger Class\", fontsize = 15,labelpad =20)\n## Converting xticks into words for better understanding\nlabels = ['Upper', 'Middle', 'Lower']\nplt.xticks(sorted(train.Pclass.unique()), labels);\n```\n\n\n![png](kernel_files/kernel_70_0.png)\n\n\nThis KDE plot is pretty self-explanatory with all the labels and colors. Something I have noticed that some readers might find questionable is that the lower class passengers have survived more than second-class passengers. It is true since there were a lot more third-class passengers than first and second. \n\n**Summary**\n***\nThe first class passengers had the upper hand during the tragedy than second and third. You can probably agree with me more on this, in the next section of visualizations where we look at the distribution of ticket fare and survived column. \n\n## 3c. Fare and Survived\n<a id=\"fare_and_survived\"></a>\n***\n\n\n```python\n# Kernel Density Plot\nfig = plt.figure(figsize=(15,8),)\nax=sns.kdeplot(train.loc[(train['Survived'] == 0),'Fare'] , color='gray',shade=True,label='not survived')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Fare'] , color='g',shade=True, label='survived')\nplt.title('Fare Distribution Survived vs Non Survived', fontsize = 25, pad = 40)\nplt.ylabel(\"Frequency of Passenger Survived\", fontsize = 15, labelpad = 20)\nplt.xlabel(\"Fare\", fontsize = 15, labelpad = 20);\n\n\n```\n\n\n![png](kernel_files/kernel_73_0.png)\n\n\nThis plot shows something impressive..\n- The spike in the plot under 100 dollar represents that a lot of passengers who bought the ticket within that range did not survive. \n- When fare is approximately more than 280 dollars, there is no gray shade which means, either everyone passed that fare point survived or maybe there is an outlier that clouds our judgment. Let's check...\n\n\n```python\ntrain[train.Fare > 280]\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n      <th>Survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>258</td>\n      <td>259</td>\n      <td>1</td>\n      <td>Ward, Miss. Anna</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>PC 17755</td>\n      <td>512.3292</td>\n      <td>B</td>\n      <td>C</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>679</td>\n      <td>680</td>\n      <td>1</td>\n      <td>Cardeza, Mr. Thomas Drake Martinez</td>\n      <td>male</td>\n      <td>36.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>PC 17755</td>\n      <td>512.3292</td>\n      <td>B</td>\n      <td>C</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>737</td>\n      <td>738</td>\n      <td>1</td>\n      <td>Lesurer, Mr. Gustave J</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>PC 17755</td>\n      <td>512.3292</td>\n      <td>B</td>\n      <td>C</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\nAs we assumed, it looks like an outlier with a fare of $512. We sure can delete this point. However, we will keep it for now. \n\n## 3d. Age and Survived\n<a id=\"age_and_survived\"></a>\n***\n\n\n```python\n# Kernel Density Plot\nfig = plt.figure(figsize=(15,8),)\nax=sns.kdeplot(train.loc[(train['Survived'] == 0),'Age'] , color='gray',shade=True,label='not survived')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Age'] , color='g',shade=True, label='survived')\nplt.title('Age Distribution - Surviver V.S. Non Survivors', fontsize = 25, pad = 40)\nplt.xlabel(\"Age\", fontsize = 15, labelpad = 20)\nplt.ylabel('Frequency', fontsize = 15, labelpad= 20);\n```\n\n\n![png](kernel_files/kernel_78_0.png)\n\n\nThere is nothing out of the ordinary about this plot, except the very left part of the distribution. This may hint on the posibility that children and infants were the priority. \n\n## 3e. Combined Feature Relations\n<a id='combined_feature_relations'></a>\n***\nIn this section, we are going to discover more than two feature relations in a single graph. I will try my best to illustrate most of the feature relations. Let's get to it. \n\n\n```python\npal = {1:\"seagreen\", 0:\"gray\"}\ng = sns.FacetGrid(train,size=5, col=\"Sex\", row=\"Survived\", margin_titles=True, hue = \"Survived\",\n                  palette=pal)\ng = g.map(plt.hist, \"Age\", edgecolor = 'white');\ng.fig.suptitle(\"Survived by Sex and Age\", size = 25)\nplt.subplots_adjust(top=0.90)\n\n```\n\n\n![png](kernel_files/kernel_81_0.png)\n\n\nFacetgrid is a great way to visualize multiple variables and their relationships at once. From the chart in section 3a we have a intuation that female passengers had better prority than males during that incident. However, from this facet grid, we can also understand which age range groups survived more than others or were not so lucky\n\n\n```python\ng = sns.FacetGrid(train,size=5, col=\"Sex\", row=\"Embarked\", margin_titles=True, hue = \"Survived\",\n                  palette = pal\n                  )\ng = g.map(plt.hist, \"Age\", edgecolor = 'white').add_legend();\ng.fig.suptitle(\"Survived by Sex and Age\", size = 25)\nplt.subplots_adjust(top=0.90)\n```\n\n\n![png](kernel_files/kernel_83_0.png)\n\n\nThis is another compelling facet grid illustrating four features relationship at once. They are **Embarked, Age, Survived & Sex**. \n* The color illustrates passengers survival status(green represents survived, gray represents not survived)\n* The column represents Sex(left being male, right stands for female)\n* The row represents Embarked(from top to bottom: S, C, Q)\n***\nNow that I have steered out the apparent let's see if we can get some insights that are not so obvious as we look at the data. \n* Most passengers seem to be boarded on Southampton(S).\n* More than 60% of the passengers died boarded on Southampton. \n* More than 60% of the passengers lived boarded on Cherbourg(C).\n* Pretty much every male that boarded on Queenstown(Q) did not survive. \n* There were very few females boarded on Queenstown, however, most of them survived. \n\n\n```python\ng = sns.FacetGrid(train, size=5,hue=\"Survived\", col =\"Sex\", margin_titles=True,\n                palette=pal,)\ng.map(plt.scatter, \"Fare\", \"Age\",edgecolor=\"w\").add_legend()\ng.fig.suptitle(\"Survived by Sex, Fare and Age\", size = 25)\nplt.subplots_adjust(top=0.85)\n```\n\n\n![png](kernel_files/kernel_85_0.png)\n\n\nThis facet grid unveils a couple of interesting insights. Let's find out.\n* The grid above clearly demonstrates the three outliers with Fare of over \\$500. At this point, I think we are quite confident that these outliers should be deleted.\n* Most of the passengers were with in the Fare range of \\$100. \n\n\n```python\n## dropping the three outliers where Fare is over $500 \ntrain = train[train.Fare < 500]\n## factor plot\nsns.factorplot(x = \"Parch\", y = \"Survived\", data = train,kind = \"point\",size = 8)\nplt.title(\"Factorplot of Parents/Children survived\", fontsize = 25)\nplt.subplots_adjust(top=0.85)\n```\n\n\n![png](kernel_files/kernel_87_0.png)\n\n\n**Passenger who traveled in big groups with parents/children had less survival rate than other passengers.**\n\n\n```python\nsns.factorplot(x =  \"SibSp\", y = \"Survived\", data = train,kind = \"point\",size = 8)\nplt.title('Factorplot of Sibilings/Spouses survived', fontsize = 25)\nplt.subplots_adjust(top=0.85)\n```\n\n\n![png](kernel_files/kernel_89_0.png)\n\n\n**While, passenger who traveled in small groups with sibilings/spouses had better changes of survivint than other passengers.**\n\n\n```python\n# Placing 0 for female and \n# 1 for male in the \"Sex\" column. \ntrain['Sex'] = train.Sex.apply(lambda x: 0 if x == \"female\" else 1)\ntest['Sex'] = test.Sex.apply(lambda x: 0 if x == \"female\" else 1)\n```\n\n# Part 4: Statistical Overview\n<a id=\"statisticaloverview\"></a>\n***\n\n![title](https://cdn-images-1.medium.com/max/400/1*hFJ-LI7IXcWpxSLtaC0dfg.png)\n\n**Train info**\n\n\n```python\ntrain.describe()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>count</td>\n      <td>888.000000</td>\n      <td>888.000000</td>\n      <td>888.000000</td>\n      <td>711.000000</td>\n      <td>888.000000</td>\n      <td>888.000000</td>\n      <td>888.000000</td>\n      <td>888.000000</td>\n    </tr>\n    <tr>\n      <td>mean</td>\n      <td>445.618243</td>\n      <td>2.313063</td>\n      <td>0.647523</td>\n      <td>29.675345</td>\n      <td>0.524775</td>\n      <td>0.381757</td>\n      <td>30.582164</td>\n      <td>0.381757</td>\n    </tr>\n    <tr>\n      <td>std</td>\n      <td>257.405474</td>\n      <td>0.834007</td>\n      <td>0.478011</td>\n      <td>14.552495</td>\n      <td>1.104186</td>\n      <td>0.806949</td>\n      <td>41.176366</td>\n      <td>0.486091</td>\n    </tr>\n    <tr>\n      <td>min</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.420000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>25%</td>\n      <td>222.750000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>20.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>7.895800</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>50%</td>\n      <td>445.500000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>28.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>14.454200</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>75%</td>\n      <td>667.250000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>38.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>30.771850</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>max</td>\n      <td>891.000000</td>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>80.000000</td>\n      <td>8.000000</td>\n      <td>6.000000</td>\n      <td>263.000000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\ntrain.describe(include =['O'])\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Ticket</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>count</td>\n      <td>888</td>\n      <td>888</td>\n      <td>888</td>\n      <td>888</td>\n    </tr>\n    <tr>\n      <td>unique</td>\n      <td>888</td>\n      <td>680</td>\n      <td>8</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <td>top</td>\n      <td>Baclini, Miss. Helene Barbara</td>\n      <td>CA. 2343</td>\n      <td>G</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <td>freq</td>\n      <td>1</td>\n      <td>7</td>\n      <td>464</td>\n      <td>644</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\ntrain[['Pclass', 'Survived']].groupby(\"Pclass\").mean().reset_index()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1</td>\n      <td>0.624413</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2</td>\n      <td>0.472826</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3</td>\n      <td>0.242363</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\n# Overview(Survived vs non survied)\nsurvived_summary = train.groupby(\"Survived\")\nsurvived_summary.mean().reset_index()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n      <th>PassengerId</th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0</td>\n      <td>447.016393</td>\n      <td>2.531876</td>\n      <td>0.852459</td>\n      <td>30.626179</td>\n      <td>0.553734</td>\n      <td>0.329690</td>\n      <td>22.117887</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1</td>\n      <td>443.353982</td>\n      <td>1.958702</td>\n      <td>0.315634</td>\n      <td>28.270627</td>\n      <td>0.477876</td>\n      <td>0.466077</td>\n      <td>44.289799</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\nsurvived_summary = train.groupby(\"Sex\")\nsurvived_summary.mean().reset_index()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sex</th>\n      <th>PassengerId</th>\n      <th>Pclass</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0</td>\n      <td>431.578275</td>\n      <td>2.162939</td>\n      <td>27.888462</td>\n      <td>0.696486</td>\n      <td>0.651757</td>\n      <td>42.985091</td>\n      <td>0.741214</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1</td>\n      <td>453.260870</td>\n      <td>2.394783</td>\n      <td>30.705477</td>\n      <td>0.431304</td>\n      <td>0.234783</td>\n      <td>23.830658</td>\n      <td>0.186087</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\nsurvived_summary = train.groupby(\"Pclass\")\nsurvived_summary.mean().reset_index()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>PassengerId</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1</td>\n      <td>460.225352</td>\n      <td>0.563380</td>\n      <td>38.280984</td>\n      <td>0.422535</td>\n      <td>0.356808</td>\n      <td>78.124061</td>\n      <td>0.624413</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>2</td>\n      <td>445.956522</td>\n      <td>0.586957</td>\n      <td>29.877630</td>\n      <td>0.402174</td>\n      <td>0.380435</td>\n      <td>20.662183</td>\n      <td>0.472826</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3</td>\n      <td>439.154786</td>\n      <td>0.706721</td>\n      <td>25.140620</td>\n      <td>0.615071</td>\n      <td>0.393075</td>\n      <td>13.675550</td>\n      <td>0.242363</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\nI have gathered a small summary from the statistical overview above. Let's see what they are...\n- This data set has 891 raw and 9 columns. \n- only 38% passenger survived during that tragedy.\n- ~74% female passenger survived, while only ~19% male passenger survived. \n- ~63% first class passengers survived, while only 24% lower class passenger survived.\n\n\n\n## 4a. Correlation Matrix and Heatmap\n<a id=\"heatmap\"></a>\n***\n### Correlations\n\n\n```python\npd.DataFrame(abs(train.corr()['Survived']).sort_values(ascending = False))\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Survived</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>Sex</td>\n      <td>0.545899</td>\n    </tr>\n    <tr>\n      <td>Pclass</td>\n      <td>0.334068</td>\n    </tr>\n    <tr>\n      <td>Fare</td>\n      <td>0.261742</td>\n    </tr>\n    <tr>\n      <td>Parch</td>\n      <td>0.082157</td>\n    </tr>\n    <tr>\n      <td>Age</td>\n      <td>0.079472</td>\n    </tr>\n    <tr>\n      <td>SibSp</td>\n      <td>0.033395</td>\n    </tr>\n    <tr>\n      <td>PassengerId</td>\n      <td>0.006916</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n** Sex is the most important correlated feature with *Survived(dependent variable)* feature followed by Pclass.** \n\n\n```python\n## get the most important variables. \ncorr = train.corr()**2\ncorr.Survived.sort_values(ascending=False)\n```\n\n\n\n\n    Survived       1.000000\n    Sex            0.298006\n    Pclass         0.111601\n    Fare           0.068509\n    Parch          0.006750\n    Age            0.006316\n    SibSp          0.001115\n    PassengerId    0.000048\n    Name: Survived, dtype: float64\n\n\n\n\n**Squaring the correlation feature not only gives on positive correlations but also amplifies the relationships.** \n\n\n```python\n## heatmeap to see the correlation between features. \n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nimport numpy as np\nmask = np.zeros_like(train.corr(), dtype=np.bool)\n#mask[np.triu_indices_from(mask)] = True\n\nplt.subplots(figsize = (15,12))\nsns.heatmap(train.corr(), \n            annot=True,\n            mask = mask,\n            cmap = 'RdBu', ## in order to reverse the bar replace \"RdBu\" with \"RdBu_r\"\n            linewidths=.9, \n            linecolor='gray',\n            fmt='.2g',\n            center = 0,\n            square=True)\nplt.title(\"Correlations Among Features\", y = 1.03,fontsize = 20, pad = 40);\n```\n\n\n![png](kernel_files/kernel_107_0.png)\n\n\n#### Positive Correlation Features:\n- Fare and Survived: 0.26\n\n#### Negative Correlation Features:\n- Fare and Pclass: -0.6\n- Sex and Survived: -0.55\n- Pclass and Survived: -0.33\n\n\n**So, Let's analyze these correlations a bit.** We have found some moderately strong relationships between different features. There is a definite positive correlation between Fare and Survived rated. This relationship reveals that the passenger who paid more money for their ticket were more likely to survive. This theory aligns with one other correlation which is the correlation between Fare and Pclass(-0.6). This relationship can be explained by saying that first class passenger(1) paid more for fare then second class passenger(2), similarly second class passenger paid more than the third class passenger(3). This theory can also be supported by mentioning another Pclass correlation with our dependent variable, Survived. The correlation between Pclass and Survived is -0.33. This can also be explained by saying that first class passenger had a better chance of surviving than the second or the third and so on.\n\nHowever, the most significant correlation with our dependent variable is the Sex variable, which is the info on whether the passenger was male or female. This negative correlation with a magnitude of -0.54 which points towards some undeniable insights. Let's do some statistics to see how statistically significant this correlation is. \n\n## 4b. Statistical Test for Correlation\n<a id=\"statistical_test\"></a>\n***\n\nStatistical tests are the scientific way to prove the validation of theories. In any case, when we look at the data, we seem to have an intuitive understanding of where data is leading us. However, when we do statistical tests, we get a scientific or mathematical perspective of how significant these results are. Let's apply some of these methods and see how we are doing with our predictions.\n\n###  Hypothesis Testing Outline\n\nA hypothesis test compares the mean of a control group and experimental group and tries to find out whether the two sample means are different from each other and if they are different, how significant that difference is.\n \nA **hypothesis test** usually consists of multiple parts: \n\n1. Formulate a well-developed research problem or question: The hypothesis test usually starts with a concrete and well-developed researched problem. We need to ask the right question that can be answered using statistical analyses. \n2. The null hypothesis($H_0$) and Alternating hypothesis($H_1$):\n> * The **null hypothesis($H_0$)** is something that is assumed to be true. It is the status quo. In a null hypothesis, the observations are the result of pure chance. When we set out to experiment, we form the null hypothesis by saying that there is no difference between the means of the control group and the experimental group.\n> *  An **Alternative hypothesis($H_A$)** is a claim and the opposite of the null hypothesis.  It is going against the status quo. In an alternative theory, the observations show a real effect combined with a component of chance variation.\n    \n3. Determine the **test statistic**: test statistic can be used to assess the truth of the null hypothesis. Depending on the standard deviation we either use t-statistics or z-statistics. In addition to that, we want to identify whether the test is a one-tailed test or two-tailed test. [This](https://support.minitab.com/en-us/minitab/18/help-and-how-to/statistics/basic-statistics/supporting-topics/basics/null-and-alternative-hypotheses/) article explains it pretty well. [This](https://stattrek.com/hypothesis-test/hypothesis-testing.aspx) article is pretty good as well. \n\n4. Specify a **Significance level**: The significance level($\\alpha$) is the probability of rejecting a null hypothesis when it is true. In other words, we are ***comfortable/confident*** with rejecting the null hypothesis a significant amount of times even though it is true. This considerable amount is our Significant level. In addition to that significance level is one minus our Confidence interval. For example, if we say, our significance level is 5%, then our confidence interval would be (1 - 0.05) = 0.95 or 95%. \n\n5. Compute the **T-statistics**: Computing the t-statistics follows a simple equation. This equation slightly differs depending on one sample test or two sample test  \n\n6. Compute the **P-value**: P-value is the probability that a test statistic at least as significant as the one observed would be obtained assuming that the null hypothesis is correct. The p-value is known to be unintuitive, and even many professors are known to explain it wrong. I think this [video](https://www.youtube.com/watch?v=E4KCfcVwzyw) explains the p-value well. **The smaller the P-value, the stronger the evidence against the null hypothesis.**\n\n7. **Describe the result and compare the p-value with the significance value($\\alpha$)**: If p<=$\\alpha$, then the observed effect is statistically significant, the null hypothesis is ruled out, and the alternative hypothesis is valid. However if the p> $\\alpha$, we say that, we fail to reject the null hypothesis. Even though this sentence is grammatically wrong, it is logically right. We never accept the null hypothesis just because we are doing the statistical test with sample data points.\n\nWe will follow each of these steps above to do your hypothesis testing below.\n\n***\n\n### Hypothesis testing for Titanic\n#### Formulating a well developed researched question: \nRegarding this dataset, we can formulate the null hypothesis and alternative hypothesis by asking the following questions. \n> * **Is there a significant difference in the mean sex between the passenger who survived and passenger who did not survive?**. \n> * **Is there a substantial difference in the survival rate between the male and female passengers?**\n\n\n#### The Null Hypothesis and The Alternative Hypothesis:\nWe can formulate our hypothesis by asking questions differently. However, it is essential to understand what our end goal is. Here our dependent variable or target variable is **Survived**. Therefore, we say\n\n> ** Null Hypothesis($H_0$):** There is no difference in the survival rate between the male and female passengers. or the mean difference between male and female passenger in the survival rate is zero.  \n>  ** Alternative Hypothesis($H_A$):** There is a difference in the survival rate between the male and female passengers. or the mean difference in the survival rate between male and female is not zero.\n\n\nOnc thing we can do is try to set up the Null and Alternative Hypothesis in such way that, when we do our t-test, we can choose to do one tailed test. According to [this](https://support.minitab.com/en-us/minitab/18/help-and-how-to/statistics/basic-statistics/supporting-topics/basics/null-and-alternative-hypotheses/) article, one-tailed tests are more powerful than two-tailed test. In addition to that, [this](https://www.youtube.com/watch?v=5NcMFlrnYp8&list=PLIeGtxpvyG-LrjxQ60pxZaimkaKKs0zGF) video is also quite helpful understanding these topics. with this in mind we can update/modify our null and alternative hypothesis. Let's see how we can rewrite this..\n\n> ** Null Hypothesis($H_0$):** The \n\n\n\n#### Determine the test statistics:\n> This will be a two-tailed test since the difference between male and female passenger in the survival rate could be higher or lower than 0. \n> Since we do not know the standard deviation($\\sigma$) and n is small, we will use the t-distribution. \n\n#### Specify the significance level:\n> Specifying a significance level is an important step of the hypothesis test. It is an ultimate balance between type 1 error and type 2 error. We will discuss more in-depth about those in another lesson. For now, we have decided to make our significance level($\\alpha$) = 0.05. So, our confidence interval or non-rejection region would be (1 - $\\alpha$) =   95%. \n\n#### Computing T-statistics and P-value:\nLet's take a random sample and see the difference.\n\n\n```python\nmale_mean = train[train['Sex'] == 1].Survived.mean()\nmale_mean\n```\n\n\n\n\n    0.18608695652173912\n\n\n\n\n```python\nmale_mean = train[train['Sex'] == 1].Survived.mean()\n\nfemale_mean = train[train['Sex'] == 0].Survived.mean()\nprint (\"Male survival mean: \" + str(male_mean))\nprint (\"female survival mean: \" + str(female_mean))\n\nprint (\"The mean difference between male and female survival rate: \" + str(female_mean - male_mean))\n```\n\n    Male survival mean: 0.18608695652173912\n    female survival mean: 0.7412140575079872\n    The mean difference between male and female survival rate: 0.5551271009862481\n\n\nNow, we have to understand that those two means are not  **the population mean ($\\bar{\\mu}$)**.  *The population mean is a statistical term statistician uses to indicate the actual average of the entire group. The group can be any gathering of multiple numbers such as animal, human, plants, money, stocks.* For example, To find the age population mean of Bulgaria; we have to account for every single person's age and take their age. Which is almost impossible and if we were to go that route; there is no point of doing statistics in the first place. Therefore we approach this problem using sample sets. The idea of using sample set is that; if we take multiple samples of the same population and take the mean of them and put them in a distribution; eventually the distribution start to look more like a **normal distribution**. The more samples we take and the more sample means will be added and, the closer the normal distribution will reach towards population mean. This is where **Central limit theory** comes from. We will go more in depth of this topic later on. \n\nGoing back to our dataset, like we are saying these means above are part of the whole story. We were given part of the data to train our machine learning models, and the other part of the data was held back for testing. Therefore, It is impossible for us at this point to know the population means of survival for male and females. Situation like this calls for a statistical approach. We will use the sampling distribution approach to do the test. let's take 50 random sample of male and female from our train data.\n\n\n```python\n# separating male and female dataframe. \nimport random\nmale = train[train['Sex'] == 1]\nfemale = train[train['Sex'] == 0]\n\n## empty list for storing mean sample\nm_mean_samples = []\nf_mean_samples = []\n\nfor i in range(50):\n    m_mean_samples.append(np.mean(random.sample(list(male['Survived']),50,)))\n    f_mean_samples.append(np.mean(random.sample(list(female['Survived']),50,)))\n    \n\n# Print them out\nprint (f\"Male mean sample mean: {round(np.mean(m_mean_samples),2)}\")\nprint (f\"Male mean sample mean: {round(np.mean(f_mean_samples),2)}\")\nprint (f\"Difference between male and female mean sample mean: {round(np.mean(f_mean_samples) - np.mean(m_mean_samples),2)}\")\n```\n\n    Male mean sample mean: 0.18\n    Male mean sample mean: 0.74\n    Difference between male and female mean sample mean: 0.55\n\n\nH0: male mean is greater or equal to female mean, \nH1: male mean is less than female mean. \n\nAccording to the samples our male samples ($\\bar{x}_m$) and female samples($\\bar{x}_f$) mean measured difference is ~ 0.55(statistically this is called the point estimate of the male population mean and female population mean). keeping in mind that...\n* We randomly select 50 people to be in the male group and 50 people to be in the female group. \n* We know our sample is selected from a broader population(trainning set). \n* We know we could have ended up with a different random sample of males or females from the total dataset. \n***\nWith all three points above in mind, how confident are we that, the measured difference is real or statistically significant? we can perform a **t-test** to evaluate that. When we perform a **t-test** we are usually trying to find out **an evidence of significant difference between population mean with hypothesized mean(1 sample t-test) or in our case difference between two population means(2 sample t-test).** \n\n\n\nThe **t-statistics** is the measure of a degree to which our groups differ standardized by the variance of our measurements. In order words, it is basically the measure of signal over noise. Let us describe the previous sentence a bit more for clarification. I am going to use [this post](http://blog.minitab.com/blog/statistics-and-quality-data-analysis/what-is-a-t-test-and-why-is-it-like-telling-a-kid-to-clean-up-that-mess-in-the-kitchen) as reference to describe the t-statistics here. \n\n\n#### Calculating the t-statistics\n# $$t = \\frac{\\bar{x}-\\mu}{\\frac{S} {\\sqrt{n}} }$$\n\nHere..\n* $\\bar{x}$ is the sample mean. \n* $\\mu$ is the hypothesized mean. \n* S is the standard devaition. \n* n is the sample size. \n\n\n1. Now, the denominator of this fraction $(\\bar{x}-\\mu)$ is basically the strength of the signal. where we calculate the difference between hypothesized mean and sample mean. If the mean difference is higher, then the signal is stronger. \n\nthe numerator of this fraction ** ${S}/ {\\sqrt{n}}$ ** calculates the amount of variation or noise of the data set. Here S is standard deviation, which tells us how much variation is there in the data. n is the sample size. \n\nSo, according to the explanation above, the t-value or t-statistics is basically measures the strength of the signal(the difference) to the amount of noise(the variation) in the data and that is how we calculate the t-value in one sample t-test. However, in order to calculate between two sample population mean or in our case we will use the follow equation. \n\n# $$t = \\frac{\\bar{x}_M - \\bar{x}_F}{\\sqrt {s^2 (\\frac{1}{n_M} + \\frac{1}{n_F})}}$$\n\nThis equation may seem too complex, however, the idea behind these two are similar. Both of them have the concept of signal/noise. The only difference is that we replace our hypothesis mean with another sample mean and the two sample sizes repalce one sample size. \n\nHere..\n* $\\bar{x}_M$ is the mean of our male group sample measurements. \n* $ \\bar{x}_F$ is the mean of female group samples. \n* $ n_M$ and $n_F$ are the sample number of observations in each group. \n* $ S^2$ is the sample variance.\n\nIt is good to have an understanding of what going on in the background. However, we will use **scipy.stats** to find the t-statistics. \n\n\n\n```python\n\n```\n\n#### Compare P-value with $\\alpha$\n> It looks like the p-value is very small compared to our significance level($\\alpha$)of 0.05. Our observation sample is statistically significant. Therefore, our null hypothesis is ruled out, and our alternative hypothesis is valid, which is \"**There is a significant difference in the survival rate between the male and female passengers.\"**\n\n# Part 5: Feature Engineering\n<a id=\"feature_engineering\"></a>\n***\nFeature Engineering is exactly what its sounds like. Sometimes we want to create extra features from with in the features that we have, sometimes we want to remove features that are alike. Features engineering is the simple word for doing all those. It is important to remember that we will create new features in such ways that will not cause **multicollinearity(when there is a relationship among independent variables)** to occur. \n\n## name_length\n***Creating a new feature \"name_length\" that will take the count of letters of each name***\n\n\n```python\n# Creating a new colomn with a \ntrain['name_length'] = [len(i) for i in train.Name]\ntest['name_length'] = [len(i) for i in test.Name]\n\ndef name_length_group(size):\n    a = ''\n    if (size <=20):\n        a = 'short'\n    elif (size <=35):\n        a = 'medium'\n    elif (size <=45):\n        a = 'good'\n    else:\n        a = 'long'\n    return a\n\n\ntrain['nLength_group'] = train['name_length'].map(name_length_group)\ntest['nLength_group'] = test['name_length'].map(name_length_group)\n\n## Here \"map\" is python's built-in function. \n## \"map\" function basically takes a function and \n## returns an iterable list/tuple or in this case series. \n## However,\"map\" can also be used like map(function) e.g. map(name_length_group) \n## or map(function, iterable{list, tuple}) e.g. map(name_length_group, train[feature]]). \n## However, here we don't need to use parameter(\"size\") for name_length_group because when we \n## used the map function like \".map\" with a series before dot, we are basically hinting that series \n## and the iterable. This is similar to .append approach in python. list.append(a) meaning applying append on list. \n\n\n## cuts the column by given bins based on the range of name_length\n#group_names = ['short', 'medium', 'good', 'long']\n#train['name_len_group'] = pd.cut(train['name_length'], bins = 4, labels=group_names)\n```\n\n## title\n**Getting the title of each name as a new feature. **\n\n\n```python\n## get the title from the name\ntrain[\"title\"] = [i.split('.')[0] for i in train.Name]\ntrain[\"title\"] = [i.split(',')[1] for i in train.title]\ntest[\"title\"] = [i.split('.')[0] for i in test.Name]\ntest[\"title\"]= [i.split(',')[1] for i in test.title]\n```\n\n\n```python\n#rare_title = ['the Countess','Capt','Lady','Sir','Jonkheer','Don','Major','Col']\n#train.Name = ['rare' for i in train.Name for j in rare_title if i == j]\n## train Data\ntrain[\"title\"] = [i.replace('Ms', 'Miss') for i in train.title]\ntrain[\"title\"] = [i.replace('Mlle', 'Miss') for i in train.title]\ntrain[\"title\"] = [i.replace('Mme', 'Mrs') for i in train.title]\ntrain[\"title\"] = [i.replace('Dr', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Col', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Major', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Don', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Jonkheer', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Sir', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Lady', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Capt', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('the Countess', 'rare') for i in train.title]\ntrain[\"title\"] = [i.replace('Rev', 'rare') for i in train.title]\n\n\n\n#rare_title = ['the Countess','Capt','Lady','Sir','Jonkheer','Don','Major','Col']\n#train.Name = ['rare' for i in train.Name for j in rare_title if i == j]\n## test data\ntest['title'] = [i.replace('Ms', 'Miss') for i in test.title]\ntest['title'] = [i.replace('Dr', 'rare') for i in test.title]\ntest['title'] = [i.replace('Col', 'rare') for i in test.title]\ntest['title'] = [i.replace('Dona', 'rare') for i in test.title]\ntest['title'] = [i.replace('Rev', 'rare') for i in test.title]\n```\n\n## family_size\n***Creating a new feature called \"family_size\".*** \n\n\n```python\n## Family_size seems like a good feature to create\ntrain['family_size'] = train.SibSp + train.Parch+1\ntest['family_size'] = test.SibSp + test.Parch+1\n```\n\n\n```python\ndef family_group(size):\n    a = ''\n    if (size <= 1):\n        a = 'loner'\n    elif (size <= 4):\n        a = 'small'\n    else:\n        a = 'large'\n    return a\n```\n\n\n```python\ntrain['family_group'] = train['family_size'].map(family_group)\ntest['family_group'] = test['family_size'].map(family_group)\n```\n\n## is_alone\n\n\n```python\ntrain['is_alone'] = [1 if i<2 else 0 for i in train.family_size]\ntest['is_alone'] = [1 if i<2 else 0 for i in test.family_size]\n```\n\n## ticket\n\n\n```python\ntrain.Ticket.value_counts().sample(10)\n```\n\n\n\n\n    234686               1\n    312993               1\n    2694                 1\n    STON/O2. 3101271     1\n    394140               1\n    364498               1\n    113806               2\n    STON/O 2. 3101273    1\n    2653                 2\n    STON/O 2. 3101274    1\n    Name: Ticket, dtype: int64\n\n\n\nI have yet to figureout how to best manage ticket feature. So, any suggestion would be truly appreciated. For now, I will get rid off the ticket feature.\n\n\n```python\ntrain.drop(['Ticket'], axis=1, inplace=True)\n\ntest.drop(['Ticket'], axis=1, inplace=True)\n```\n\n## calculated_fare\n\n\n```python\n## Calculating fare based on family size. \ntrain['calculated_fare'] = train.Fare/train.family_size\ntest['calculated_fare'] = test.Fare/test.family_size\n```\n\nSome people have travelled in groups like family or friends. It seems like Fare column kept a record of the total fare rather than the fare of individual passenger, therefore calculated fare will be much handy in this situation. \n\n## fare_group\n\n\n```python\ndef fare_group(fare):\n    a= ''\n    if fare <= 4:\n        a = 'Very_low'\n    elif fare <= 10:\n        a = 'low'\n    elif fare <= 20:\n        a = 'mid'\n    elif fare <= 45:\n        a = 'high'\n    else:\n        a = \"very_high\"\n    return a\n\ntrain['fare_group'] = train['calculated_fare'].map(fare_group)\ntest['fare_group'] = test['calculated_fare'].map(fare_group)\n\n#train['fare_group'] = pd.cut(train['calculated_fare'], bins = 4, labels=groups)\n```\n\nFare group was calculated based on <i>calculated_fare</i>. This can further help our cause. \n\n## PassengerId\n\nIt seems like <i>PassengerId</i> column only works as an id in this dataset without any significant effect on the dataset. Let's drop it.\n\n\n```python\ntrain.drop(['PassengerId'], axis=1, inplace=True)\n\ntest.drop(['PassengerId'], axis=1, inplace=True)\n```\n\n## Creating dummy variables\n\nYou might be wondering what is a dummy variable? \n\nDummy variable is an important **prepocessing machine learning step**. Often times Categorical variables are an important features, which can be the difference between a good model and a great model. While working with a dataset, having meaningful value for example, \"male\" or \"female\" instead of 0's and 1's is more intuitive for us. However, machines do not understand the value of categorical values, for example, in this dataset we have gender male or female, algorithms do not accept categorical variables as input. In order to feed data in a machine learning model, we  \n\n\n```python\n\ntrain = pd.get_dummies(train, columns=['title',\"Pclass\", 'Cabin','Embarked','nLength_group', 'family_group', 'fare_group'], drop_first=False)\ntest = pd.get_dummies(test, columns=['title',\"Pclass\",'Cabin','Embarked','nLength_group', 'family_group', 'fare_group'], drop_first=False)\ntrain.drop(['family_size','Name', 'Fare','name_length'], axis=1, inplace=True)\ntest.drop(['Name','family_size',\"Fare\",'name_length'], axis=1, inplace=True)\n```\n\n## age\n\nAs I promised before, we are going to use Random forest regressor in this section to predict the missing age values. Let's see how many missing values do we have now\n\n\n```python\n## rearranging the columns so that I can easily use the dataframe to predict the missing age values. \ntrain = pd.concat([train[[\"Survived\", \"Age\", \"Sex\",\"SibSp\",\"Parch\"]], train.loc[:,\"is_alone\":]], axis=1)\ntest = pd.concat([test[[\"Age\", \"Sex\"]], test.loc[:,\"SibSp\":]], axis=1)\n```\n\n\n```python\n## Importing RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n## writing a function that takes a dataframe with missing values and outputs it by filling the missing values. \ndef completing_age(df):\n    ## gettting all the features except survived\n    age_df = df.loc[:,\"Age\":] \n    \n    temp_train = age_df.loc[age_df.Age.notnull()] ## df with age values\n    temp_test = age_df.loc[age_df.Age.isnull()] ## df without age values\n    \n    y = temp_train.Age.values ## setting target variables(age) in y \n    x = temp_train.loc[:, \"Sex\":].values\n    \n    rfr = RandomForestRegressor(n_estimators=1500, n_jobs=-1)\n    rfr.fit(x, y)\n    \n    predicted_age = rfr.predict(temp_test.loc[:, \"Sex\":])\n    \n    df.loc[df.Age.isnull(), \"Age\"] = predicted_age\n    \n\n    return df\n\n## Implementing the completing_age function in both train and test dataset. \ncompleting_age(train)\ncompleting_age(test);\n```\n\nLet's take a look at the histogram of the age column. \n\n\n```python\n## Let's look at the his\nplt.subplots(figsize = (22,10),)\nsns.distplot(train.Age, bins = 100, kde = True, rug = False, norm_hist=False);\n```\n\n\n![png](kernel_files/kernel_151_0.png)\n\n\n## age_group\nWe can create a new feature by grouping the \"Age\" column\n\n\n```python\n## create bins for age\ndef age_group_fun(age):\n    a = ''\n    if age <= 1:\n        a = 'infant'\n    elif age <= 4: \n        a = 'toddler'\n    elif age <= 13:\n        a = 'child'\n    elif age <= 18:\n        a = 'teenager'\n    elif age <= 35:\n        a = 'Young_Adult'\n    elif age <= 45:\n        a = 'adult'\n    elif age <= 55:\n        a = 'middle_aged'\n    elif age <= 65:\n        a = 'senior_citizen'\n    else:\n        a = 'old'\n    return a\n        \n## Applying \"age_group_fun\" function to the \"Age\" column.\ntrain['age_group'] = train['Age'].map(age_group_fun)\ntest['age_group'] = test['Age'].map(age_group_fun)\n\n## Creating dummies for \"age_group\" feature. \ntrain = pd.get_dummies(train,columns=['age_group'], drop_first=True)\ntest = pd.get_dummies(test,columns=['age_group'], drop_first=True);\n\n\"\"\"train.drop('Age', axis=1, inplace=True)\ntest.drop('Age', axis=1, inplace=True)\"\"\"\n```\n\n\n\n\n    \"train.drop('Age', axis=1, inplace=True)\\ntest.drop('Age', axis=1, inplace=True)\"\n\n\n\n# Part 6: Pre-Modeling Tasks\n## 6a. Separating dependent and independent variables\n<a id=\"dependent_independent\"></a>\n***\nBefore we apply any machine learning models, It is important to separate dependent and independent variables. Our dependent variable or target variable is something that we are trying to find, and our independent variable is the features we use to find the dependent variable. The way we use machine learning algorithm in a dataset is that we train our machine learning model by specifying independent variables and dependent variable. To specify them, we need to separate them from each other, and the code below does just that.\n\nP.S. In our test dataset, we do not have a dependent variable feature. We are to predict that using machine learning models. \n\n\n```python\n# separating our independent and dependent variable\nX = train.drop(['Survived'], axis = 1)\ny = train[\"Survived\"]\n\n\n#age_filled_data_nor = NuclearNormMinimization().complete(df1)\n#Data_1 = pd.DataFrame(age_filled_data, columns = df1.columns)\n#pd.DataFrame(zip(Data[\"Age\"],Data_1[\"Age\"],df[\"Age\"]))\n```\n\n## 6b. Splitting the training data\n<a id=\"split_training_data\" ></a>\n***\nThere are multiple ways of splitting data. They are...\n* train_test_split.\n* cross_validation. \n\nWe have separated dependent and independent features; We have separated train and test data. So, why do we still have to split our training data? If you are curious about that, I have the answer. For this competition, when we train the machine learning algorithms, we use part of the training set usually two-thirds of the train data. Once we train our algorithm using 2/3 of the train data, we start to test our algorithms using the remaining data. If the model performs well we dump our test data in the algorithms to predict and submit the competition. The code below, basically splits the train data into 4 parts, **X_train**, **X_test**, **y_train**, **y_test**.  \n* **X_train** and **y_train** first used to train the algorithm. \n* then, **X_test** is used in that trained algorithms to predict **outcomes. **\n* Once we get the **outcomes**, we compare it with **y_test**\n\nBy comparing the **outcome** of the model with **y_test**, we can determine whether our algorithms are performing well or not. As we compare we use confusion matrix to determine different aspects of model performance.\n\nP.S. When we use cross validation it is important to remember not to use **X_train, X_test, y_train and y_test**, rather we will use **X and y**. I will discuss more on that. \n\n\n```python\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = .33, random_state=0)\n```\n\n## 6c. Feature Scaling\n<a id=\"feature_scaling\" ></a>\n***\nFeature scaling is an important concept of machine learning models. Often times a dataset contain features highly varying in magnitude and unit. For some machine learning models, it is not a problem. However, for many other ones, its quite a problem. Many machine learning algorithms uses euclidian distances to calculate the distance between two points, it is quite a problem. Let's again look at a the sample of the **train** dataset below.\n\n\n```python\ntrain.sample(5)\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>is_alone</th>\n      <th>calculated_fare</th>\n      <th>title_ Master</th>\n      <th>title_ Miss</th>\n      <th>title_ Mr</th>\n      <th>...</th>\n      <th>fare_group_mid</th>\n      <th>fare_group_very_high</th>\n      <th>age_group_adult</th>\n      <th>age_group_child</th>\n      <th>age_group_infant</th>\n      <th>age_group_middle_aged</th>\n      <th>age_group_old</th>\n      <th>age_group_senior_citizen</th>\n      <th>age_group_teenager</th>\n      <th>age_group_toddler</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>630</td>\n      <td>1</td>\n      <td>80.000000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>30.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>806</td>\n      <td>0</td>\n      <td>39.000000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>732</td>\n      <td>0</td>\n      <td>37.887676</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>809</td>\n      <td>1</td>\n      <td>33.000000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>26.55</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>179</td>\n      <td>0</td>\n      <td>36.000000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 46 columns</p>\n</div>\n\n\n\nHere **Age** and **Calculated_fare** is much higher in magnitude compared to others machine learning features. This can create problems as many machine learning models will get confused thinking **Age** and **Calculated_fare** have higher weight than other features. Therefore, we need to do feature scaling to get a better result. \nThere are multiple ways to do feature scaling. \n<ul>\n    <li><b>MinMaxScaler</b>-Scales the data using the max and min values so that it fits between 0 and 1.</li>\n    <li><b>StandardScaler</b>-Scales the data so that it has mean 0 and variance of 1.</li>\n    <li><b>RobustScaler</b>-Scales the data similary to Standard Scaler, but makes use of the median and scales using the interquertile range so as to aviod issues with large outliers.</b>\n </ul>\nI will discuss more on that in a different kernel. For now we will use <b>Standard Scaler</b> to feature scale our dataset. \n\nP.S. I am showing a sample of both before and after so that you can see how scaling changes the dataset. \n\n<h3><font color=\"$5831bc\" face=\"Comic Sans MS\">Before Scaling</font></h3>\n\n\n```python\nheaders = X_train.columns \n\nX_train.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>is_alone</th>\n      <th>calculated_fare</th>\n      <th>title_ Master</th>\n      <th>title_ Miss</th>\n      <th>title_ Mr</th>\n      <th>title_ Mrs</th>\n      <th>...</th>\n      <th>fare_group_mid</th>\n      <th>fare_group_very_high</th>\n      <th>age_group_adult</th>\n      <th>age_group_child</th>\n      <th>age_group_infant</th>\n      <th>age_group_middle_aged</th>\n      <th>age_group_old</th>\n      <th>age_group_senior_citizen</th>\n      <th>age_group_teenager</th>\n      <th>age_group_toddler</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>170</td>\n      <td>61.000000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>33.5000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>187</td>\n      <td>45.000000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>26.5500</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>849</td>\n      <td>41.396667</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>44.5521</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>433</td>\n      <td>17.000000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>7.1250</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>651</td>\n      <td>18.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>11.5000</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 45 columns</p>\n</div>\n\n\n\n\n```python\n# Feature Scaling\n## We will be using standardscaler to transform\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\n## transforming \"train_x\"\nX_train = sc.fit_transform(X_train)\n## transforming \"test_x\"\nX_test = sc.transform(X_test)\n\n## transforming \"The testset\"\ntest = sc.transform(test)\n```\n\n<h3><font color=\"#5831bc\" face=\"Comic Sans MS\">After Scaling</font></h3>\n\n\n```python\npd.DataFrame(X_train, columns=headers).head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>is_alone</th>\n      <th>calculated_fare</th>\n      <th>title_ Master</th>\n      <th>title_ Miss</th>\n      <th>title_ Mr</th>\n      <th>title_ Mrs</th>\n      <th>...</th>\n      <th>fare_group_mid</th>\n      <th>fare_group_very_high</th>\n      <th>age_group_adult</th>\n      <th>age_group_child</th>\n      <th>age_group_infant</th>\n      <th>age_group_middle_aged</th>\n      <th>age_group_old</th>\n      <th>age_group_senior_citizen</th>\n      <th>age_group_teenager</th>\n      <th>age_group_toddler</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>2.259134</td>\n      <td>0.725942</td>\n      <td>-0.464750</td>\n      <td>-0.463616</td>\n      <td>0.794901</td>\n      <td>0.554725</td>\n      <td>-0.230633</td>\n      <td>-0.521487</td>\n      <td>0.837858</td>\n      <td>-0.383038</td>\n      <td>...</td>\n      <td>-0.455321</td>\n      <td>-0.30317</td>\n      <td>-0.444500</td>\n      <td>-0.246403</td>\n      <td>-0.116841</td>\n      <td>-0.309743</td>\n      <td>-0.101015</td>\n      <td>5.223573</td>\n      <td>-0.286299</td>\n      <td>-0.200699</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1.117348</td>\n      <td>0.725942</td>\n      <td>-0.464750</td>\n      <td>-0.463616</td>\n      <td>0.794901</td>\n      <td>0.292298</td>\n      <td>-0.230633</td>\n      <td>-0.521487</td>\n      <td>0.837858</td>\n      <td>-0.383038</td>\n      <td>...</td>\n      <td>-0.455321</td>\n      <td>-0.30317</td>\n      <td>2.249717</td>\n      <td>-0.246403</td>\n      <td>-0.116841</td>\n      <td>-0.309743</td>\n      <td>-0.101015</td>\n      <td>-0.191440</td>\n      <td>-0.286299</td>\n      <td>-0.200699</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.860208</td>\n      <td>-1.377520</td>\n      <td>0.356862</td>\n      <td>-0.463616</td>\n      <td>-1.258018</td>\n      <td>0.972044</td>\n      <td>-0.230633</td>\n      <td>-0.521487</td>\n      <td>-1.193520</td>\n      <td>2.610707</td>\n      <td>...</td>\n      <td>-0.455321</td>\n      <td>-0.30317</td>\n      <td>2.249717</td>\n      <td>-0.246403</td>\n      <td>-0.116841</td>\n      <td>-0.309743</td>\n      <td>-0.101015</td>\n      <td>-0.191440</td>\n      <td>-0.286299</td>\n      <td>-0.200699</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>-0.880777</td>\n      <td>0.725942</td>\n      <td>-0.464750</td>\n      <td>-0.463616</td>\n      <td>0.794901</td>\n      <td>-0.441176</td>\n      <td>-0.230633</td>\n      <td>-0.521487</td>\n      <td>0.837858</td>\n      <td>-0.383038</td>\n      <td>...</td>\n      <td>-0.455321</td>\n      <td>-0.30317</td>\n      <td>-0.444500</td>\n      <td>-0.246403</td>\n      <td>-0.116841</td>\n      <td>-0.309743</td>\n      <td>-0.101015</td>\n      <td>-0.191440</td>\n      <td>3.492850</td>\n      <td>-0.200699</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>-0.809416</td>\n      <td>-1.377520</td>\n      <td>-0.464750</td>\n      <td>0.703282</td>\n      <td>-1.258018</td>\n      <td>-0.275979</td>\n      <td>-0.230633</td>\n      <td>1.917594</td>\n      <td>-1.193520</td>\n      <td>-0.383038</td>\n      <td>...</td>\n      <td>2.196253</td>\n      <td>-0.30317</td>\n      <td>-0.444500</td>\n      <td>-0.246403</td>\n      <td>-0.116841</td>\n      <td>-0.309743</td>\n      <td>-0.101015</td>\n      <td>-0.191440</td>\n      <td>3.492850</td>\n      <td>-0.200699</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows \u00d7 45 columns</p>\n</div>\n\n\n\nYou can see how the features have transformed above.\n\n# Part 7: Modeling the Data\n<a id=\"modelingthedata\"></a>\n***\nSince the problem we are trying to solve is a classification problem, we are going to use a bunch of classification model to get the best prediction possible. Let's start with Logistic Regression. \n\n## 7a. Logistic Regression\n<a id=\"logistic_regression\"></a>\n***\nWe will start with one of the most basic but effective machine learning model, **Logistic Regression**. Logistic regression is a famous classifier still used today frequently despite its age. It is a regression similar to **Linear regression**, yet operates as a classifier. To understand logistic regression, we should have some idea about linear regression. Let's have a look at it. \n\nHopefully, we all know that any linear equation can be written in the form of...\n\n# $$ {y} = mX + b $$\n\n* Here, m = slope of the regression line. it represents the relationship between X and y. \n* b = y-intercept. \n* x and y are the points location in x_axis and y_axis respectively. \n<br/>\n\nIf you want to know how, check out this [video](https://www.khanacademy.org/math/algebra/two-var-linear-equations/writing-slope-intercept-equations/v/graphs-using-slope-intercept-form). So, this slope equation can also be written as...\n\n## $$ y = \\beta_0 + \\beta_1 x + \\epsilon \\\\ $$\n\nThis is the equation for a simple linear regression.\nhere,\n* y = Dependent variable. \n* $\\beta_0$ = the intercept, it is constant. \n* $\\beta_1$ = Coefficient of independent variable. \n* $x$ = Indepentent variable. \n* $ \\epsilon$ = error or residual. \n\n\nWe use this function to predict the value of a dependent variable with the help of only one independent variable. Therefore this regression is called **Simple Linear Regression.** \n\nSimilar to **Simple Linear Regression**, there is **Multiple Linear Regression** which can be used to predict dependent variable using multiple independent variables. Let's look at the equation for **Multiple Linear Regression**, \n\n## $$ \\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n $$\n\n\nIf you would like to know more about **Linear Regression** checkout this [kernel](https://www.kaggle.com/masumrumi/a-stats-analysis-and-ml-workflow-of-house-pricing). \n\nSo, we know/reviewed a bit about linear regression, and therefore we know how to deal with data that looks like this, \n<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/1200px-Linear_regression.svg.png\" width=\"600\">\n\nHere the data point's in this graph is continuous and therefore the problem is a regression one. However, what if we have data that when plotted in a scatter graph, looks like this...\n\n\n\n```python\ntrain.calculated_fare = train.calculated_fare.astype(float)\n```\n\n\n```python\nplt.subplots(figsize = (12,10))\nplt.scatter(train.Age, train.Survived);\nplt.xlabel(\"Age\")\nplt.ylabel('Survival Status');\n```\n\n\n![png](kernel_files/kernel_170_0.png)\n\n\nHere the data points are not continuous; rather categorical. The two horizontal dot lines represent the survival status in the y-axis and age in the x-axis. This is probably not the best graph to explain logistic regression. For the convenience of understanding the model, let's look at a similar scatter plot with some characteristics.\n\n<img src=\"https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/39_blog_image_3.png\" width=\"600\">\n<h5 align=\"right\">SuperDataScience team</h5>\n\nThis chart clearly divides the binary categorical values in the x-axis, keeping most of the 0's on the left side, and 1's on the right side. So, now that the distinction is apparent, we can use our knowledge of linear regression and come up with a regression line. So, how can we apply a regression line to explain this data?\n\n<img src=\"https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/39_blog_image_4.png\" width=\"800\">\n<h5 align=\"right\">SuperDataScience team</h5>\n\nAs you can see from the chart above, The linear regression is probably not the best approach to take for categorical data. The Linear regression line barely aligns with the data points, and even if in some best-case scenario we were to use straight regression line, we would end up with a considerable error rate, which is super inconvenient. This is where logistic regression comes in. \n\n #### This part of the kernel is a working progress. Please check back again for future updates.####\n\n\n```python\n# import LogisticRegression model in python. \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_absolute_error, accuracy_score\n\n## call on the model object\nlogreg = LogisticRegression(solver='liblinear')\n\n## fit the model with \"train_x\" and \"train_y\"\nlogreg.fit(X_train,y_train)\n\n## Once the model is trained we want to find out how well the model is performing, so we test the model. \n## we use \"test_x\" portion of the data(this data was not used to fit the model) to predict model outcome. \ny_pred = logreg.predict(X_test)\n\n## Once predicted we save that outcome in \"y_pred\" variable.\n## Then we compare the predicted value( \"y_pred\") and actual value(\"test_y\") to see how well our model is performing. \n\nprint (\"So, Our accuracy Score is: {}\".format(round(accuracy_score(y_pred, y_test),4)))\n```\n\n    So, Our accuracy Score is: 0.7959\n\n\n<h2><font color=\"#5831bc\" face=\"Comic Sans MS\">Evaluating the model</font></h2>\nWhile we try to evaluate the model, we want to focus on a couple of things. \n\n<ul>\n    <li>Which are the most importnat features(relatively) of a project ?(<b>Relative Feature Importance</b>)</li>\n    <li>Which features have the biggest impact on the project on the project success ? (<b>Permutation Importance</b>) </li>\n    <li>How does changes in those featues affact the project success? (<b>Partial Dependencies</b>)</li>\n    <li>Digging deeper into the decisions made by the model(<b>SHAP values</b>)\n</ul>\n\n<h3>Explaining the results of the model.</h3>\n<ul>\n    <li>How well is the model ?</li>\n    <li>What are the most important features ?</li>\n</ul>\n\n<h3>Introducting Confusion Matrix</h3>\n\nSo, what is accuracy score? what does it tell us? \n\nIntroducing <b>confusion matrix</b>, a table that <b>describes the performance of a classification model</b>. We use the classification model by using data where we already know the true outcome and compare it with the model predicted an outcome. Confusion Matrix tells us how many our model predicted correctly and incorrectly in terms of binary/multiple outcome classes. For example, in terms of this dataset, our model is trying to classify whether the passenger survived or not survived. Let's introduce ourselves with some of the terminologies of the confusion matrix. \n\n\n<ul style=\"list-style-type:square;\">\n    <li><b>True Positive</b></li>\n    <li><b>True Negative</b></li>\n    <li><b>False Positive</b></li>\n    <li><b>False Negative</b></li>\n</ul>\n\nLet's find out the confusion matrix for titanic dataset. \n\nwe have our confusion matrix. How about we give it a little more character. \n\n\n```python\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn.metrics import confusion_matrix\n\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\n\n\nnp.set_printoptions(precision=2)\n\nclass_names = np.array(['not_survived','survived'])\n\n# Plot non-normalized confusion matrix\nplot_confusion_matrix(y_test, y_pred, classes=class_names,\n                      title='Confusion matrix, without normalization')\n\n# Plot normalized confusion matrix\nplot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n                      title='Normalized confusion matrix')\n\nplt.show()\n```\n\n    Confusion matrix, without normalization\n    [[149  28]\n     [ 32  85]]\n    Normalized confusion matrix\n    [[0.84 0.16]\n     [0.27 0.73]]\n\n\n\n![png](kernel_files/kernel_175_1.png)\n\n\n\n![png](kernel_files/kernel_175_2.png)\n\n\n\n```python\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))\n```\n\n                  precision    recall  f1-score   support\n    \n               0       0.82      0.84      0.83       177\n               1       0.75      0.73      0.74       117\n    \n        accuracy                           0.80       294\n       macro avg       0.79      0.78      0.79       294\n    weighted avg       0.79      0.80      0.80       294\n    \n\n\n\n```python\n\n```\n\n #### This part of the kernel is a working progress. Please check back again for future updates.####\n \n Resources: \n * [Confusion Matrix](https://www.youtube.com/watch?v=8Oog7TXHvFY)\n### Under-fitting & Over-fitting: \nSo, we have our first model and its score. But, how do we make sure that our model is performing well. Our model may be overfitting or underfitting. In fact, for those of you don't know what overfitting and underfitting is, Let's find out.\n\n![](https://cdncontribute.geeksforgeeks.org/wp-content/uploads/fittings.jpg)\n\nAs you see in the chart above. **Underfitting** is when the model fails to capture important aspects of the data and therefore introduces more bias and performs poorly. On the other hand, **Overfitting** is when the model performs too well on the training data but does poorly in the validation set or test sets.  This situation is also known as having less bias but more variation and perform poorly as well. Ideally, we want to configure a model that performs well not only in the training data but also in the test data. This is where **bias-variance tradeoff** comes in. When we have a model that overfits, meaning less biased and more of variance, we introduce some bias in exchange of having much less variance. One particular tactic for this task is regularization models (Ridge, Lasso, Elastic Net).  These models are built to deal with the bias-variance tradeoff. This [kernel](https://www.kaggle.com/dansbecker/underfitting-and-overfitting) explains this topic well. Also, the following chart gives us a mental picture of where we want our models to be. \n![](http://scott.fortmann-roe.com/docs/docs/BiasVariance/biasvariance.png)\n\nIdeally, we want to pick a sweet spot where the model performs well in training set, validation set, and test set. As the model gets complex, bias decreases, variance increases. However, the most critical part is the error rates. We want our models to be at the bottom of that **U** shape where the error rate is the least. That sweet spot is also known as **Optimum Model Complexity(OMC).**\n\nNow that we know what we want in terms of under-fitting and over-fitting, let's talk about how to combat them. \n\nHow to combat over-fitting?\n<ul>\n    <li>Simplify the model by using less parameters.</li>\n    <li>Simplify the model by changing the hyperparameters.</li>\n    <li>Introducing regularization models. </li>\n    <li>Use more training data. </li>\n    <li>Gatter more data ( and gather better quality data). </li>\n    </ul>\n #### This part of the kernel is a working progress. Please check back again for future updates.####\n\n<h1>AUC & ROC Curve</h1>\n\n\n```python\nfrom sklearn.metrics import roc_curve, auc\n#plt.style.use('seaborn-pastel')\ny_score = logreg.decision_function(X_test)\n\nFPR, TPR, _ = roc_curve(y_test, y_score)\nROC_AUC = auc(FPR, TPR)\nprint (ROC_AUC)\n\nplt.figure(figsize =[11,9])\nplt.plot(FPR, TPR, label= 'ROC curve(area = %0.2f)'%ROC_AUC, linewidth= 4)\nplt.plot([0,1],[0,1], 'k--', linewidth = 4)\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel('False Positive Rate', fontsize = 18)\nplt.ylabel('True Positive Rate', fontsize = 18)\nplt.title('ROC for Titanic survivors', fontsize= 18)\nplt.show()\n```\n\n    0.834419817470665\n\n\n\n![png](kernel_files/kernel_180_1.png)\n\n\n\n```python\nfrom sklearn.metrics import precision_recall_curve\n\ny_score = logreg.decision_function(X_test)\n\nprecision, recall, _ = precision_recall_curve(y_test, y_score)\nPR_AUC = auc(recall, precision)\n\nplt.figure(figsize=[11,9])\nplt.plot(recall, precision, label='PR curve (area = %0.2f)' % PR_AUC, linewidth=4)\nplt.xlabel('Recall', fontsize=18)\nplt.ylabel('Precision', fontsize=18)\nplt.title('Precision Recall Curve for Titanic survivors', fontsize=18)\nplt.legend(loc=\"lower right\")\nplt.show()\n```\n\n\n![png](kernel_files/kernel_181_0.png)\n\n\n## Using Cross-validation:\nPros: \n* Helps reduce variance. \n* Expends models predictability. \n\n\n\n```python\n## Using StratifiedShuffleSplit\n## We can use KFold, StratifiedShuffleSplit, StratiriedKFold or ShuffleSplit, They are all close cousins. look at sklearn userguide for more info.   \nfrom sklearn.model_selection import StratifiedShuffleSplit, cross_val_score\ncv = StratifiedShuffleSplit(n_splits = 10, test_size = .25, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\n## Using standard scale for the whole dataset.\n\n## saving the feature names for decision tree display\ncolumn_names = X.columns\n\nX = sc.fit_transform(X)\naccuracies = cross_val_score(LogisticRegression(solver='liblinear'), X,y, cv  = cv)\nprint (\"Cross-Validation accuracy scores:{}\".format(accuracies))\nprint (\"Mean Cross-Validation accuracy score: {}\".format(round(accuracies.mean(),5)))\n```\n\n    Cross-Validation accuracy scores:[0.82 0.85 0.82 0.85 0.83 0.82 0.8  0.86 0.82 0.82]\n    Mean Cross-Validation accuracy score: 0.82793\n\n\n## Grid Search on Logistic Regression\n* What is grid search? \n* What are the pros and cons?\n\n**Gridsearch** is a simple concept but effective technique in Machine Learning. The word **GridSearch** stands for the fact that we are searching for optimal parameter/parameters over a \"grid.\" These optimal parameters are also known as **Hyperparameters**. **The Hyperparameters are model parameters that are set before fitting the model and determine the behavior of the model.**. For example, when we choose to use linear regression, we may decide to add a penalty to the loss function such as Ridge or Lasso. These penalties require specific alpha (the strength of the regularization technique) to set beforehand. The higher the value of alpha, the more penalty is being added. GridSearch finds the optimal value of alpha among a range of values provided by us, and then we go on and use that optimal value to fit the model and get sweet results. It is essential to understand those model parameters are different from models outcomes, for example, **coefficients** or model evaluation metrics such as **accuracy score** or **mean squared error** are model outcomes and different than hyperparameters.\n\n#### This part of the kernel is a working progress. Please check back again for future updates.####\n\n\n```python\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\n## C_vals is the alpla value of lasso and ridge regression(as alpha increases the model complexity decreases,)\n## remember effective alpha scores are 0<alpha<infinity \nC_vals = [0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16,16.5,17,17.5,18]\n## Choosing penalties(Lasso(l1) or Ridge(l2))\npenalties = ['l1','l2']\n## Choose a cross validation strategy. \ncv = StratifiedShuffleSplit(n_splits = 10, test_size = .25)\n\n## setting param for param_grid in GridSearchCV. \nparam = {'penalty': penalties, 'C': C_vals}\n\nlogreg = LogisticRegression(solver='liblinear')\n## Calling on GridSearchCV object. \ngrid = GridSearchCV(estimator=LogisticRegression(), \n                           param_grid = param,\n                           scoring = 'accuracy',\n                            n_jobs =-1,\n                           cv = cv\n                          )\n## Fitting the model\ngrid.fit(X, y)\n```\n\n\n\n\n    GridSearchCV(cv=StratifiedShuffleSplit(n_splits=10, random_state=None, test_size=0.25,\n                train_size=None),\n                 error_score='raise-deprecating',\n                 estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                              fit_intercept=True,\n                                              intercept_scaling=1, l1_ratio=None,\n                                              max_iter=100, multi_class='warn',\n                                              n_jobs=None, penalty='l2',\n                                              random_state=None, solver='warn',\n                                              tol=0.0001, verbose=0,\n                                              warm_start=False),\n                 iid='warn', n_jobs=-1,\n                 param_grid={'C': [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 3,\n                                   4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 16.5,\n                                   17, 17.5, 18],\n                             'penalty': ['l1', 'l2']},\n                 pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n                 scoring='accuracy', verbose=0)\n\n\n\n\n```python\n## Getting the best of everything. \nprint (grid.best_score_)\nprint (grid.best_params_)\nprint(grid.best_estimator_)\n\n\n```\n\n    0.8234234234234235\n    {'C': 12, 'penalty': 'l2'}\n    LogisticRegression(C=12, class_weight=None, dual=False, fit_intercept=True,\n                       intercept_scaling=1, l1_ratio=None, max_iter=100,\n                       multi_class='warn', n_jobs=None, penalty='l2',\n                       random_state=None, solver='warn', tol=0.0001, verbose=0,\n                       warm_start=False)\n\n\n\n#### Using the best parameters from the grid-search. \n\n\n```python\n### Using the best parameters from the grid-search.\nlogreg_grid = grid.best_estimator_\nlogreg_grid.score(X,y)\n```\n\n\n\n\n    0.8412162162162162\n\n\n\n## 7b. K-Nearest Neighbor classifier(KNN)\n<a id=\"knn\"></a>\n***\n\n\n```python\n## Importing the model. \nfrom sklearn.neighbors import KNeighborsClassifier\n## calling on the model oject. \nknn = KNeighborsClassifier(metric='minkowski', p=2)\n## knn classifier works by doing euclidian distance \n\n\n## doing 10 fold staratified-shuffle-split cross validation \ncv = StratifiedShuffleSplit(n_splits=10, test_size=.25, random_state=2)\n\naccuracies = cross_val_score(knn, X,y, cv = cv, scoring='accuracy')\nprint (\"Cross-Validation accuracy scores:{}\".format(accuracies))\nprint (\"Mean Cross-Validation accuracy score: {}\".format(round(accuracies.mean(),3)))\n```\n\n    Cross-Validation accuracy scores:[0.82 0.81 0.78 0.8  0.82 0.81 0.79 0.79 0.77 0.82]\n    Mean Cross-Validation accuracy score: 0.8\n\n\n#### Manually find the best possible k value for KNN\n\n\n```python\n## Search for an optimal value of k for KNN.\nk_range = range(1,31)\nk_scores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X,y, cv = cv, scoring = 'accuracy')\n    k_scores.append(scores.mean())\nprint(\"Accuracy scores are: {}\\n\".format(k_scores))\nprint (\"Mean accuracy score: {}\".format(np.mean(k_scores)))\n\n```\n\n    Accuracy scores are: [0.7504504504504506, 0.7774774774774775, 0.7923423423423424, 0.7869369369369368, 0.8004504504504505, 0.8004504504504505, 0.7972972972972971, 0.7932432432432432, 0.8018018018018018, 0.7927927927927929, 0.7972972972972971, 0.7909909909909911, 0.7968468468468468, 0.7927927927927927, 0.8009009009009007, 0.7846846846846847, 0.7896396396396396, 0.7788288288288288, 0.7905405405405406, 0.7815315315315315, 0.786036036036036, 0.7761261261261262, 0.7851351351351351, 0.773873873873874, 0.7761261261261262, 0.7639639639639639, 0.7666666666666666, 0.7599099099099099, 0.7626126126126127, 0.7554054054054056]\n    \n    Mean accuracy score: 0.7834384384384384\n\n\n\n```python\nfrom matplotlib import pyplot as plt\nplt.plot(k_range, k_scores)\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x7f0ea1d65438>]\n\n\n\n\n![png](kernel_files/kernel_193_1.png)\n\n\n### Grid search on KNN classifier\n\n\n```python\nfrom sklearn.model_selection import GridSearchCV\n## trying out multiple values for k\nk_range = range(1,31)\n## \nweights_options=['uniform','distance']\n# \nparam = {'n_neighbors':k_range, 'weights':weights_options}\n## Using startifiedShufflesplit. \ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n# estimator = knn, param_grid = param, n_jobs = -1 to instruct scikit learn to use all available processors. \ngrid = GridSearchCV(KNeighborsClassifier(), param,cv=cv,verbose = False, n_jobs=-1)\n## Fitting the model. \ngrid.fit(X,y)\n```\n\n\n\n\n    GridSearchCV(cv=StratifiedShuffleSplit(n_splits=10, random_state=15, test_size=0.3,\n                train_size=None),\n                 error_score='raise-deprecating',\n                 estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30,\n                                                metric='minkowski',\n                                                metric_params=None, n_jobs=None,\n                                                n_neighbors=5, p=2,\n                                                weights='uniform'),\n                 iid='warn', n_jobs=-1,\n                 param_grid={'n_neighbors': range(1, 31),\n                             'weights': ['uniform', 'distance']},\n                 pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n                 scoring=None, verbose=False)\n\n\n\n\n```python\nprint (grid.best_score_)\nprint (grid.best_params_)\nprint(grid.best_estimator_)\n\n```\n\n    0.8056179775280898\n    {'n_neighbors': 5, 'weights': 'uniform'}\n    KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                         metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n                         weights='uniform')\n\n\n#### Using best estimator from grid search using KNN. \n\n\n```python\n### Using the best parameters from the grid-search.\nknn_grid= grid.best_estimator_\nknn_grid.score(X,y)\n```\n\n\n\n\n    0.865990990990991\n\n\n\n#### Using RandomizedSearchCV\nRandomized search is a close cousin of grid search. It doesn't  always provide the best result but its fast. \n\n\n```python\nfrom sklearn.model_selection import RandomizedSearchCV\n## trying out multiple values for k\nk_range = range(1,31)\n## \nweights_options=['uniform','distance']\n# \nparam = {'n_neighbors':k_range, 'weights':weights_options}\n## Using startifiedShufflesplit. \ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30)\n# estimator = knn, param_grid = param, n_jobs = -1 to instruct scikit learn to use all available processors. \n## for RandomizedSearchCV, \ngrid = RandomizedSearchCV(KNeighborsClassifier(), param,cv=cv,verbose = False, n_jobs=-1, n_iter=40)\n## Fitting the model. \ngrid.fit(X,y)\n```\n\n\n\n\n    RandomizedSearchCV(cv=StratifiedShuffleSplit(n_splits=10, random_state=None, test_size=0.3,\n                train_size=None),\n                       error_score='raise-deprecating',\n                       estimator=KNeighborsClassifier(algorithm='auto',\n                                                      leaf_size=30,\n                                                      metric='minkowski',\n                                                      metric_params=None,\n                                                      n_jobs=None, n_neighbors=5,\n                                                      p=2, weights='uniform'),\n                       iid='warn', n_iter=40, n_jobs=-1,\n                       param_distributions={'n_neighbors': range(1, 31),\n                                            'weights': ['uniform', 'distance']},\n                       pre_dispatch='2*n_jobs', random_state=None, refit=True,\n                       return_train_score=False, scoring=None, verbose=False)\n\n\n\n\n```python\nprint (grid.best_score_)\nprint (grid.best_params_)\nprint(grid.best_estimator_)\n```\n\n    0.7865168539325843\n    {'weights': 'uniform', 'n_neighbors': 17}\n    KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                         metric_params=None, n_jobs=None, n_neighbors=17, p=2,\n                         weights='uniform')\n\n\n\n```python\n### Using the best parameters from the grid-search.\nknn_ran_grid = grid.best_estimator_\nknn_ran_grid.score(X,y)\n```\n\n\n\n\n    0.8265765765765766\n\n\n\n## Gaussian Naive Bayes\n<a id=\"gaussian_naive\"></a>\n***\n\n\n```python\n# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(X, y)\ny_pred = gaussian.predict(X_test)\ngaussian_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(gaussian_accy)\n```\n\n    0.789\n\n\n## Support Vector Machines(SVM)\n<a id=\"svm\"></a>\n***\n\n\n```python\nfrom sklearn.svm import SVC\nCs = [0.001, 0.01, 0.1, 1,1.5,2,2.5,3,4,5, 10] ## penalty parameter C for the error term. \ngammas = [0.0001,0.001, 0.01, 0.1, 1]\nparam_grid = {'C': Cs, 'gamma' : gammas}\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\ngrid_search = GridSearchCV(SVC(kernel = 'rbf', probability=True), param_grid, cv=cv) ## 'rbf' stands for gaussian kernel\ngrid_search.fit(X,y)\n```\n\n\n\n\n    GridSearchCV(cv=StratifiedShuffleSplit(n_splits=10, random_state=15, test_size=0.3,\n                train_size=None),\n                 error_score='raise-deprecating',\n                 estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n                               decision_function_shape='ovr', degree=3,\n                               gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n                               probability=True, random_state=None, shrinking=True,\n                               tol=0.001, verbose=False),\n                 iid='warn', n_jobs=None,\n                 param_grid={'C': [0.001, 0.01, 0.1, 1, 1.5, 2, 2.5, 3, 4, 5, 10],\n                             'gamma': [0.0001, 0.001, 0.01, 0.1, 1]},\n                 pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n                 scoring=None, verbose=0)\n\n\n\n\n```python\nprint(grid_search.best_score_)\nprint(grid_search.best_params_)\nprint(grid_search.best_estimator_)\n```\n\n    0.8453183520599251\n    {'C': 2, 'gamma': 0.001}\n    SVC(C=2, cache_size=200, class_weight=None, coef0=0.0,\n        decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n        max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n        verbose=False)\n\n\n\n```python\n# using the best found hyper paremeters to get the score. \nsvm_grid = grid_search.best_estimator_\nsvm_grid.score(X,y)\n```\n\n\n\n\n    0.8265765765765766\n\n\n\n## Decision Tree Classifier\n\nDecision tree works by breaking down the dataset into small subsets. This breaking down process is done by asking questions about the features of the datasets. The idea is to unmix the labels by asking fewer questions necessary. As we ask questions, we are breaking down the dataset into more subsets. Once we have a subgroup with only the unique type of labels, we end the tree in that node. If you would like to get a detailed understanding of Decision tree classifier, please take a look at [this](https://www.kaggle.com/masumrumi/decision-tree-with-titanic-dataset) kernel. \n\n\n```python\nfrom sklearn.tree import DecisionTreeClassifier\nmax_depth = range(1,30)\nmax_feature = [21,22,23,24,25,26,28,29,30,'auto']\ncriterion=[\"entropy\", \"gini\"]\n\nparam = {'max_depth':max_depth, \n         'max_features':max_feature, \n         'criterion': criterion}\ngrid = GridSearchCV(DecisionTreeClassifier(), \n                                param_grid = param, \n                                 verbose=False, \n                                 cv=StratifiedKFold(n_splits=20, random_state=15, shuffle=True),\n                                n_jobs = -1)\ngrid.fit(X, y) \n```\n\n    /opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n      DeprecationWarning)\n\n\n\n\n\n    GridSearchCV(cv=StratifiedKFold(n_splits=20, random_state=15, shuffle=True),\n                 error_score='raise-deprecating',\n                 estimator=DecisionTreeClassifier(class_weight=None,\n                                                  criterion='gini', max_depth=None,\n                                                  max_features=None,\n                                                  max_leaf_nodes=None,\n                                                  min_impurity_decrease=0.0,\n                                                  min_impurity_split=None,\n                                                  min_samples_leaf=1,\n                                                  min_samples_split=2,\n                                                  min_weight_fraction_leaf=0.0,\n                                                  presort=False, random_state=None,\n                                                  splitter='best'),\n                 iid='warn', n_jobs=-1,\n                 param_grid={'criterion': ['entropy', 'gini'],\n                             'max_depth': range(1, 30),\n                             'max_features': [21, 22, 23, 24, 25, 26, 28, 29, 30,\n                                              'auto']},\n                 pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n                 scoring=None, verbose=False)\n\n\n\n\n```python\nprint( grid.best_params_)\nprint (grid.best_score_)\nprint (grid.best_estimator_)\n```\n\n    {'criterion': 'entropy', 'max_depth': 5, 'max_features': 29}\n    0.8355855855855856\n    DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n                           max_features=29, max_leaf_nodes=None,\n                           min_impurity_decrease=0.0, min_impurity_split=None,\n                           min_samples_leaf=1, min_samples_split=2,\n                           min_weight_fraction_leaf=0.0, presort=False,\n                           random_state=None, splitter='best')\n\n\n\n```python\ndectree_grid = grid.best_estimator_\n## using the best found hyper paremeters to get the score. \ndectree_grid.score(X,y)\n```\n\n\n\n\n    0.8581081081081081\n\n\n\nLet's look at the visual of your Decision Tree.\n\n\n```python\nfrom sklearn.externals.six import StringIO\nfrom sklearn.tree import export_graphviz\nimport pydot\nfrom IPython.display import Image\ndot_data = StringIO()  \nexport_graphviz(dectree_grid, out_file=dot_data,  \n                feature_names=column_names,  class_names = ([\"Survived\" if int(i) is 1 else \"Not_survived\" for i in y.unique()]),\n                filled=True, rounded=True,\n                proportion=True,\n                special_characters=True)  \n(graph,) = pydot.graph_from_dot_data(dot_data.getvalue())\n\n## alternative tree\n#import graphviz\n#from sklearn import tree\n#dot_data = tree.export_graphviz(decision_tree=dectree_grid, out_file=None, feature_names=column_names, )\n#graph = graphviz.Source(dot_data)\n#graph.render(\"house\")\n#graph\n\nImage(graph.create_png())\n```\n\n    /opt/conda/lib/python3.6/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n      \"(https://pypi.org/project/six/).\", DeprecationWarning)\n\n\n\n\n\n![png](kernel_files/kernel_214_1.png)\n\n\n\nAbove is a full-grown decision tree. I think having a tree shown like that can help a lot in understanding how the decision tree works.\n\n\n```python\n## feature importance\nfeature_importances = pd.DataFrame(dectree_grid.feature_importances_,\n                                   index = column_names,\n                                    columns=['importance'])\nfeature_importances.sort_values(by='importance', ascending=False).head(10)\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>title_ Mr</td>\n      <td>0.481873</td>\n    </tr>\n    <tr>\n      <td>Pclass_3</td>\n      <td>0.123568</td>\n    </tr>\n    <tr>\n      <td>title_ rare</td>\n      <td>0.078702</td>\n    </tr>\n    <tr>\n      <td>calculated_fare</td>\n      <td>0.065281</td>\n    </tr>\n    <tr>\n      <td>Cabin_G</td>\n      <td>0.058128</td>\n    </tr>\n    <tr>\n      <td>Age</td>\n      <td>0.056328</td>\n    </tr>\n    <tr>\n      <td>SibSp</td>\n      <td>0.035061</td>\n    </tr>\n    <tr>\n      <td>fare_group_very_high</td>\n      <td>0.019869</td>\n    </tr>\n    <tr>\n      <td>Pclass_1</td>\n      <td>0.019286</td>\n    </tr>\n    <tr>\n      <td>Sex</td>\n      <td>0.012477</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\nThese are the top 10 features determined by **Decision Tree** helped classifing the fates of many passenger on Titanic on that night.\n\n## 7f. Random Forest Classifier\n<a id=\"random_forest\"></a>\n\nI admire working with decision trees because of the potential and basics they provide towards building a more complex model like Random Forest(RF). RF is an ensemble method (combination of many decision trees) which is where the \"forest\" part comes in. One crucial details about Random Forest is that while using a forest of decision trees, RF model <b>takes random subsets of the original dataset(bootstrapped)</b> and <b>random subsets of the variables(features/columns)</b>. Using this method, the RF model creates 100's-1000's(the amount can be menually determined) of a wide variety of decision trees. This variety makes the RF model more effective and accurate. We then run each test data point through all of these 100's to 1000's of decision trees or the RF model and take a vote on the output. \n\n\n\n\n```python\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.ensemble import RandomForestClassifier\nn_estimators = [140,145,150,155,160];\nmax_depth = range(1,10);\ncriterions = ['gini', 'entropy'];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n\n\nparameters = {'n_estimators':n_estimators,\n              'max_depth':max_depth,\n              'criterion': criterions\n              \n        }\ngrid = GridSearchCV(estimator=RandomForestClassifier(max_features='auto'),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\ngrid.fit(X,y) \n```\n\n\n\n\n    GridSearchCV(cv=StratifiedShuffleSplit(n_splits=10, random_state=15, test_size=0.3,\n                train_size=None),\n                 error_score='raise-deprecating',\n                 estimator=RandomForestClassifier(bootstrap=True, class_weight=None,\n                                                  criterion='gini', max_depth=None,\n                                                  max_features='auto',\n                                                  max_leaf_nodes=None,\n                                                  min_impurity_decrease=0.0,\n                                                  min_impurity_split=None,\n                                                  min_samples_leaf=1,\n                                                  min_samples_split=2,\n                                                  min_weight_fraction_leaf=0.0,\n                                                  n_estimators='warn', n_jobs=None,\n                                                  oob_score=False,\n                                                  random_state=None, verbose=0,\n                                                  warm_start=False),\n                 iid='warn', n_jobs=-1,\n                 param_grid={'criterion': ['gini', 'entropy'],\n                             'max_depth': range(1, 10),\n                             'n_estimators': [140, 145, 150, 155, 160]},\n                 pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n                 scoring=None, verbose=0)\n\n\n\n\n```python\nprint (grid.best_score_)\nprint (grid.best_params_)\nprint (grid.best_estimator_)\n```\n\n    0.8426966292134831\n    {'criterion': 'gini', 'max_depth': 5, 'n_estimators': 155}\n    RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n                           max_depth=5, max_features='auto', max_leaf_nodes=None,\n                           min_impurity_decrease=0.0, min_impurity_split=None,\n                           min_samples_leaf=1, min_samples_split=2,\n                           min_weight_fraction_leaf=0.0, n_estimators=155,\n                           n_jobs=None, oob_score=False, random_state=None,\n                           verbose=0, warm_start=False)\n\n\n\n```python\nrf_grid = grid.best_estimator_\nrf_grid.score(X,y)\n```\n\n\n\n\n    0.8445945945945946\n\n\n\n\n```python\nfrom sklearn.metrics import classification_report\n# Print classification report for y_test\nprint(classification_report(y_test, y_pred, labels=rf_grid.classes_))\n```\n\n                  precision    recall  f1-score   support\n    \n               0       0.82      0.84      0.83       177\n               1       0.74      0.72      0.73       117\n    \n        accuracy                           0.79       294\n       macro avg       0.78      0.78      0.78       294\n    weighted avg       0.79      0.79      0.79       294\n    \n\n\n## Feature Importance\n\n\n```python\n## feature importance\nfeature_importances = pd.DataFrame(rf_grid.feature_importances_,\n                                   index = column_names,\n                                    columns=['importance'])\nfeature_importances.sort_values(by='importance', ascending=False).head(10)\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>importance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>title_ Mr</td>\n      <td>0.212276</td>\n    </tr>\n    <tr>\n      <td>Sex</td>\n      <td>0.156270</td>\n    </tr>\n    <tr>\n      <td>title_ Miss</td>\n      <td>0.079972</td>\n    </tr>\n    <tr>\n      <td>calculated_fare</td>\n      <td>0.073707</td>\n    </tr>\n    <tr>\n      <td>title_ Mrs</td>\n      <td>0.065400</td>\n    </tr>\n    <tr>\n      <td>Pclass_3</td>\n      <td>0.065230</td>\n    </tr>\n    <tr>\n      <td>Age</td>\n      <td>0.047500</td>\n    </tr>\n    <tr>\n      <td>family_group_small</td>\n      <td>0.029006</td>\n    </tr>\n    <tr>\n      <td>Pclass_1</td>\n      <td>0.023179</td>\n    </tr>\n    <tr>\n      <td>family_group_large</td>\n      <td>0.023047</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n<h3>Why Random Forest?(Pros and Cons)</h3>\n\n***\n<h2>Introducing Ensemble Learning</h2>\nIn statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. \n\nThere are two types of ensemple learnings. \n\n**Bagging/Averaging Methods**\n> In averaging methods, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.\n\n**Boosting Methods**\n> The other family of ensemble methods are boosting methods, where base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.\n\n<h4 align=\"right\">Source:GA</h4>\n\nResource: <a href=\"https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205\">Ensemble methods: bagging, boosting and stacking</a>\n***\n## 7g. Bagging Classifier\n<a id=\"bagging\"></a>\n***\n\n<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\">Bagging Classifier</a>(Bootstrap Aggregating) is the ensemble method that involves manipulating the training set by resampling and running algorithms on it. Let's do a quick review:\n* Bagging classifier uses a process called bootstrapped dataset to create multiple datasets from one original dataset and runs algorithm on each one of them. Here is an image to show how bootstrapped dataset works. \n<img src=\"https://uc-r.github.io/public/images/analytics/bootstrap/bootstrap.png\" width=\"600\">\n<h4 align=\"center\">Resampling from original dataset to bootstrapped datasets</h4>\n<h4 align=\"right\">Source: https://uc-r.github.io</h4>\n\n\n* After running a learning algorithm on each one of the bootstrapped datasets, all models are combined by taking their average. the test data/new data then go through this averaged classifier/combined classifier and predict the output. \n\nHere is an image to make it clear on how bagging works, \n<img src=\"https://prachimjoshi.files.wordpress.com/2015/07/screen_shot_2010-12-03_at_5-46-21_pm.png\" width=\"600\">\n<h4 align=\"right\">Source: https://prachimjoshi.files.wordpress.com</h4>\nPlease check out [this](https://www.kaggle.com/masumrumi/bagging-with-titanic-dataset) kernel if you want to find out more about bagging classifier. \n\n\n```python\nfrom sklearn.ensemble import BaggingClassifier\nn_estimators = [10,30,50,70,80,150,160, 170,175,180,185];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\n\nparameters = {'n_estimators':n_estimators,\n              \n        }\ngrid = GridSearchCV(BaggingClassifier(base_estimator= None, ## If None, then the base estimator is a decision tree.\n                                      bootstrap_features=False),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\ngrid.fit(X,y) \n```\n\n\n\n\n    GridSearchCV(cv=StratifiedShuffleSplit(n_splits=10, random_state=15, test_size=0.3,\n                train_size=None),\n                 error_score='raise-deprecating',\n                 estimator=BaggingClassifier(base_estimator=None, bootstrap=True,\n                                             bootstrap_features=False,\n                                             max_features=1.0, max_samples=1.0,\n                                             n_estimators=10, n_jobs=None,\n                                             oob_score=False, random_state=None,\n                                             verbose=0, warm_start=False),\n                 iid='warn', n_jobs=-1,\n                 param_grid={'n_estimators': [10, 30, 50, 70, 80, 150, 160, 170,\n                                              175, 180, 185]},\n                 pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n                 scoring=None, verbose=0)\n\n\n\n\n```python\nprint (grid.best_score_)\nprint (grid.best_params_)\nprint (grid.best_estimator_)\n```\n\n    0.8138576779026218\n    {'n_estimators': 185}\n    BaggingClassifier(base_estimator=None, bootstrap=True, bootstrap_features=False,\n                      max_features=1.0, max_samples=1.0, n_estimators=185,\n                      n_jobs=None, oob_score=False, random_state=None, verbose=0,\n                      warm_start=False)\n\n\n\n```python\nbagging_grid = grid.best_estimator_\nbagging_grid.score(X,y)\n```\n\n\n\n\n    0.9887387387387387\n\n\n\n<h3>Why use Bagging? (Pros and cons)</h3>\nBagging works best with strong and complex models(for example, fully developed decision trees). However, don't let that fool you to thinking that similar to a decision tree, bagging also overfits the model. Instead, bagging reduces overfitting since a lot of the sample training data are repeated and used to create base estimators. With a lot of equally likely training data, bagging is not very susceptible to overfitting with noisy data, therefore reduces variance. However, the downside is that this leads to an increase in bias.\n\n<h4>Random Forest VS. Bagging Classifier</h4>\n\nIf some of you are like me, you may find Random Forest to be similar to Bagging Classifier. However, there is a fundamental difference between these two which is **Random Forests ability to pick subsets of features in each node.** I will elaborate on this in a future update.\n\n## 7h. AdaBoost Classifier\n<a id=\"AdaBoost\"></a>\n***\nAdaBoost is another <b>ensemble model</b> and is quite different than Bagging. Let's point out the core concepts. \n> AdaBoost combines a lot of \"weak learners\"(they are also called stump; a tree with only one node and two leaves) to make classifications.\n\n> This base model fitting is an iterative process where each stump is chained one after the other; <b>It cannot run in parallel.</b>\n\n> <b>Some stumps get more say in the final classifications than others.</b> The models use weights that are assigned to each data point/raw indicating their \"importance.\" Samples with higher weight have a higher influence on the total error of the next model and gets more priority. The first stump starts with uniformly distributed weight which means, in the beginning, every datapoint have an equal amount of weights. \n\n> <b>Each stump is made by talking the previous stump's mistakes into account.</b> After each iteration weights gets re-calculated in order to take the errors/misclassifications from the last stump into consideration. \n\n> The final prediction is typically constructed by a weighted vote where weights for each base model depends on their training errors or misclassification rates. \n\nTo illustrate what we have talked about so far let's look at the following visualization. \n\n<img src=\"https://cdn-images-1.medium.com/max/1600/0*paPv7vXuq4eBHZY7.png\">\n<h5 align=\"right\"> Source: Diogo(Medium)</h5>\n\n\n\n\nLet's dive into each one of the nitty-gritty stuff about AdaBoost:\n***\n> <b>First</b>, we determine the best feature to split the dataset using Gini index(basics from decision tree). The feature with the lowest Gini index becomes the first stump in the AdaBoost stump chain(the lower the Gini index is, the better unmixed the label is, therefore, better split).\n***\n> <b>Secondly</b>, we need to determine how much say a stump will have in the final classification and how we can calculate that.\n* We learn how much say a stump has in the final classification by calculating how well it classified the samples (aka calculate the total error of the weight).\n* The <b>Total Error</b> for a stump is the sum of the weights associated with the incorrectly classified samples. For example, lets say, we start a stump with 10 datasets. The first stump will uniformly distribute an weight amoung all the datapoints. Which means each data point will have 1/10 weight. Let's say once the weight is distributed we run the model and find 2 incorrect predicitons. In order to calculate the total erorr we add up all the misclassified weights. Here we get 1/10 + 1/10 = 2/10 or 1/5. This is our total error. We can also think about it\n### $$ \\epsilon_t = \\frac{\\text{misclassifications}_t}{\\text{observations}_t} $$\n* Since the weight is uniformly distributed(all add up to 1) among all data points, the total error will always be between 0(perfect stump) and 1(horrible stump).\n* We use the total error to determine the amount of say a stump has in the final classification using the following formula\n \n### $$ \\alpha_t = \\frac{1}{2}ln \\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right) \\text{where } \\epsilon_t < 1$$\n\nWhere $\\epsilon_t$ is the misclassification rate for the current classifier:\n\n### $$ \\epsilon_t = \\frac{\\text{misclassifications}_t}{\\text{observations}_t} $$\n\nHere...\n* $\\alpha_t$ = Amount of Say\n* $\\epsilon_t$ = Total error\n\n\n\nWe can draw a graph to determine the amount of say using the value of total error(0 to 1)\n\n<img src=\"http://chrisjmccormick.files.wordpress.com/2013/12/adaboost_alphacurve.png\">\n<h5 align=\"right\"> Source: Chris McCormick</h5>\n\n* The blue line tells us the amount of say for <b>Total Error(Error rate)</b> between 0 and 1. \n* When the stump does a reasonably good job, and the <b>total error</b> is minimal, then the <b>amount of say(Alpha)</b> is relatively large, and the alpha value is positive. \n* When the stump does an average job(similar to a coin flip/the ratio of getting correct and incorrect ~50%/50%), then the <b>total error</b> is ~0.5. In this case the <b>amount of say</b> is <b>0</b>.\n* When the error rate is high let's say close to 1, then the <b>amount of say</b> will be negative, which means if the stump outputs a value as \"survived\" the included weight will turn that value into \"not survived.\"\n\nP.S. If the <b>Total Error</b> is 1 or 0, then this equation will freak out. A small amount of error is added to prevent this from happening. \n \n ***\n> <b>Third</b>, We need to learn how to modify the weights so that the next stump will take the errors that the current stump made into account. The pseducode for calculating the new sample weight is as follows. \n### $$ New Sample Weight = Sample Weight + e^{\\alpha_t}$$\nHere the $\\alpha_t(AmountOfSay)$ can be positive or negative depending whether the sample was correctly classified or misclassified by the current stump. We want to increase the sample weight of the misclassified samples; hinting the next stump to put more emphasize on those. Inversely, we want to decrease the sample weight of the correctly classified samples; hinting the next stump to put less emphasize on those. \n\nThe following equation help us to do this calculation. \n### $$ D_{t+1}(i) = D_t(i) e^{-\\alpha_t y_i h_t(x_i)} $$\n\nHere, \n* $D_{t+1}(i)$ = New Sample Weight. \n* $D_t(i)$ = Current Sample weight.\n* $\\alpha_t$ = Amount of Say, alpha value, this is the coefficient that gets updated in each iteration and \n* $y_i h_t(x_i)$ = place holder for 1 if stump correctly classified, -1 if misclassified. \n\nFinally, we put together the combined classifier, which is \n### $$ AdaBoost(X) = sign\\left(\\sum_{t=1}^T\\alpha_t h_t(X)\\right) $$ \n\nHere, \n\n$AdaBoost(X)$ is the classification predictions for $y$ using predictor matrix $X$\n\n$T$ is the set of \"weak learners\"\n\n$\\alpha_t$ is the contribution weight for weak learner $t$\n\n$h_t(X)$ is the prediction of weak learner $t$\n\nand $y$ is binary **with values -1 and 1**\n\n\nP.S. Since the stump barely captures essential specs about the dataset, the model is highly biased in the beginning. However, as the chain of stumps continues and at the end of the process, AdaBoost becomes a strong tree and reduces both bias and variance.\n\n<h3>Resources:</h3>\n<ul>\n    <li><a href=\"https://www.youtube.com/watch?v=LsK-xG1cLYA\">Statquest</a></li>\n    <li><a href=\"https://www.youtube.com/watch?v=-DUxtdeCiB4\">Principles of Machine Learning | AdaBoost(Video)</a></li>\n</ul>\n\n\n```python\nfrom sklearn.ensemble import AdaBoostClassifier\nn_estimators = [100,140,145,150,160, 170,175,180,185];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\nlearning_r = [0.1,1,0.01,0.5]\n\nparameters = {'n_estimators':n_estimators,\n              'learning_rate':learning_r\n              \n        }\ngrid = GridSearchCV(AdaBoostClassifier(base_estimator= None, ## If None, then the base estimator is a decision tree.\n                                     ),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\ngrid.fit(X,y) \n```\n\n\n\n\n    GridSearchCV(cv=StratifiedShuffleSplit(n_splits=10, random_state=15, test_size=0.3,\n                train_size=None),\n                 error_score='raise-deprecating',\n                 estimator=AdaBoostClassifier(algorithm='SAMME.R',\n                                              base_estimator=None,\n                                              learning_rate=1.0, n_estimators=50,\n                                              random_state=None),\n                 iid='warn', n_jobs=-1,\n                 param_grid={'learning_rate': [0.1, 1, 0.01, 0.5],\n                             'n_estimators': [100, 140, 145, 150, 160, 170, 175,\n                                              180, 185]},\n                 pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n                 scoring=None, verbose=0)\n\n\n\n\n```python\nprint (grid.best_score_)\nprint (grid.best_params_)\nprint (grid.best_estimator_)\n```\n\n    0.8232209737827715\n    {'learning_rate': 0.1, 'n_estimators': 100}\n    AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=0.1,\n                       n_estimators=100, random_state=None)\n\n\n\n```python\nadaBoost_grid = grid.best_estimator_\nadaBoost_grid.score(X,y)\n```\n\n\n\n\n    0.8355855855855856\n\n\n\n## Pros and cons of boosting\n\n---\n\n### Pros\n\n- Achieves higher performance than bagging when hyper-parameters tuned properly.\n- Can be used for classification and regression equally well.\n- Easily handles mixed data types.\n- Can use \"robust\" loss functions that make the model resistant to outliers.\n\n---\n\n### Cons\n\n- Difficult and time consuming to properly tune hyper-parameters.\n- Cannot be parallelized like bagging (bad scalability when huge amounts of data).\n- More risk of overfitting compared to bagging.\n\n<h3>Resources: </h3>\n<ul>\n    <li><a href=\"http://mccormickml.com/2013/12/13/adaboost-tutorial/\">AdaBoost Tutorial-Chris McCormick</a></li>\n    <li><a href=\"http://rob.schapire.net/papers/explaining-adaboost.pdf\">Explaining AdaBoost by Robert Schapire(One of the original author of AdaBoost)</a></li>\n</ul>\n\n## 7i. Gradient Boosting Classifier\n<a id=\"gradient_boosting\"></a>\n***\n\n\n```python\n# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngradient_boost = GradientBoostingClassifier()\ngradient_boost.fit(X, y)\ny_pred = gradient_boost.predict(X_test)\ngradient_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(gradient_accy)\n```\n\n    0.85\n\n\n<h3>Resources: </h3>\n<ul>\n    <li><a href=\"https://www.youtube.com/watch?v=sDv4f4s2SB8\">Gradient Descent(StatQuest)</a></li>\n    <li><a href=\"https://www.youtube.com/watch?v=3CC4N4z3GJc\">Gradient Boost(Regression Main Ideas)(StatQuest)</a></li>\n    <li><a href=\"https://www.youtube.com/watch?v=3CC4N4z3GJc\">Gradient Boost(Regression Calculation)(StatQuest)</a></li>\n    <li><a href=\"https://www.youtube.com/watch?v=jxuNLH5dXCs\">Gradient Boost(Classification Main Ideas)(StatQuest)</a></li>\n    <li><a href=\"https://www.youtube.com/watch?v=StWY5QWMXCw\">Gradient Boost(Classification Calculation)(StatQuest)</a></li>\n    <li><a href=\"https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\">Complete Machine Learning Guide to Parameter Tuning in Gradient Boosting (GBM) in Python</a></li>\n</ul>\n\n\n## 7j. XGBClassifier\n<a id=\"XGBClassifier\"></a>\n***\n\n\n```python\nfrom xgboost import XGBClassifier\nXGBClassifier = XGBClassifier()\nXGBClassifier.fit(X, y)\ny_pred = XGBClassifier.predict(X_test)\nXGBClassifier_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(XGBClassifier_accy)\n```\n\n    0.847\n\n\n## 7k. Extra Trees Classifier\n<a id=\"extra_tree\"></a>\n***\n\n\n```python\nfrom sklearn.ensemble import ExtraTreesClassifier\nExtraTreesClassifier = ExtraTreesClassifier()\nExtraTreesClassifier.fit(X, y)\ny_pred = ExtraTreesClassifier.predict(X_test)\nextraTree_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(extraTree_accy)\n\n```\n\n    0.959\n\n\n## 7l. Gaussian Process Classifier\n<a id=\"GaussianProcessClassifier\"></a>\n***\n\n\n```python\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nGaussianProcessClassifier = GaussianProcessClassifier()\nGaussianProcessClassifier.fit(X, y)\ny_pred = GaussianProcessClassifier.predict(X_test)\ngau_pro_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(gau_pro_accy)\n```\n\n    0.925\n\n\n## 7m. Voting Classifier\n<a id=\"voting_classifer\"></a>\n***\n\n\n```python\nfrom sklearn.ensemble import VotingClassifier\n\nvoting_classifier = VotingClassifier(estimators=[\n    ('lr_grid', logreg_grid),\n    ('svc', svm_grid),\n    ('random_forest', rf_grid),\n    ('gradient_boosting', gradient_boost),\n    ('decision_tree_grid',dectree_grid),\n    ('knn_classifier', knn_grid),\n    ('XGB_Classifier', XGBClassifier),\n    ('bagging_classifier', bagging_grid),\n    ('adaBoost_classifier',adaBoost_grid),\n    ('ExtraTrees_Classifier', ExtraTreesClassifier),\n    ('gaussian_classifier',gaussian),\n    ('gaussian_process_classifier', GaussianProcessClassifier)\n],voting='hard')\n\n#voting_classifier = voting_classifier.fit(train_x,train_y)\nvoting_classifier = voting_classifier.fit(X,y)\n```\n\n\n```python\ny_pred = voting_classifier.predict(X_test)\nvoting_accy = round(accuracy_score(y_pred, y_test), 3)\nprint(voting_accy)\n```\n\n    0.871\n\n\n\n```python\n#models = pd.DataFrame({\n#    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n#              'Random Forest', 'Naive Bayes', \n#              'Decision Tree', 'Gradient Boosting Classifier', 'Voting Classifier', 'XGB Classifier','ExtraTrees Classifier','Bagging Classifier'],\n#    'Score': [svc_accy, knn_accy, logreg_accy, \n#              random_accy, gaussian_accy, dectree_accy,\n#               gradient_accy, voting_accy, XGBClassifier_accy, extraTree_accy, bagging_accy]})\n#models.sort_values(by='Score', ascending=False)\n```\n\n# Part 8: Submit test predictions\n<a id=\"submit_predictions\"></a>\n***\n\n\n```python\nall_models = [logreg_grid,\n              knn_grid, \n              knn_ran_grid,\n              svm_grid,\n              dectree_grid,\n              rf_grid,\n              bagging_grid,\n              adaBoost_grid,\n              voting_classifier]\n\nc = {}\nfor i in all_models:\n    a = i.predict(X_test)\n    b = accuracy_score(a, y_test)\n    c[i] = b\n    \n\n```\n\n\n```python\ntest_prediction = (max(c, key=c.get)).predict(test)\nsubmission = pd.DataFrame({\n        \"PassengerId\": passengerid,\n        \"Survived\": test_prediction\n    })\n\nsubmission.PassengerId = submission.PassengerId.astype(int)\nsubmission.Survived = submission.Survived.astype(int)\n\nsubmission.to_csv(\"titanic1_submission.csv\", index=False)\n```\n\n# Credits\n\n* To [Brandon Foltz](https://(www.youtube.com/channel/UCFrjdcImgcQVyFbK04MBEhA) for being a fantastic statistics teacher. Love all those inspirational intro's. \n* To [Khan Academy](https://www.khanacademy.org), Amazing place to keep track of my mathematics journey. \n* To [General Assambly](https://generalassemb.ly); Where I started my data science journey. \n* To [Corey Schafer](https://www.youtube.com/channel/UCCezIgC97PvUuR4_gbFUs5g); Corey explains programming terms incredibly well. To all the newcomers, please check out his style of teaching.\n\n# Resources\nHere are some of the links I found helpful while writing this kernel. I do not assume them to be great articles; neither do I recommend them. I mentioned them because I have found them to be helpful. \n\n## Statistics\n* [What Is a t-test? And Why Is It Like Telling a Kid to Clean Up that Mess in the Kitchen?](https://blog.minitab.com/blog/statistics-and-quality-data-analysis/what-is-a-t-test-and-why-is-it-like-telling-a-kid-to-clean-up-that-mess-in-the-kitchen)\n* [What Are T Values and P Values in Statistics?](https://blog.minitab.com/blog/statistics-and-quality-data-analysis/what-are-t-values-and-p-values-in-statistics)\n* [What is p-value? How we decide on our confidence level.](https://www.youtube.com/watch?v=E4KCfcVwzyw)\n\n\n\n***\n\nIf you like to discuss any other projects or have a chat about data science topics, I'll be more than happy to connect with you on:\n\n**LinkedIn:** https://www.linkedin.com/in/masumrumi/ \n\n**My Website:** http://masumrumi.com/ \n\n*** This kernel is a work in progress. I will always incorporate new concepts of data science as I master them. This journey of learning is worth sharing as well as collaborating. Therefore any comments about further improvements would be genuinely appreciated.***\n***\n## If you have come this far, Congratulations!!\n\n## If this notebook helped you in any way, please upvote!!\n\n\n"
 },
 {
  "repo": "DanielGunna/Titanic-Machine-Learning",
  "language": "Python",
  "readme_contents": "# KappaVoid\n\nA team of (aspiring?) Data Scientists having adventures at Kaggle. Here we will describe our approach to the Titanic problem.\n\n## Titanic Problem \n\nBased on the sinking of the RMS Titanic, that ended up killing 1502 out of 2224 passengers and crew. One of the reasons for such loss was that there were not enough lifeboats for the passengers and crew. Some groups of people were more likely to survive than others. In this challenge you are requested to **analyse** data applying *machine learning* and **predict** which passenger survived the tragedy\n**Link: https://www.kaggle.com/c/titanic**\n\n## Models Implemented\n\nRight now we have implemented 9 *Machine Learning* models.\n\n1) RandomForest\n2) LinearSVC\n3) Stochasthic\n4) Gradient Descent\n5) Gaussian Naive Bayes\n6) K-Neighbors\n7) Perceptron\n8) DecisionTree\n9) Logistic Regression\n\n## Data Insights\n\nHere are some insights we had analysing the Data.\n- Pclass, Sex, Cabin and Embarked are **Categorical _features_**.\n- Comparing *Genders*, Females are way more likely to survive.\n- The fares didn't contribute much to the model\n- We decided to unite Age and Pclass due to the correlation with results\n- Names are unique in the dataset, so they are useless without preprocessing\n- Dividing age *feature* by groups is important to improve Machine Learning performance. \n"
 },
 {
  "repo": "paulhendricks/titanic",
  "language": "R",
  "readme_contents": "---\noutput:\n  github_document\n---\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n```{r, echo = FALSE}\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#>\",\n  fig.path = \"README-\"\n)\n```\n\n# titanic\n\n[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/titanic)](http://cran.r-project.org/package=titanic)\n[![Downloads from the RStudio CRAN mirror](http://cranlogs.r-pkg.org/badges/titanic)](http://cran.rstudio.com/package=titanic)\n[![Build Status](https://travis-ci.org/paulhendricks/titanic.png?branch=master)](https://travis-ci.org/paulhendricks/titanic)\n[![Build status](https://ci.appveyor.com/api/projects/status/rux3xlfcdhuxuw4t/branch/master?svg=true)](https://ci.appveyor.com/project/paulhendricks/titanic/branch/master)\n[![Project Status: Active - The project has reached a stable, usable state and is being actively developed.](http://www.repostatus.org/badges/0.1.0/active.svg)](http://www.repostatus.org/#active)\n\n`titanic` is an R package containing data sets providing information on the fate of passengers on the fatal maiden voyage of the ocean liner \"Titanic\", with variables such as economic status (class), sex, age and survival. These data sets are often used as an introduction to machine learning on [Kaggle](https://www.kaggle.com/). More details about the competition can be found [here](https://www.kaggle.com/c/titanic), and the original data sets can be found [here](https://www.kaggle.com/c/titanic/data).\n\n## Installation\n\nYou can install the latest development version from CRAN:\n\n```R\ninstall.packages(\"titanic\")\n````\n\nOr from GitHub with:\n\n```R\nif (packageVersion(\"devtools\") < 1.6) {\n  install.packages(\"devtools\")\n}\ndevtools::install_github(\"paulhendricks/titanic\")\n```\n\nIf you encounter a clear bug, please file a [minimal reproducible example](http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example) on [GitHub](https://github.com/paulhendricks/titanic/issues).\n\n## Citation\n\nTo cite package \u2018titanic\u2019 in publications use:\n\n```\nPaul Hendricks (2015). titanic: Titanic Passenger Survival Data Set. R package version 0.1.0. https://github.com/paulhendricks/titanic\n```\n\nA BibTeX entry for LaTeX users is\n\n```\n@Manual{,\n  title = {titanic: Titanic Passenger Survival Data Set},\n  author = {Paul Hendricks},\n  year = {2015}, \n  note = {R package version 0.1.0},\n  url = {https://github.com/paulhendricks/titanic},\n}\n```\n"
 },
 {
  "repo": "raybuhr/plumber-titanic",
  "language": "R",
  "readme_contents": "# plumber-titanic\n\nCode for [blog post 2017/10/27](https://raybuhr.github.io/2017/10/making-predictions-over-http/)\n\nDemonstrates how we can take a trained predictive model (i.e. statistical learning model or machine learning model) and deploy it to a server as a RESTful API that accepts JSON requests and returns JSON responses. To do this, we will use the package [plumber](https://www.rplumber.io/).\n\nThere are two main files:\n\n- `titanic-api.R` which uses functions in R as the logic for our REST API\n- `server.R` which sources the functions and uses the plumber annotations to generate web API endpoints, then starts a local webserver\n\nThe API here has three main components:\n\n1. Landing Page - presents a simple HTML page explaining the API at `/`I\n\n![](screenshots/plumber_landing_page_screenshot.png)\n\n2. Health Check - a generic endpoint at `/healthcheck` used to test if server is responding\n\n![](screenshots/plumber_healthcheck_screenshot.png)\n\n3. Survival Prediction - the main goal, a RESTful HTTP API \n\n  - responds to url query string or JSON body payload requests with a probability of survival\n\n![](screenshots/plumber_survival_prediction_screenshot.png)\n\n  - provides useful responses when requests features don't meet expectations\n\n![](screenshots/plumber_survival_error_screenshot.png)\n\n\nAnd over course, this works from the command line as well. Here is hitting with a simple curl request (piped into jq).\n\n![](screenshots/plumber_curl_screenshot.png)\n"
 },
 {
  "repo": "ageitgey/titanic_machine_learning_example",
  "language": "Python",
  "readme_contents": "titanic_machine_learning_example\n================================\n\nA simple example of how to solve Kaggle's\n\"[Titanic: Machine Learning from Disaster](http://www.kaggle.com/c/titanic-gettingStarted)\"\nchallenge using Python and scikit-learn.\n\nThis simple example will get you about 78% accuracy. It shows you how to instantiate\nand use various classifiers in scikit-learn.\n\nNote: This example combines six different classifiers, just as example of\nhow to run and average multiple classifiers.  You can actually get a better\naccuracy by being smarter about how to combine classifiers and which ones to use.\nThis just shows you the scikit-learn syntax.\n\nThis example also assumes you've already done a grid search and found the best\nhyper parameters for your classifiers (especially the SVM). But if you aren't\nsure how to do that, the scikit-learn docs have a\n[good example](http://scikit-learn.org/stable/auto_examples/grid_search_digits.html#example-grid-search-digits-py)\nthat you can copy.\n"
 },
 {
  "repo": "yangvnks/titanic-classification",
  "language": "Jupyter Notebook",
  "readme_contents": "# Titanic survivor classification challenge\nTitanic classification [challenge on Kaggle](https://www.kaggle.com/c/titanic).\nGiven a dataset of a subset of the Titanic's passengers predict whether they will survive or not.\n\n## Credits\n* Claudia Chianella ([@clauchian](https://github.com/clauchian))\n* Yannick Giovanakis ([@yangvnks](https://github.com/yangvnks))\n* Flavio Primo ([@flaprimo](https://github.com/flaprimo/))\n* Francesco Zinnari ([@FrancescoZinnari](https://github.com/FrancescoZinnari))\n\n## Method\nBelow are provided the steps that were followed for this project. Each step and classifiers have their own document.\n\n1. **Data visualization**: data analysis to understand missing values, data relations and usefulness of features\n2. **Preprocessing**: with the knowledge acquired with the preceding step, apply preprocessing of data including dealing with missing values, drop unuseful features and build new features\n3. **Classifier**: build classifiers based on the preprocessed data using a variety of techniques\n\n## Classification techniques\nClassification techniques together with the relative scores.\n\n| Classifier | Test set score | CV score | Kaggle score |\n| ------ |:------:|:------:|:------:|\n| *KNN* | - | - | - |\n| *Logistic Regression* | - | 0.82 | 0.78947 |\n| *Neural Networks* | - | - | - |\n| *Random Forest* | 0.82 | 0.84 | 0.79425 |\n| *Support Vector Machines* | 0.85 | 0.84 | 0.80861 |\n| *Perceptron* | 0.78 | - | 0.62679 |\n| *Naive Bayes* | 0.78 | 0.80 | 0.76076 |\n\n## Folder structures\n* `\\` contains all of the jupyter's notebooks including classifiers, preprocessing and data visualization\n* `\\Data` contains the project dataset given in the Kaggle challenge\n* `\\Data\\outputs` contains the outputs given by the classifiers that were submitted to Kaggle\n\n## Installation instructions\n1. Install Python and clone this repository\n2. Install required Python modules with `pip install -r requirements.txt`\n\nto run the [jupyter](http://jupyter.org/)'s notebooks just go with `jupyter notebook`\n"
 },
 {
  "repo": "banga9nishant/survival",
  "language": "Jupyter Notebook",
  "readme_contents": "# survival forecast\nsurvival forecast of titanic by logistic regression\n"
 },
 {
  "repo": "Geoyi/Cleaning-Titanic-Data",
  "language": "R",
  "readme_contents": "# Cleaning-Titanic-Data\n##About the data\n One of the most popular starter data sets in data science, the Titanic data set. This is a data set that records various attributes of passengers on the Titanic, including who survived and who didn\u2019t. \n##What did I do\n Here I have detected some missing value, replace the missing values and also create new values added to the dataset. \n ![rplot](https://cloud.githubusercontent.com/assets/14057932/15985130/22c7cf80-2fa9-11e6-8c4e-8f0cd123dfda.png)\n                       \n                        here is the overview of the missing value in the original dataset.\n##The output\n There are two csv files, first one is titanic_original.csv and second one is tatanic_clean.csv. Second csv is generated from the R code, called 'titanic.r' here. Have fun.\n![titanic data](https://cloud.githubusercontent.com/assets/14057932/15985178/a403fe60-2faa-11e6-999e-d826230f603a.png)\n                 \n                  camparision between the titanic_original.csv and tatanic_clean.csv\n"
 },
 {
  "repo": "mneedham/kaggle-titanic",
  "language": "Python",
  "readme_contents": "## Steps to get going with this\n\n### Install virtual env and create a profile for kaggle\n\n    pip install https://github.com/pypa/virtualenv/tarball/develop\n    virtualenv kaggle\n    . kaggle/bin/activate\n\n### Install all the things\n\n    pip install numpy scikit-learn pandas scipy sklearn-pandas\n\n### Generate some entries for the Kaggle Titanic problem\n\n    # Simple version using tutorial from the website\n    python titanic.ml\n    \n    # Version using the ExtraTreesClassifier from scikit-learn\n    python titanic-ml.py\n    \nExploration of which classifier would work best can be seen at:\n\n    python titanic-ml-explore.py\n"
 },
 {
  "repo": "jakesherman/titanic-kaggle",
  "language": "Jupyter Notebook",
  "readme_contents": "# titanic-kaggle\n\n> The sinking of the RMS Titanic is one of the most infamous shipwrecks in history...In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to *predict which passengers survived the tragedy*.\n\nThis is an introductory Kaggle challenge where the goal is to predict which passengers survived the sinking of the Titanic based on a set of attributes of the passengers, including name, gender, age, and more.\n\n## Feature engineering\n\nAfter taking an initial stab at feature engineering, I took some ideas from [Megan Risdal](https://www.kaggle.com/mrisdal/titanic/exploring-survival-on-the-titanic/discussion) and [piplimoon](https://www.kaggle.com/piplimoon/titanic/leaderboard-0-8134). One of the fun parts about this challenge was seeing all of the creative ideas that others have thought up. To summarise what I did:\n\n* Extracted a person's title (Mr, Mrs, Miss, Col, etc.) from the person's name\n* Created a family size feature by adding up the number of siblings/spouses and parents/children on board\n* Created a family variable from people's last names and their family size - since non-related people can share last names, last name + family size should be a good proxy for a specific family\n* Used the ticket feature (where multiple people can share a ticket) only for cases where a ticket was shared by two or more people across the training and test sets (ths result is a bit of bleeding between the training and test sets)\n* Figured out which deck a person's cabin was on from the cabin feature\n* Used one-hot encoding to create dummies for categorical features\n* Used the `fancyimpute` package to impute missing values using MICE\n\n## Modeling\n\nI used 5-fold grid search to choose hyperparameters and do model selection. I tried logistic regression, KNN, random forest, SVM, and gradient boosted trees models. They all performed reasonably well (accuracy in the ~ .78 - .82 range) except for KNN. My best score on the public leaderboard was from creating a majority voting ensemble of the four reasonably well performing models but giving the random forest model 2 votes (out of 5), giving a score of ~ .825.\n\n## To run\n\nUses Python 2.7, tested on Ubuntu 14.04 LTS.\n\n```bash\npython project.py --name <FILE-NAME>\n```\n\nArguments:\n* `--name` required, name of the resulting .csv file to create\n* `--findhyperparameters` if you don't include this argument the script uses pre-optimized hyperparameters - including this argument results in grid search being used to optimize the hyperparameters. This takes ~ 1 - 1.5 hours depending on your machine.\n"
 },
 {
  "repo": "r-hassan/mlbook-titanic",
  "language": null,
  "readme_contents": "# \u09ae\u09c7\u09b6\u09bf\u09a8 \u09b2\u09be\u09b0\u09cd\u09a8\u09bf\u0982: \u09b9\u09be\u09a4\u09c7 \u0995\u09b2\u09ae\u09c7\n\n### '_\u099f\u09be\u0987\u099f\u09be\u09a8\u09bf\u0995 '\u09aa\u09cd\u09b0\u099c\u09c7\u0995\u09cd\u099f \u0993\u09df\u09be\u09b0\u09cd\u0995\u09ac\u09c1\u0995_\n\n---\n\n> #### A breakthrough in machine learning would be worth ten Microsofts.\n>\n> #### \u2014 Bill Gates\n\n### [\u0995\u09cd\u09af\u09be\u0997\u09b2 \u09aa\u09cd\u09b0\u09a4\u09bf\u09af\u09cb\u0997\u09bf\u09a4\u09be: \u099f\u09be\u0987\u099f\u09be\u09a8\u09bf\u0995](https://www.kaggle.com/c/titanic/ \"\u099f\u09be\u0987\u099f\u09be\u09a8\u09bf\u0995 \") - kaggle.com/c/titanic![](/assets/titanic-kaggle.JPG)\n\n_**\u0996\u09b8\u09dc\u09be \u09ad\u09be\u09b0\u09cd\u09b8\u09a8: \u09e7.\u09e6\u09e6 \u09b0\u09bf\u09ad\u09bf\u09b6\u09a8 \u09e9\u0964 \u09b8\u09ae\u09df\u09b8\u09c0\u09ae\u09be: \u0985\u0995\u09cd\u099f\u09cb\u09ac\u09b0-\u09a8\u09ad\u09c7\u09ae\u09cd\u09ac\u09b0 \u09e8\u09e6\u09e7\u09ed **_\n\n> \u09aa\u09cd\u09b0\u099a\u09c1\u09b0 \u09aa\u09b0\u09bf\u09ac\u09b0\u09cd\u09a4\u09a8 \u09aa\u09cd\u09b0\u09df\u09cb\u099c\u09a8, \u099a\u09cd\u09af\u09be\u09aa\u09cd\u099f\u09be\u09b0 \u09b9\u09ac\u09c7 \u09ef\u099f\u09be\n\n\u09b6\u09c1\u09b0\u09c1\u09a4\u09c7\u0987 \u201c\u09b9\u09be\u09a4\u09c7 \u0995\u09b2\u09ae\u09c7\u201d\u09b0 \u09b0\u09cb\u09a1\u09ae\u09cd\u09af\u09be\u09aa\n\n\\(\u0986\u09b8\u09b2 \"\u099f\u09c7\u09ac\u09bf\u09b2 \u0985\u09ab \u0995\u09a8\u099f\u09c7\u09a8\u09cd\u099f\" \u09ac\u09be\u0981 \u09aa\u09be\u09b6\u09c7\u09b0 \u09aa\u09cd\u09af\u09be\u09a8\u09c7\u0964 \u0995\u09cd\u09b2\u09bf\u0995 \u0995\u09b0\u09c1\u09a8 \u09ac\u09be\u0981 \u09aa\u09be\u09b6\u09c7\u0964 \u09ae\u09cb\u09ac\u09be\u0987\u09b2\u09c7\u09b0 \u099c\u09a8\u09cd\u09af \u099a\u09be\u09aa \u09a6\u09bf\u09a8 \u0993\u09aa\u09b0\u09c7\u09b0 \"\u09ac\u09be\u09b0\" \u09ac\u09be\u099f\u09a8\\)\n\n\u09ae\u09c7\u09b6\u09bf\u09a8 \u09b2\u09be\u09b0\u09cd\u09a8\u09bf\u0982: \u09b9\u09be\u09a4\u09c7 \u0995\u09b2\u09ae\u09c7\n\n\u09ae\u09c1\u0996\u09ac\u09a8\u09cd\u09a7\n\n\u0995\u09be\u09a6\u09c7\u09b0 \u099c\u09a8\u09cd\u09af \u09ac\u0987\u099f\u09be?\n\n\u09aa\u09b0\u09cd\u09ac \u09e6: \u09aa\u09be\u09b2\u09cd\u099f\u09c7 \u09af\u09be\u09ac\u09be\u09b0 \u0998\u099f\u09a8\u09be\n\n* \u09e6.\u09e7. \u09b8\u09bf\u09a1\u09bf\u09b8\u09bf\u2019\u09b0 \u0993\u09af\u09bc\u09be\u09b0\u09cd\u09a8\u09bf\u0982\n* \u09e6.\u09e8. \u0987\u09a8\u09cd\u099f\u09be\u09b0\u0995\u09be\u09a8\u09c7\u0995\u09b6\u09a8 \u0995\u09b8\u09cd\u099f \u09ae\u09a1\u09c7\u09b2\u09bf\u0982\n* \u09e6.\u09e9. \u09ae\u09be\u09a8\u09c1\u09b7\u09c7\u09b0 \u09aa\u09be\u09b6\u09c7 \u09a1\u09be\u099f\u09be\n* \u09e6.\u09ea. \u0997\u09c1\u0997\u09b2 \u09ab\u099f\u09cb\u099c\n* \u09e6.\u09eb. \u09b8\u09b0\u0995\u09be\u09b0\u09bf \u0993\u09aa\u09c7\u09a8 \u09a1\u09be\u099f\u09be\n* \u09e6.\u09ec. \u0986\u09b0\u09cd\u099f\u09bf\u09ab\u09bf\u09b8\u09bf\u09df\u09be\u09b2 \u0987\u09a8\u09cd\u099f\u09c7\u09b2\u09bf\u099c\u09c7\u09a8\u09cd\u09b8 - \u09ae\u09c7\u09b6\u09bf\u09a8 \u09b2\u09be\u09b0\u09cd\u09a8\u09bf\u0982\u098f\u09b0 \u09aa\u09cd\u09b0\u09b8\u09be\u09b0\n\n\u09ae\u09c7\u09b6\u09bf\u09a8 \u09b2\u09be\u09b0\u09cd\u09a8\u09bf\u0982 \u09b9\u09cd\u09af\u09be\u0995\n\n\u09ad\u09ac\u09bf\u09b7\u09cd\u09af\u09a4 \u09a6\u09c7\u0996\u09be\u09b0 \u09a7\u09be\u09b0\u09a3\u09be\n\n\u09aa\u09b0\u09cd\u09ac \u09e7: \u09ae\u09c7\u09b6\u09bf\u09a8 \u09b2\u09be\u09b0\u09cd\u09a8\u09bf\u0982 \u0995\u09bf? \\(\u09e9\u09e6 \u09ae\u09bf\u09a8\u09bf\u099f\\)\n\n* \u09e7.\u09e7. \u09ae\u09c7\u09b6\u09bf\u09a8 \u09b2\u09be\u09b0\u09cd\u09a8\u09bf\u0982 \u099c\u09bf\u09a8\u09bf\u09b8\u099f\u09be \u0995\u09bf?\n* \u09e7.\u09e8. \u0995\u09c7\u09a8 \u09a6\u09b0\u0995\u09be\u09b0 \u09ae\u09c7\u09b6\u09bf\u09a8 \u09b2\u09be\u09b0\u09cd\u09a8\u09bf\u0982?\n* \u09e7.\u09e9. \u0995\u09c7\u09a8\u0987 \u09ac\u09be \u09a6\u09b0\u0995\u09be\u09b0 \u0995\u09c3\u09a4\u09cd\u09b0\u09bf\u09ae \u09ac\u09c1\u09a6\u09cd\u09a7\u09bf\u09ae\u09a4\u09cd\u09a4\u09be?\n* \u09e7.\u09ea. \u09b6\u09bf\u0995\u09cd\u09b7\u09be\u0997\u09a4 \u09af\u09cb\u0997\u09cd\u09af\u09a4\u09be: \u0995\u09c7\u09a8 \u098f\u0995\u09be\u09a6\u09b6 \u09b6\u09cd\u09b0\u09c7\u09a3\u09bf\u09b0 \u09b6\u09bf\u0995\u09cd\u09b7\u09be\u09b0\u09cd\u09a5\u09c0?\n\n\u09e7.\u09eb. \u0995\u09bf\u09ad\u09be\u09ac\u09c7 \u09b6\u09bf\u0996\u09ac\u09c7\u09a8?\n\n\u09aa\u09b0\u09cd\u09ac \u09e8: \u0995\u09cd\u09af\u09be\u0997\u09b2 \u09aa\u09cd\u09b0\u09a4\u09bf\u09af\u09cb\u0997\u09bf\u09a4\u09be \\(\u09ea \u09b8\u09aa\u09cd\u09a4\u09be\u09b9-\u09ec \u09b8\u09aa\u09cd\u09a4\u09be\u09b9\\)\n\n* \u09e8.\u09e7. \u2018\u0995\u09cd\u09af\u09be\u0997\u09b2\u2019 \u0995\u09bf \u0986\u09b0 \u09a6\u09b0\u0995\u09be\u09b0\u0987 \u09ac\u09be \u0995\u09c7\u09a8?\n* \u09e8.\u09e8. \u0995\u09bf \u0995\u09b0\u09a4\u09c7 \u09b9\u09ac\u09c7 \u0995\u09cd\u09af\u09be\u0997\u09b2\u09c7?\n* \u09e8.\u09e9. \u09a5\u09bf\u0993\u09b0\u09bf \u09ac\u09be\u09a6, \u0995\u09c7\u09a8 \u09aa\u09cd\u09b0\u099c\u09c7\u0995\u09cd\u099f \u09a6\u09bf\u09df\u09c7 \u09b6\u09c1\u09b0\u09c1?\n* \u09e8.\u09ea. \u0995\u09c7\u09a8 \u09b6\u09c1\u09b0\u09c1\u09a4\u09c7\u0987 \u2018\u0986\u09b0\u2019 \u09aa\u09cd\u09b0\u09cb\u0997\u09cd\u09b0\u09be\u09ae\u09bf\u0982 \u098f\u09a8\u09ad\u09be\u09df\u09b0\u09a8\u09ae\u09c7\u09a8\u09cd\u099f?\n* \u09e8.\u09eb. \u099f\u09be\u0987\u099f\u09be\u09a8\u09bf\u0995\u09c7\u09b0 \u0997\u09b2\u09cd\u09aa\n\n\u09aa\u09b0\u09cd\u09ac \u09e9: \"\u0986\u09b0\" \u098f\u09a8\u09ad\u09be\u09df\u09b0\u09a8\u09ae\u09c7\u09a8\u09cd\u099f\n\n* \u09e9.\u09e7. \"\u0986\u09b0\" + \"\u0986\u09b0\" \u09b7\u09cd\u099f\u09c1\u09a1\u09bf\u0993\n* \u09e9.\u09e8. \u0986\u09b0 \u09b7\u09cd\u099f\u09c1\u09a1\u09bf\u0993\u09b0 \u0995\u09bf\u099b\u09c1 \u0996\u09c1\u0981\u099f\u09bf\u09a8\u09be\u099f\u09bf \\(\u0987\u09a8\u09b8\u09cd\u099f\u09b2\u09c7\u09b6\u09a8 \u09b8\u09b9\\)\n* \u09e9.\u09e9. '\u0986\u09b0' \u098f\u09b0 \u0995\u09bf\u099b\u09c1 \u0995\u09be\u099c \\(\u09a4\u09bf\u09a8 \u09ae\u09bf\u09a8\u09bf\u099f\\)\n\n\u09aa\u09b0\u09cd\u09ac \u09ea: \u09aa\u09cd\u09b0\u099c\u09c7\u0995\u09cd\u099f \u099f\u09be\u0987\u099f\u09be\u09a8\u09bf\u0995: \u09ac\u09bf\u09aa\u09b0\u09cd\u09af\u09df\u09c7 \u09ae\u09c7\u09b6\u09bf\u09a8 \u09b2\u09be\u09b0\u09cd\u09a8\u09bf\u0982\n\n* \u09ea.\u09e7. \u0995\u09c7\u09a8 \u09aa\u09cd\u09b0\u099c\u09c7\u0995\u09cd\u099f \"\u099f\u09be\u0987\u099f\u09be\u09a8\u09bf\u0995 \"?\n* \u09ea.\u09e8. \u2018\u099f\u09cd\u09b0\u09c7\u09a8\u09bf\u0982\u2019 \u0986\u09b0 \u2018\u099f\u09c7\u09b8\u09cd\u099f\u2019 \u09a1\u09be\u099f\u09be \u09b8\u09c7\u099f\n* \u09ea.\u09e9. \u0997\u09bf\u099f\u09b9\u09be\u09ac \u09b8\u09cd\u0995\u09cd\u09b0\u09bf\u09aa\u09cd\u099f\n* \u09ea.\u09ea. \u0985\u09ab\u09b2\u09be\u0987\u09a8 \u09a8\u09cb\u099f\n\n\u09aa\u09b0\u09cd\u09ac \u09eb: \u09aa\u09be\u0987\u09a5\u09a8 \u09a6\u09bf\u09df\u09c7 \u2018\u0995\u09cd\u09af\u09be\u0997\u09b2\u2019 \u09aa\u09cd\u09b0\u09a4\u09bf\u09af\u09cb\u0997\u09bf\u09a4\u09be\n\n* \u09eb.\u09e7. \u09aa\u09be\u0987\u09a5\u09a8 \u0995\u09c7\u09a8?\n* \u09eb.\u09e8. \u09aa\u09be\u0987\u09a5\u09a8 \u09a8\u09bf\u09df\u09c7 \u0996\u09c1\u0981\u099f\u09bf\u09a8\u09be\u099f\u09bf\n\n\u09aa\u09b0\u09cd\u09ac \u09ec: \u0995\u09bf \u0986\u099b\u09c7 \u09b8\u09be\u09ae\u09a8\u09c7?\n\n* \u09ec.\u09e7. \u0986\u09b8\u09b2\u09c7\u0987 \u0995\u09bf \u0986\u099b\u09c7 \u09b8\u09be\u09ae\u09a8\u09c7?\n* \u09ec.\u09e8. \u0986\u09a8-\u09b8\u09c1\u09aa\u09be\u09b0\u09ad\u09be\u0987\u099c\u09a1 \u09b2\u09be\u09b0\u09cd\u09a8\u09bf\u0982\n* \u09ec.\u09e9. \u09b0\u09bf-\u0987\u09a8\u09ab\u09cb\u09b0\u09cd\u09b8\u09ae\u09c7\u09a8\u09cd\u099f \u09b2\u09be\u09b0\u09cd\u09a8\u09bf\u0982\n\n\u09af\u09cb\u0997\u09be\u09af\u09cb\u0997\u09c7\u09b0 \u09b8\u09c1\u09a4\u09cb\n\n.......... \\(\u099a\u09b2\u09ac\u09c7\\)\n\n---\n\n\u09aa\u09cd\u09b0\u09df\u09cb\u099c\u09a8\u09c0\u09df \u09af\u09cb\u0997\u09be\u09af\u09cb\u0997: [https://github.com/r-hassan](https://github.com/r-hassan) +\u09ee\u09ee\u09e6\u09e7\u09ed\u09e7\u09e9\u09e6\u09ef\u09eb\u09ed\u09ec\u09ed \u0964 \u09ab\u09c7\u0987\u09b8\u09ac\u09c1\u0995 \u09ae\u09c7\u09b8\u09c7\u099e\u09cd\u099c\u09be\u09b0\n\n\u09b2\u09bf\u0982\u0995\u09a1\u0987\u09a8: [https://www.linkedin.com/in/raqueeb](https://www.linkedin.com/in/raqueeb) \u09ab\u09c7\u0987\u09b8\u09ac\u09c1\u0995 [https://www.facebook.com/raqueeb](https://www.facebook.com/raqueeb)\n\n\u0987\u098a\u099f\u09bf\u098a\u09ac \u099a\u09cd\u09af\u09be\u09a8\u09c7\u09b2 https://goo.gl/aiWJso \u09ab\u09c7\u0987\u09b8\u09ac\u09c1\u0995 \u09aa\u09c7\u099c https://www.facebook.com/mltraining/\n\n"
 },
 {
  "repo": "YLTsai0609/Kaggle-Titanic-Top3-percent",
  "language": "Jupyter Notebook",
  "readme_contents": "# Kaggle-Titanic-Survival-prediction\n[Kaggle-Titanic-Survival-prediction](https://www.kaggle.com/c/titanic)<br>\nE-mail : [yltsai0609@gmail.com](yltsai0609@gmail.com) <br>\n**********************************************\n\u7d50\u8ad6 : Titanic\u70ba\u6a5f\u5668\u5b78\u7fd2\u7684\u5165\u9580\u6b3e\u9805\u76ee\uff0cKaggle\u4e0a\u6709\u8a31\u591a\u89e3\u6cd5\u53ca\u601d\u8def\uff0c\n\u7136\u800c\u6c92\u6709\u4e00\u7bc7\u4f7f\u7528\u8f03\u5c11\u7684\u7279\u5fb5\u9054\u5230\u8f03\u597d\u7684\u6548\u679c\uff0c\u591a\u534a\u4f7f\u7528\u5927\u91cf\u7684\u8abf\u53c3\u6280\u5de7\uff0c\n\u5c11\u91cf\uff0c\u6709\u54c1\u8cea\u7684\u7279\u5fb5\u63d0\u4f9b\u7a69\u5b9a\u7684\u9810\u6e2c\u54c1\u8cea\uff0c\u5feb\u901f\u7684\u8a13\u7df4\u53ca\u9810\u6e2c\uff0c<br>\n\u4f60\u53ef\u4ee5\u5728\u9019\u88e1\u627e\u5230\u4f7f\u7528\u524d\u9805\u9078\u64c7\u6cd5\u4f86\u505a\u7279\u5fb5\u9078\u64c7\u7684Kernel: \n* [Recursive Forward Elimination Workflow to 0.82296](https://github.com/YLTsai0609/Kaggle-Titanic-Top3-percent/blob/master/Recursive%20Forward%20Elimination%20Workflow%20to%200.82296.ipynb)<br>\n\u95dc\u65bc\u4e2d\u6587\u7248\u7684\u5efa\u6a21\u601d\u8def\u53ef\u4ee5\u5728\u9019\u7bc7Medium\u6587\u7ae0\u627e\u5230:\n[The post on Medium](https://medium.com/@yulongtsai/https-medium-com-yulongtsai-titanic-top3-8e64741cc11f)<br>\n**********************************************\n<b>\u4ee5\u4e0b\u6703\u91dd\u5c0d\u7f3a\u5931\u503c\u586b\u88dc\u7684\u65b9\u5f0f\uff0c\u7279\u5fb5\u5de5\u7a0b\u53ca\u6d89\u53ca\u7684encoding\u65b9\u5f0f\u9032\u884c\u8aaa\u660e</b>\n\n### \u7f3a\u5931\u8077\u586b\u88dc\n\u5e74\u9f61(Age)\u70ba\u4e00\u9805\u91cd\u8981\u7684\u7279\u5fb5\uff0c\u7531\u65bc\u672c\u8cc7\u6599\u96c6\u7684\u7279\u6027\uff0c\u8001\u5f31\u5a66\u5b7a\u512a\u5148\u4e0b\u8239\uff0c\u4f7f\u5f97\u5e74\u9f61\u548c\u548c\u5e74\u9f61\u9ad8\u80fd\u5920\u6709\u8f03\u9ad8\u7684\u5b58\u6d3b\u6a5f\u7387\uff0c\u91dd\u5c0d\u7f3a\u5931\u503c\uff0c\u6211\u5011\u63a1\u7528\u7684\u65b9\u6cd5\u70ba\u4f7f\u7528\u59d3\u540d\u7a31\u8b02\u4e2d\u4f4d\u6578\u9032\u884c\u586b\u88dc\n\n|\u7a31\u8b02|\u4e2d\u4f4d\u6578|\n|----|---|\n|Mr|29|\n|Rare|47|\n|Master|4|\n|Miss|22|\n|Mrs|36|\n\n\u6839\u64daEDA\u7684\u7d50\u679c\uff0c\u5728\u4e00\u4e8c\u8259\u7b49\u4e2d\u6211\u5011\u767c\u73fe\u5c0f\u65bc16\u6b72\u5e74\u9f61\u7684\u4e58\u5ba2\u6709\u8f03\u9ad8\u7684\u5b58\u6d3b\u7387\uff0c\n\u56e0\u6b64\u6211\u5011\u62ff16\u6b72\u7576\u4f5c\u5207\u5206\u503c\n\n|\u7279\u5fb5|\u503c|\n|---|--|\n|age < 16|1|\n|age > 16|0|\n\n### \u7279\u5fb5\u5de5\u7a0b\n\n|\u7279\u5fb5|\u8655\u7406\u65b9\u5f0f|\u52d5\u6a5f|\n|----|------|---|\n|Age|Binning\uff0c\u4f7f\u752816\u70ba\u95a5\u503c|\u6839\u64daEDA\u7684\u7d50\u679c\uff0c\u5728\u4e00\u4e8c\u8259\u7b49\u4e2d\u6211\u5011\u767c\u73fe\u5c0f\u65bc16\u6b72\u5e74\u9f61\u7684\u4e58\u5ba2\u6709\u8f03\u9ad8\u7684\u5b58\u6d3b\u7387\uff0c\u4e14binning\u53ef\u4ee5\u6709\u6548\u7684\u964d\u4f4eoverfitting|\n|Sex|\u76f4\u63a5\u4f7f\u7528|\u9ad8\u9810\u6e2c\u6027\u7279\u5fb5\uff0c\u7537\u6027\u591a\u534a\u5e6b\u52a9\u5973\u6027\u9003\u751f\uff0c\u7b26\u5408\u76f4\u89ba|\n|Pclass|\u76f4\u63a5\u4f7f\u7528|\u4e58\u5ba2\u7684\u793e\u6703\u5730\u4f4d\uff0c\u8259\u7b49\u8d8a\u9ad8\u5b58\u6d3b\u7387\u8d8a\u9ad8\uff0c\u7b26\u5408\u76f4\u89ba|\n|Fare|Binning\uff0c\u6309\u7167\u5206\u4f4d\u6578\u5207\u5206\u62104, 5, 6\u4efd\u6bd4\u8f03\u7d50\u679c|\u8239\u7968\u50f9\u683c\u8d8a\u9ad8\u8868\u793a\u5176\u793e\u6703\u5730\u4f4d\u8d8a\u9ad8\uff0c\u4e26\u4e14binning\u53ef\u4ee5\u6709\u6548\u7684\u964d\u4f4eoverfitting|\n|SibSp, Partch, Name|\u5efa\u7acbFamily_size\u7279\u5fb5\uff0c\u4ee5\u53caConnected-Survival\u7279\u5fb5|\u9023\u7d50\u5728\u4e00\u8d77\u7684\u5bb6\u4eba\u5f9eEDA\u4e2d\u767c\u73fe\u591a\u534a\u6709\u4e00\u8d77\u5b58\u6d3b\u6216\u662f\u4e00\u8d77\u70ba\u5b58\u6d3b\u7684\u73fe\u8c61\uff0c\u4ea6\u70ba\u4e00\u7a2eTarget Encoding|\n\n\n## 3\u7a2e\u7f3a\u5931\u8077\u586b\u88dc\u7684\u7b56\u7565\u6bd4\u8f03\n[3 Strategies analyzing Age and Their Impact](https://github.com/YLTsai0609/Kaggle-Titanic-Top3-percent/blob/master/3%20Strategies%20analyzing%20Age%20and%20Their%20Impact%20.ipynb)\n"
 },
 {
  "repo": "ashishpatel26/Titanic-Machine-Learning-from-Disaster",
  "language": "Jupyter Notebook",
  "readme_contents": "\n<h2>Titanic Passanger Survival Analysis</h2>\n\n\n```python\nfrom IPython.display import Image\nImage(url= \"https://static1.squarespace.com/static/5006453fe4b09ef2252ba068/5095eabce4b06cb305058603/5095eabce4b02d37bef4c24c/1352002236895/100_anniversary_titanic_sinking_by_esai8mellows-d4xbme8.jpg\")\n```\n\n\n\n\n<img src=\"https://static1.squarespace.com/static/5006453fe4b09ef2252ba068/5095eabce4b06cb305058603/5095eabce4b02d37bef4c24c/1352002236895/100_anniversary_titanic_sinking_by_esai8mellows-d4xbme8.jpg\"/>\n\n\n\n\n```python\nimport pandas as pd\nimport numpy as np\n```\n\n\n```python\ntrain = pd.read_csv(\"input/train.csv\")\ntest = pd.read_csv(\"input/test.csv\")\n```\n\n\n```python\ntrain.isnull().sum()\nprint(\"Train Shape:\",train.shape)\ntest.isnull().sum()\nprint(\"Test Shape:\",test.shape)\n```\n\n    Train Shape: (891, 12)\n    Test Shape: (418, 11)\n    \n\n\n```python\ntrain.info()\n```\n\n    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 891 entries, 0 to 890\n    Data columns (total 12 columns):\n    PassengerId    891 non-null int64\n    Survived       891 non-null int64\n    Pclass         891 non-null int64\n    Name           891 non-null object\n    Sex            891 non-null object\n    Age            714 non-null float64\n    SibSp          891 non-null int64\n    Parch          891 non-null int64\n    Ticket         891 non-null object\n    Fare           891 non-null float64\n    Cabin          204 non-null object\n    Embarked       889 non-null object\n    dtypes: float64(2), int64(5), object(5)\n    memory usage: 83.6+ KB\n    \n\n\n```python\ntest.info()\n```\n\n    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 418 entries, 0 to 417\n    Data columns (total 11 columns):\n    PassengerId    418 non-null int64\n    Pclass         418 non-null int64\n    Name           418 non-null object\n    Sex            418 non-null object\n    Age            332 non-null float64\n    SibSp          418 non-null int64\n    Parch          418 non-null int64\n    Ticket         418 non-null object\n    Fare           417 non-null float64\n    Cabin          91 non-null object\n    Embarked       418 non-null object\n    dtypes: float64(2), int64(4), object(5)\n    memory usage: 36.0+ KB\n    \n\n### Data Dictionary\n\n* Survived: 0 = No, 1 = Yes\n* pclass: Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd\n* sibsp: # of siblings / spouses aboard the Titanic\n* parch: # of parents / children aboard the Titanic\n* ticket: Ticket number\n* cabin: Cabin number\n* embarked: Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton\n\n**Total rows and columns**\n\nWe can see that there are 891 rows and 12 columns in our training dataset.\n\n\n```python\ntrain.head(10)\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Moran, Mr. James</td>\n      <td>male</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330877</td>\n      <td>8.4583</td>\n      <td>NaN</td>\n      <td>Q</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>0</td>\n      <td>1</td>\n      <td>McCarthy, Mr. Timothy J</td>\n      <td>male</td>\n      <td>54.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>17463</td>\n      <td>51.8625</td>\n      <td>E46</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Palsson, Master. Gosta Leonard</td>\n      <td>male</td>\n      <td>2.0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>349909</td>\n      <td>21.0750</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n      <td>female</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>347742</td>\n      <td>11.1333</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>1</td>\n      <td>2</td>\n      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n      <td>female</td>\n      <td>14.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>237736</td>\n      <td>30.0708</td>\n      <td>NaN</td>\n      <td>C</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\ntrain.describe()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>714.000000</td>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>891.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>446.000000</td>\n      <td>0.383838</td>\n      <td>2.308642</td>\n      <td>29.699118</td>\n      <td>0.523008</td>\n      <td>0.381594</td>\n      <td>32.204208</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>257.353842</td>\n      <td>0.486592</td>\n      <td>0.836071</td>\n      <td>14.526497</td>\n      <td>1.102743</td>\n      <td>0.806057</td>\n      <td>49.693429</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.420000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>223.500000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>20.125000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>7.910400</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>446.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>28.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>14.454200</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>668.500000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>38.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>31.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>891.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>80.000000</td>\n      <td>8.000000</td>\n      <td>6.000000</td>\n      <td>512.329200</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\ntest.describe()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Pclass</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>418.000000</td>\n      <td>418.000000</td>\n      <td>332.000000</td>\n      <td>418.000000</td>\n      <td>418.000000</td>\n      <td>417.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1100.500000</td>\n      <td>2.265550</td>\n      <td>30.272590</td>\n      <td>0.447368</td>\n      <td>0.392344</td>\n      <td>35.627188</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>120.810458</td>\n      <td>0.841838</td>\n      <td>14.181209</td>\n      <td>0.896760</td>\n      <td>0.981429</td>\n      <td>55.907576</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>892.000000</td>\n      <td>1.000000</td>\n      <td>0.170000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>996.250000</td>\n      <td>1.000000</td>\n      <td>21.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>7.895800</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1100.500000</td>\n      <td>3.000000</td>\n      <td>27.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>14.454200</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1204.750000</td>\n      <td>3.000000</td>\n      <td>39.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>31.500000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1309.000000</td>\n      <td>3.000000</td>\n      <td>76.000000</td>\n      <td>8.000000</td>\n      <td>9.000000</td>\n      <td>512.329200</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\ntrain.isnull().sum()\n```\n\n\n\n\n    PassengerId      0\n    Survived         0\n    Pclass           0\n    Name             0\n    Sex              0\n    Age            177\n    SibSp            0\n    Parch            0\n    Ticket           0\n    Fare             0\n    Cabin          687\n    Embarked         2\n    dtype: int64\n\n\n\n\n```python\ntest.isnull().sum()\ntest[\"Survived\"] = \"\"\ntest.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n      <th>Survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>892</td>\n      <td>3</td>\n      <td>Kelly, Mr. James</td>\n      <td>male</td>\n      <td>34.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330911</td>\n      <td>7.8292</td>\n      <td>NaN</td>\n      <td>Q</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>893</td>\n      <td>3</td>\n      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n      <td>female</td>\n      <td>47.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>363272</td>\n      <td>7.0000</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>894</td>\n      <td>2</td>\n      <td>Myles, Mr. Thomas Francis</td>\n      <td>male</td>\n      <td>62.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>240276</td>\n      <td>9.6875</td>\n      <td>NaN</td>\n      <td>Q</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>895</td>\n      <td>3</td>\n      <td>Wirz, Mr. Albert</td>\n      <td>male</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>315154</td>\n      <td>8.6625</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>896</td>\n      <td>3</td>\n      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n      <td>female</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3101298</td>\n      <td>12.2875</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n# Data Visualization using Matplotlib and Seaborn packages.\n\n\n```python\nimport matplotlib.pyplot as plt # Plot the graphes\n%matplotlib inline\nimport seaborn as sns\nsns.set() # setting seaborn default for plots\n```\n\n# Bar Chart for Categorical Features \n\n* Pclass\n* Sex\n* SibSp ( # of siblings and spouse)\n* Parch ( # of parents and children)\n* Embarked\n* Cabin\n\n\n```python\ndef bar_chart(feature):\n    survived = train[train['Survived']==1][feature].value_counts()\n    dead = train[train['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([survived,dead])\n    df.index = ['Survived','Dead']\n    df.plot(kind='bar',stacked=True, figsize=(10,5))\n```\n\n\n```python\nbar_chart('Sex')\nprint(\"Survived :\\n\",train[train['Survived']==1]['Sex'].value_counts())\nprint(\"Dead:\\n\",train[train['Survived']==0]['Sex'].value_counts())\n```\n\n    Survived :\n     female    233\n    male      109\n    Name: Sex, dtype: int64\n    Dead:\n     male      468\n    female     81\n    Name: Sex, dtype: int64\n    \n\n\n![png](output_17_1.png)\n\n\nThe Chart confirms **Women more likely survivied than Men**.\n\n\n```python\nbar_chart('Pclass')\nprint(\"Survived :\\n\",train[train['Survived']==1]['Pclass'].value_counts())\nprint(\"Dead:\\n\",train[train['Survived']==0]['Pclass'].value_counts())\n```\n\n    Survived :\n     1    136\n    3    119\n    2     87\n    Name: Pclass, dtype: int64\n    Dead:\n     3    372\n    2     97\n    1     80\n    Name: Pclass, dtype: int64\n    \n\n\n![png](output_19_1.png)\n\n\nThe Chart confirms **1st class** more likely survivied than **other classes**.  \nThe Chart confirms **3rd class** more likely dead than **other classes**\n\n\n```python\nbar_chart('SibSp')\nprint(\"Survived :\\n\",train[train['Survived']==1]['SibSp'].value_counts())\nprint(\"Dead:\\n\",train[train['Survived']==0]['SibSp'].value_counts())\n```\n\n    Survived :\n     0    210\n    1    112\n    2     13\n    3      4\n    4      3\n    Name: SibSp, dtype: int64\n    Dead:\n     0    398\n    1     97\n    4     15\n    2     15\n    3     12\n    8      7\n    5      5\n    Name: SibSp, dtype: int64\n    \n\n\n![png](output_21_1.png)\n\n\nThe Chart confirms a **person aboarded with more than 2 siblings or spouse** more likely survived.  \nThe Chart confirms a **person aboarded without siblings or spouse** more likely dead\n\n\n```python\nbar_chart('Parch')\nprint(\"Survived :\\n\",train[train['Survived']==1]['Parch'].value_counts())\nprint(\"Dead:\\n\",train[train['Survived']==0]['Parch'].value_counts())\n```\n\n    Survived :\n     0    233\n    1     65\n    2     40\n    3      3\n    5      1\n    Name: Parch, dtype: int64\n    Dead:\n     0    445\n    1     53\n    2     40\n    5      4\n    4      4\n    3      2\n    6      1\n    Name: Parch, dtype: int64\n    \n\n\n![png](output_23_1.png)\n\n\nThe Chart confirms a **person aboarded with more than 2 parents or children more likely survived.**  \nThe Chart confirms a **person aboarded alone more likely dead**\n\n\n```python\nbar_chart('Embarked')\nprint(\"Survived :\\n\",train[train['Survived']==1]['Embarked'].value_counts())\nprint(\"Dead:\\n\",train[train['Survived']==0]['Embarked'].value_counts())\n```\n\n    Survived :\n     S    217\n    C     93\n    Q     30\n    Name: Embarked, dtype: int64\n    Dead:\n     S    427\n    C     75\n    Q     47\n    Name: Embarked, dtype: int64\n    \n\n\n![png](output_25_1.png)\n\n\nThe Chart confirms a **person aboarded from C** slightly more likely survived.  \nThe Chart confirms a **person aboarded from Q** more likely dead.  \nThe Chart confirms a **person aboarded from S** more likely dead.  \n\n## 4. Feature engineering\n\nFeature engineering is the process of using domain knowledge of the data\nto create features (**feature vectors**) that make machine learning algorithms work.  \n\nfeature vector is an n-dimensional vector of numerical features that represent some object.\nMany algorithms in machine learning require a numerical representation of objects,\nsince such representations facilitate processing and statistical analysis.\n\n\n```python\ntrain.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n#### 4.1 how titanic sank?\n\n\n```python\nImage(url= \"https://static1.squarespace.com/static/5006453fe4b09ef2252ba068/t/5090b249e4b047ba54dfd258/1351660113175/TItanic-Survival-Infographic.jpg?format=1500w\")\n```\n\n\n\n\n<img src=\"https://static1.squarespace.com/static/5006453fe4b09ef2252ba068/t/5090b249e4b047ba54dfd258/1351660113175/TItanic-Survival-Infographic.jpg?format=1500w\"/>\n\n\n\n\n```python\ntrain.head(10)\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Moran, Mr. James</td>\n      <td>male</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330877</td>\n      <td>8.4583</td>\n      <td>NaN</td>\n      <td>Q</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>0</td>\n      <td>1</td>\n      <td>McCarthy, Mr. Timothy J</td>\n      <td>male</td>\n      <td>54.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>17463</td>\n      <td>51.8625</td>\n      <td>E46</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Palsson, Master. Gosta Leonard</td>\n      <td>male</td>\n      <td>2.0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>349909</td>\n      <td>21.0750</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n      <td>female</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>347742</td>\n      <td>11.1333</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>1</td>\n      <td>2</td>\n      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n      <td>female</td>\n      <td>14.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>237736</td>\n      <td>30.0708</td>\n      <td>NaN</td>\n      <td>C</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\ntrain_test_data = [train,test] # combine dataset\n\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n```\n\n\n```python\ntrain['Title'].value_counts()\n```\n\n\n\n\n    Mr          517\n    Miss        182\n    Mrs         125\n    Master       40\n    Dr            7\n    Rev           6\n    Mlle          2\n    Col           2\n    Major         2\n    Lady          1\n    Sir           1\n    Mme           1\n    Ms            1\n    Don           1\n    Countess      1\n    Capt          1\n    Jonkheer      1\n    Name: Title, dtype: int64\n\n\n\n\n```python\ntest['Title'].value_counts()\n```\n\n\n\n\n    Mr        240\n    Miss       78\n    Mrs        72\n    Master     21\n    Rev         2\n    Col         2\n    Dr          1\n    Ms          1\n    Dona        1\n    Name: Title, dtype: int64\n\n\n\n#### Title Map\n\nMr : 0   \nMiss : 1  \nMrs: 2  \nOthers: 3  \n\n\n```python\ntitle_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \n                 \"Master\": 3, \"Dr\": 3, \"Rev\": 3, \"Col\": 3, \"Major\": 3, \"Mlle\": 3,\"Countess\": 3,\n                 \"Ms\": 3, \"Lady\": 3, \"Jonkheer\": 3, \"Don\": 3, \"Dona\" : 3, \"Mme\": 3,\"Capt\": 3,\"Sir\": 3 }\n\nfor dataset in train_test_data:\n    dataset['Title'] = dataset[\"Title\"].map(title_mapping)\n```\n\n\n```python\ndataset.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n      <th>Survived</th>\n      <th>Title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>892</td>\n      <td>3</td>\n      <td>Kelly, Mr. James</td>\n      <td>male</td>\n      <td>34.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330911</td>\n      <td>7.8292</td>\n      <td>NaN</td>\n      <td>Q</td>\n      <td></td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>893</td>\n      <td>3</td>\n      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n      <td>female</td>\n      <td>47.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>363272</td>\n      <td>7.0000</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td></td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>894</td>\n      <td>2</td>\n      <td>Myles, Mr. Thomas Francis</td>\n      <td>male</td>\n      <td>62.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>240276</td>\n      <td>9.6875</td>\n      <td>NaN</td>\n      <td>Q</td>\n      <td></td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>895</td>\n      <td>3</td>\n      <td>Wirz, Mr. Albert</td>\n      <td>male</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>315154</td>\n      <td>8.6625</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td></td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>896</td>\n      <td>3</td>\n      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n      <td>female</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3101298</td>\n      <td>12.2875</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td></td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\ntest.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n      <th>Survived</th>\n      <th>Title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>892</td>\n      <td>3</td>\n      <td>Kelly, Mr. James</td>\n      <td>male</td>\n      <td>34.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330911</td>\n      <td>7.8292</td>\n      <td>NaN</td>\n      <td>Q</td>\n      <td></td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>893</td>\n      <td>3</td>\n      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n      <td>female</td>\n      <td>47.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>363272</td>\n      <td>7.0000</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td></td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>894</td>\n      <td>2</td>\n      <td>Myles, Mr. Thomas Francis</td>\n      <td>male</td>\n      <td>62.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>240276</td>\n      <td>9.6875</td>\n      <td>NaN</td>\n      <td>Q</td>\n      <td></td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>895</td>\n      <td>3</td>\n      <td>Wirz, Mr. Albert</td>\n      <td>male</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>315154</td>\n      <td>8.6625</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td></td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>896</td>\n      <td>3</td>\n      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n      <td>female</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3101298</td>\n      <td>12.2875</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td></td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\nbar_chart('Title')\n```\n\n\n![png](output_40_0.png)\n\n\n\n```python\n# delete unnecessary feature from dataset\ntrain.drop('Name', axis=1, inplace=True)\ntest.drop('Name', axis=1, inplace=True)\n```\n\n\n```python\ntrain.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n      <th>Title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\nsex_mapping = {\"male\": 0, \"female\": 1}\nfor dataset in train_test_data:\n    dataset['Sex'] = dataset['Sex'].map(sex_mapping)\n```\n\n\n```python\nbar_chart('Sex')\n```\n\n\n![png](output_44_0.png)\n\n\n\n```python\ntest.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n      <th>Survived</th>\n      <th>Title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>892</td>\n      <td>3</td>\n      <td>0</td>\n      <td>34.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330911</td>\n      <td>7.8292</td>\n      <td>NaN</td>\n      <td>Q</td>\n      <td></td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>893</td>\n      <td>3</td>\n      <td>1</td>\n      <td>47.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>363272</td>\n      <td>7.0000</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td></td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>894</td>\n      <td>2</td>\n      <td>0</td>\n      <td>62.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>240276</td>\n      <td>9.6875</td>\n      <td>NaN</td>\n      <td>Q</td>\n      <td></td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>895</td>\n      <td>3</td>\n      <td>0</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>315154</td>\n      <td>8.6625</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td></td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>896</td>\n      <td>3</td>\n      <td>1</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3101298</td>\n      <td>12.2875</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td></td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\ntrain[\"Age\"].fillna(train.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace= True)\ntest[\"Age\"].fillna(test.groupby('Title')['Age'].transform(\"median\"), inplace= True)\n```\n\n\n```python\ntrain.head(30)\n#train.groupby(\"Title\")[\"Age\"].transform(\"median\")\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n      <th>Title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>30.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330877</td>\n      <td>8.4583</td>\n      <td>NaN</td>\n      <td>Q</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>54.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>17463</td>\n      <td>51.8625</td>\n      <td>E46</td>\n      <td>S</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>349909</td>\n      <td>21.0750</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>347742</td>\n      <td>11.1333</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>14.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>237736</td>\n      <td>30.0708</td>\n      <td>NaN</td>\n      <td>C</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>4.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>PP 9549</td>\n      <td>16.7000</td>\n      <td>G6</td>\n      <td>S</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>58.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>113783</td>\n      <td>26.5500</td>\n      <td>C103</td>\n      <td>S</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>13</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>20.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>A/5. 2151</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>14</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>39.0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>347082</td>\n      <td>31.2750</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>15</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>14.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>350406</td>\n      <td>7.8542</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>16</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>55.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>248706</td>\n      <td>16.0000</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>17</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>382652</td>\n      <td>29.1250</td>\n      <td>NaN</td>\n      <td>Q</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>18</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>30.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>244373</td>\n      <td>13.0000</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>19</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>31.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>345763</td>\n      <td>18.0000</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>20</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2649</td>\n      <td>7.2250</td>\n      <td>NaN</td>\n      <td>C</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>21</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>239865</td>\n      <td>26.0000</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>22</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>34.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>248698</td>\n      <td>13.0000</td>\n      <td>D56</td>\n      <td>S</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>23</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>15.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330923</td>\n      <td>8.0292</td>\n      <td>NaN</td>\n      <td>Q</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>24</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>28.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>113788</td>\n      <td>35.5000</td>\n      <td>A6</td>\n      <td>S</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>25</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>8.0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>349909</td>\n      <td>21.0750</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>26</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>347077</td>\n      <td>31.3875</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>27</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>30.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2631</td>\n      <td>7.2250</td>\n      <td>NaN</td>\n      <td>C</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>28</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>19.0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>19950</td>\n      <td>263.0000</td>\n      <td>C23 C25 C27</td>\n      <td>S</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>29</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>21.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330959</td>\n      <td>7.8792</td>\n      <td>NaN</td>\n      <td>Q</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>30</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>30.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>349216</td>\n      <td>7.8958</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\nfacet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend() \nplt.show()\n\nfacet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend() \nplt.xlim(10,50)\n\n```\n\n\n![png](output_48_0.png)\n\n\n\n\n\n    (10, 50)\n\n\n\n\n![png](output_48_2.png)\n\n\nThose who were **20 to 30 years old** were **more dead and more survived.**\n\n\n```python\ntrain.info()\ntest.info()\n```\n\n    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 891 entries, 0 to 890\n    Data columns (total 12 columns):\n    PassengerId    891 non-null int64\n    Survived       891 non-null int64\n    Pclass         891 non-null int64\n    Sex            891 non-null int64\n    Age            891 non-null float64\n    SibSp          891 non-null int64\n    Parch          891 non-null int64\n    Ticket         891 non-null object\n    Fare           891 non-null float64\n    Cabin          204 non-null object\n    Embarked       889 non-null object\n    Title          891 non-null int64\n    dtypes: float64(2), int64(7), object(3)\n    memory usage: 83.6+ KB\n    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 418 entries, 0 to 417\n    Data columns (total 12 columns):\n    PassengerId    418 non-null int64\n    Pclass         418 non-null int64\n    Sex            418 non-null int64\n    Age            418 non-null float64\n    SibSp          418 non-null int64\n    Parch          418 non-null int64\n    Ticket         418 non-null object\n    Fare           417 non-null float64\n    Cabin          91 non-null object\n    Embarked       418 non-null object\n    Survived       418 non-null object\n    Title          418 non-null int64\n    dtypes: float64(2), int64(6), object(4)\n    memory usage: 39.3+ KB\n    \n\n**Binning**\n\nBinning/Converting Numerical Age to Categorical Variable\n\nfeature vector map:\n* child: 0\n* young: 1\n* adult: 2\n* mid-age: 3\n* senior: 4\n\n\n```python\ntrain.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n      <th>Title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\nfor dataset in train_test_data:\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0,\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 26), 'Age'] = 1,\n    dataset.loc[(dataset['Age'] > 26) & (dataset['Age'] <= 36), 'Age'] = 2,\n    dataset.loc[(dataset['Age'] > 36) & (dataset['Age'] <= 62), 'Age'] = 3,\n    dataset.loc[ dataset['Age'] > 62, 'Age'] = 4\n# for dataset in train_test_data:\n#     dataset.loc[]\n#train[train['Age'].isin([23])]\n```\n\n\n```python\ntrain.head()\nbar_chart('Age')\n```\n\n\n![png](output_54_0.png)\n\n\n\n```python\nPclass1 = train[train['Pclass'] == 1]['Embarked'].value_counts()\nPclass2 = train[train['Pclass'] == 2]['Embarked'].value_counts()\nPclass3 = train[train['Pclass'] == 3]['Embarked'].value_counts()\ndf = pd.DataFrame([Pclass1,Pclass2,Pclass3])\ndf.index = ['1st Class','2nd Class','3rd Class']\ndf.plot(kind = 'bar', stacked =  True, figsize=(10,5))\nplt.show()\nprint(\"Pclass1:\\n\",Pclass1)\nprint(\"Pclass2:\\n\",Pclass2)\nprint(\"Pclass3:\\n\",Pclass3)\n```\n\n\n![png](output_55_0.png)\n\n\n    Pclass1:\n     S    127\n    C     85\n    Q      2\n    Name: Embarked, dtype: int64\n    Pclass2:\n     S    164\n    C     17\n    Q      3\n    Name: Embarked, dtype: int64\n    Pclass3:\n     S    353\n    Q     72\n    C     66\n    Name: Embarked, dtype: int64\n    \n\nmore than 50 % of 1st class are from S embark.  \nmore than 50 % of 2st class are from S embark.   \nmore than 50 % of 3st class are from S embark.  \n\n**fill out missing embark with S embark**\n\n\n```python\nfor dataset in train_test_data:\n    dataset['Embarked'] =  dataset['Embarked'].fillna('S')\n```\n\n\n```python\ntrain.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n      <th>Title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\nembarked_mapping = {'S':0,'C':1,'Q':2}\nfor dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].map(embarked_mapping)\n```\n\n\n```python\n# train[\"Fare\"].fillna(train.groupby(\"Pclass\")[\"Fare\"])\n# train[\"Fare\"].fillna(train.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace = True)\n# test[\"Fare\"].fillna(test.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace = True)\n# train.head(50)\n\n\n# fill missing Fare with median fare for each Pclass\ntrain[\"Fare\"].fillna(train.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)\ntest[\"Fare\"].fillna(test.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)\ntrain.head(50)\n\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n      <th>Title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330877</td>\n      <td>8.4583</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>17463</td>\n      <td>51.8625</td>\n      <td>E46</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>349909</td>\n      <td>21.0750</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>347742</td>\n      <td>11.1333</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>237736</td>\n      <td>30.0708</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>PP 9549</td>\n      <td>16.7000</td>\n      <td>G6</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>113783</td>\n      <td>26.5500</td>\n      <td>C103</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>13</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>A/5. 2151</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>14</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>347082</td>\n      <td>31.2750</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>15</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>350406</td>\n      <td>7.8542</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>16</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>248706</td>\n      <td>16.0000</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>17</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>382652</td>\n      <td>29.1250</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>18</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>244373</td>\n      <td>13.0000</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>19</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>345763</td>\n      <td>18.0000</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>20</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2649</td>\n      <td>7.2250</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>21</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>239865</td>\n      <td>26.0000</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>22</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>248698</td>\n      <td>13.0000</td>\n      <td>D56</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>23</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330923</td>\n      <td>8.0292</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>24</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>113788</td>\n      <td>35.5000</td>\n      <td>A6</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>25</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>349909</td>\n      <td>21.0750</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>26</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>5</td>\n      <td>347077</td>\n      <td>31.3875</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>27</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2631</td>\n      <td>7.2250</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>28</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>3</td>\n      <td>2</td>\n      <td>19950</td>\n      <td>263.0000</td>\n      <td>C23 C25 C27</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>29</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330959</td>\n      <td>7.8792</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>30</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>349216</td>\n      <td>7.8958</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>31</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>PC 17601</td>\n      <td>27.7208</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>32</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17569</td>\n      <td>146.5208</td>\n      <td>B78</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>33</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>335677</td>\n      <td>7.7500</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>34</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>4.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>C.A. 24579</td>\n      <td>10.5000</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>35</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17604</td>\n      <td>82.1708</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>36</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113789</td>\n      <td>52.0000</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>37</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2677</td>\n      <td>7.2292</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>38</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>A./5. 2152</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>39</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>345764</td>\n      <td>18.0000</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>40</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2651</td>\n      <td>11.2417</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>41</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7546</td>\n      <td>9.4750</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>42</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>11668</td>\n      <td>21.0000</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>43</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>349253</td>\n      <td>7.8958</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>44</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>SC/Paris 2123</td>\n      <td>41.5792</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>45</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330958</td>\n      <td>7.8792</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>46</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>S.C./A.4. 23567</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>47</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>370371</td>\n      <td>15.5000</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>48</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>14311</td>\n      <td>7.7500</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>49</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2662</td>\n      <td>21.6792</td>\n      <td>NaN</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>50</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>349237</td>\n      <td>17.8000</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\nfacet = sns.FacetGrid(train, hue=\"Survived\",aspect=4 )\nfacet.map(sns.kdeplot, 'Fare', shade = True)\nfacet.set(xlim = (0, train['Fare'].max()))\nfacet.add_legend()\nplt.show()\n```\n\n\n![png](output_61_0.png)\n\n\n\n```python\nfacet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Fare',shade= True)\nfacet.set(xlim=(0, train['Fare'].max()))\nfacet.add_legend()\nplt.xlim(0, 20)\n```\n\n\n\n\n    (0, 20)\n\n\n\n\n![png](output_62_1.png)\n\n\n\n```python\nfor dataset in train_test_data:\n    dataset.loc[dataset['Fare'] <= 17, 'Fare'] = 0,\n    dataset.loc[(dataset['Fare'] > 17) & (dataset['Fare'] <= 30), 'Fare'] = 1,\n    dataset.loc[(dataset['Fare'] > 30) & (dataset['Fare'] <= 100), 'Fare'] = 2,\n    dataset.loc[dataset['Fare'] >= 100, 'Fare'] = 3\n```\n\n\n```python\ntrain.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n      <th>Title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>2.0</td>\n      <td>C85</td>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>2.0</td>\n      <td>C123</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\ntrain.Cabin.value_counts()\n```\n\n\n\n\n    B96 B98            4\n    G6                 4\n    C23 C25 C27        4\n    E101               3\n    C22 C26            3\n    D                  3\n    F2                 3\n    F33                3\n    B57 B59 B63 B66    2\n    E24                2\n    B20                2\n    B22                2\n    D17                2\n    C92                2\n    E33                2\n    E67                2\n    C52                2\n    F4                 2\n    B5                 2\n    B49                2\n    C65                2\n    D36                2\n    C93                2\n    C78                2\n    E25                2\n    B28                2\n    D33                2\n    D20                2\n    D35                2\n    B18                2\n                      ..\n    C62 C64            1\n    B102               1\n    E46                1\n    B69                1\n    E68                1\n    C50                1\n    C106               1\n    D28                1\n    E50                1\n    D46                1\n    B19                1\n    C47                1\n    A24                1\n    C70                1\n    E36                1\n    C86                1\n    A34                1\n    C111               1\n    A32                1\n    D15                1\n    B101               1\n    A6                 1\n    B41                1\n    B94                1\n    B50                1\n    E17                1\n    C104               1\n    D56                1\n    B78                1\n    C95                1\n    Name: Cabin, Length: 147, dtype: int64\n\n\n\n\n```python\nfor dataset in train_test_data:\n    dataset['Cabin'] =  dataset['Cabin'].str[:1]\n```\n\n\n```python\nPclass1 = train[train['Pclass']==1]['Cabin'].value_counts()\nPclass2 = train[train['Pclass']==2]['Cabin'].value_counts()\nPclass3 = train[train['Pclass']==3]['Cabin'].value_counts()\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class','2nd class', '3rd class']\ndf.plot(kind='bar',stacked=True, figsize=(10,5))\n```\n\n\n\n\n    <matplotlib.axes._subplots.AxesSubplot at 0x2085f6b8748>\n\n\n\n\n![png](output_67_1.png)\n\n\n\n```python\ncabin_mapping = {\"A\": 0, \"B\": 0.4, \"C\": 0.8, \"D\": 1.2, \"E\": 1.6, \"F\": 2, \"G\": 2.4, \"T\": 2.8}\nfor dataset in train_test_data:\n    dataset['Cabin'] = dataset['Cabin'].map(cabin_mapping)\n```\n\n\n```python\n# fill missing Fare with median fare for each Pclass\ntrain[\"Cabin\"].fillna(train.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)\ntest[\"Cabin\"].fillna(test.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)\n```\n\n**family Size**\n\n\n```python\ntrain[\"FamilySize\"] = train[\"SibSp\"] + train[\"Parch\"] + 1\ntest[\"FamilySize\"] = test[\"SibSp\"] + test[\"Parch\"] + 1\n```\n\n\n```python\nfacet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'FamilySize',shade= True)\nfacet.set(xlim=(0, train['FamilySize'].max()))\nfacet.add_legend()\nplt.xlim(0)\n```\n\n\n\n\n    (0, 11.0)\n\n\n\n\n![png](output_72_1.png)\n\n\n\n```python\nfamily_mapping = {1: 0, 2: 0.4, 3: 0.8, 4: 1.2, 5: 1.6, 6: 2, 7: 2.4, 8: 2.8, 9: 3.2, 10: 3.6, 11: 4}\nfor dataset in train_test_data:\n    dataset['FamilySize'] = dataset['FamilySize'].map(family_mapping)\n```\n\n\n```python\ntrain.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n      <th>Title</th>\n      <th>FamilySize</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>2.0</td>\n      <td>0.8</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>2.0</td>\n      <td>0.8</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0.4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\nfeatures_drop = ['Ticket','SibSp','Parch']\ntrain = train.drop(features_drop, axis = 1)\ntest = test.drop(features_drop,axis=1)\ntrain = train.drop(['PassengerId'], axis=1)\n```\n\n\n```python\ntrain_data = train.drop('Survived', axis = 1)\ntarget = train['Survived']\ntrain_data.shape, target.shape\n```\n\n\n\n\n    ((891, 8), (891,))\n\n\n\n\n```python\ntrain_data.head(10)\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n      <th>Title</th>\n      <th>FamilySize</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>0.8</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.8</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0.4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>1.6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>3</td>\n      <td>1.6</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>3</td>\n      <td>1</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0.8</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>1.8</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0.4</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n# 5. Modelling\n\n\n```python\n# Importing Classifier Modules\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier,ExtraTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\nimport numpy as np\n```\n\n\n```python\ntrain.info()\n```\n\n    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 891 entries, 0 to 890\n    Data columns (total 9 columns):\n    Survived      891 non-null int64\n    Pclass        891 non-null int64\n    Sex           891 non-null int64\n    Age           891 non-null float64\n    Fare          891 non-null float64\n    Cabin         891 non-null float64\n    Embarked      891 non-null int64\n    Title         891 non-null int64\n    FamilySize    891 non-null float64\n    dtypes: float64(4), int64(5)\n    memory usage: 62.7 KB\n    \n\n# 6.Cross Validation(k-fold)\n\n\n```python\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n```\n\n\n```python\nclf = KNeighborsClassifier(n_neighbors = 13)\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\n```\n\n    [0.82222222 0.76404494 0.80898876 0.83146067 0.87640449 0.82022472\n     0.85393258 0.79775281 0.84269663 0.84269663]\n    \n\n\n```python\n#learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\nclf = [KNeighborsClassifier(n_neighbors = 13),DecisionTreeClassifier(),\n       RandomForestClassifier(n_estimators=13),GaussianNB(),SVC(),ExtraTreeClassifier(),\n      GradientBoostingClassifier(n_estimators=10, learning_rate=1,max_features=3, max_depth =3, random_state = 10),AdaBoostClassifier(),ExtraTreesClassifier()]\ndef model_fit():\n    scoring = 'accuracy'\n    for i in range(len(clf)):\n        score = cross_val_score(clf[i], train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\n        print(\"Score of Model\",i,\":\",round(np.mean(score)*100,2))\n#     round(np.mean(score)*100,2)\n#     print(\"Score of :\\n\",score)\nmodel_fit()\n```\n\n    Score of Model 0 : 82.6\n    Score of Model 1 : 79.8\n    Score of Model 2 : 80.92\n    Score of Model 3 : 78.78\n    Score of Model 4 : 83.5\n    Score of Model 5 : 80.02\n    Score of Model 6 : 81.25\n    Score of Model 7 : 81.03\n    Score of Model 8 : 80.7\n    \n\n\n```python\nclf1 = SVC()\nclf1.fit(train_data, target)\ntest\ntest_data = test.drop(['Survived','PassengerId'], axis=1)\nprediction = clf1.predict(test_data)\n# test_data\n\n```\n\n\n```python\ntest_data['Survived'] = prediction\nsubmission = pd.DataFrame(test['PassengerId'],test_data['Survived'])\nsubmission.to_csv(\"Submission.csv\")\n```\n"
 },
 {
  "repo": "NotAyushXD/Titanic-dataset",
  "language": "Jupyter Notebook",
  "readme_contents": "# Titanic-dataset\nUsing the titanic data to predict the survival of the passengers.\n\nTo get a better understanding of the workflow of a Machine Learning project, have a read:\nhttps://medium.com/@NotAyushXD/workflow-of-a-machine-learning-project-ec1dba419b94\n"
 },
 {
  "repo": "AaronJny/simple_titanic",
  "language": "Python",
  "readme_contents": "# scikit-learn\u5728Kaggle Titanic\u6570\u636e\u96c6\u4e0a\u7684\u7b80\u5355\u5b9e\u8df5\uff08\u65b0\u624b\u5411\uff09\n\n\u535a\u5ba2\u94fe\u63a5\uff1a[scikit-learn\u5728Kaggle Titanic\u6570\u636e\u96c6\u4e0a\u7684\u7b80\u5355\u5b9e\u8df5\uff08\u65b0\u624b\u5411\uff09](https://blog.csdn.net/aaronjny/article/details/79735998)\n\nTitanic\u4e58\u5ba2\u751f\u5b58\u9884\u6d4b\u662fKaggle\u4e0a\u7684\u4e00\u9879\u5165\u95e8\u7ade\u8d5b\uff0c\u5373\u7ed9\u5b9a\u4e00\u4e9b\u4e58\u5ba2\u7684\u4fe1\u606f\uff0c\u9884\u6d4b\u8be5\u4e58\u5ba2\u662f\u5426\u5728Tatanic\u707e\u96be\u4e2d\u5e78\u5b58\u4e0b\u6765\u3002\n\n> \u4ec0\u4e48\u662fKaggle\uff1f\n\n> \u7ed9\u51fa\u767e\u5ea6\u767e\u79d1\u7684\u5b9a\u4e49\u4f5c\u4e3a\u53c2\u8003\uff1aKaggle\u662f\u7531\u8054\u5408\u521b\u59cb\u4eba\u3001\u9996\u5e2d\u6267\u884c\u5b98\u5b89\u4e1c\u5c3c\u00b7\u9ad8\u5fb7\u5e03\u5362\u59c6\uff08Anthony Goldbloom\uff092010\u5e74\u5728\u58a8\u5c14\u672c\u521b\u7acb\u7684\uff0c\u4e3b\u8981\u4e3a\u5f00\u53d1\u5546\u548c\u6570\u636e\u79d1\u5b66\u5bb6\u63d0\u4f9b\u4e3e\u529e\u673a\u5668\u5b66\u4e60\u7ade\u8d5b\u3001\u6258\u7ba1\u6570\u636e\u5e93\u3001\u7f16\u5199\u548c\u5206\u4eab\u4ee3\u7801\u7684\u5e73\u53f0\u3002\n\n\u4eca\u5929\uff0c\u6211\u4eec\u4f7f\u7528scikit-learn\u6846\u67b6\u5728Titanic\u6570\u636e\u96c6\u4e0a\u505a\u4e00\u4e9b\u57fa\u7840\u5b9e\u8df5\u3002\n\n-------------------------\n\n## \u5206\u6790\u6570\u636e\u96c6\n\n**1.\u83b7\u53d6\u6570\u636e\u96c6**\n\nTitanic\u6570\u636e\u96c6\u5206\u4e3a\u4e24\u90e8\u5206\uff1a\n\n- \u8bad\u7ec3\u6570\u636e\u96c6-\u5305\u542b\u7279\u5f81\u4fe1\u606f\u548c\u5b58\u6d3b\u4e0e\u5426\u7684\u6807\u7b7e\n\n- \u6d4b\u8bd5\u6570\u636e\u96c6-\u53ea\u5305\u542b\u7279\u5f81\u4fe1\u606f\n\n\u6570\u636e\u96c6\u53ef\u4ee5\u4ecekaggle\u4e0a\u4e0b\u8f7d\uff08\u70b9\u51fb\u4e0b\u9762\u7684\u94fe\u63a5\u8fdb\u884c\u4e0b\u8f7d\uff09\uff0c\u683c\u5f0f\u4e3acsv\uff1a\n\n- [train.csv](https://www.kaggle.com/c/3136/download/train.csv)\n\n- [test.csv](https://www.kaggle.com/c/3136/download/test.csv)\n\n\n**2.\u5206\u6790\u6570\u636e\u96c6**\n\n\u6570\u636e\u96c6\u4e0b\u8f7d\u5230\u672c\u5730\u540e\uff0c\u6211\u4eec\u4f7f\u7528pandas\u8bfb\u53d6\uff0c\u67e5\u770b\u4e00\u4e0b\u6570\u636e\u96c6\u4fe1\u606f\uff1a\n\n```python\nimport pandas as pd\n\n# \u8bfb\u53d6\u6570\u636e\u96c6\ntrain_data = pd.read_csv('dataset/train.csv')\ntest_data = pd.read_csv('dataset/test.csv')\n# \u6253\u5370\u4fe1\u606f\ntrain_data.info()\n```\n\n\u8f93\u51fa\u4fe1\u606f\u5982\u4e0b\uff1a\n\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\nPassengerId    891 non-null int64\nSurvived       891 non-null int64\nPclass         891 non-null int64\nName           891 non-null object\nSex            891 non-null object\nAge            714 non-null float64\nSibSp          891 non-null int64\nParch          891 non-null int64\nTicket         891 non-null object\nFare           891 non-null float64\nCabin          204 non-null object\nEmbarked       889 non-null object\ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.6+ KB\n```\n\n\n\u5bf9\u4e0a\u9762\u7684\u7279\u5f81\u8fdb\u884c\u4e00\u4e0b\u89e3\u91ca\uff1a\n\n- PassengerId \u4e58\u5ba2\u7f16\u53f7\n\n- Survived \u662f\u5426\u5e78\u5b58\n\n- Pclass \u8239\u7968\u7b49\u7ea7\n\n- Name \u4e58\u5ba2\u59d3\u540d\n\n- Sex \u4e58\u5ba2\u6027\u522b\n\n- SibSp\u3001Parch \u4eb2\u621a\u6570\u91cf\n\n- Ticket \u8239\u7968\u53f7\u7801\n\n- Fare \u8239\u7968\u4ef7\u683c\n\n- Cabin \u8239\u8231\n\n- Embarked \u767b\u5f55\u6e2f\u53e3\n\n\n\u8fd9\u91cc\uff0c\u6839\u636e\u5e38\u8bc6\u548c\u7ecf\u9a8c\uff0c\u7b80\u5355\u9009\u62e9\u4e00\u4e0b\u7528\u4e8e\u8bad\u7ec3\u7684\u7279\u5f81\uff1a`['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']`\u3002\u5f53\u7136\u4e86\uff0c\u5982\u679c\u4f7f\u7528\u7279\u5f81\u5de5\u7a0b\u6765\u7b5b\u9009\u548c\u521b\u9020\u8bad\u7ec3\u7279\u5f81\u7684\u8bdd\uff0c\u6548\u679c\u8981\u6bd4\u8fd9\u4e2a\u597d\u5f97\u591a\u3002\n\n```python\n# \u9009\u62e9\u7528\u4e8e\u8bad\u7ec3\u7684\u7279\u5f81\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\nx_train = train_data[features]\nx_test = test_data[features]\n\ny_train=train_data['Survived']\n```\n\n\u73b0\u5728\uff0c\u6211\u4eec\u67e5\u770b\u4e00\u4e0b\u7b5b\u9009\u540e\u7684\u6570\u636e\u96c6\uff1a\n\n```python\n# \u68c0\u67e5\u7f3a\u5931\u503c\nx_train.info()\nprint '-'*100\nx_test.info()\n```\n\n\u8f93\u51fa\u7684\u7ed3\u679c\u5982\u4e0b:\n\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 7 columns):\nPclass      891 non-null int64\nSex         891 non-null object\nAge         714 non-null float64\nSibSp       891 non-null int64\nParch       891 non-null int64\nFare        891 non-null float64\nEmbarked    889 non-null object\ndtypes: float64(2), int64(3), object(2)\nmemory usage: 48.8+ KB\n------------------------------\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 418 entries, 0 to 417\nData columns (total 7 columns):\nPclass      418 non-null int64\nSex         418 non-null object\nAge         332 non-null float64\nSibSp       418 non-null int64\nParch       418 non-null int64\nFare        417 non-null float64\nEmbarked    418 non-null object\ndtypes: float64(2), int64(3), object(2)\nmemory usage: 22.9+ KB\n```\n\n\u53ef\u4ee5\u53d1\u73b0\uff0c\u5728\u8fd9\u4e9b\u6570\u636e\u4e2d\uff0c\u662f\u5b58\u5728\u7f3a\u5931\u503c\u7684\uff0c\u6bd4\u5982\u8bf4\u8bad\u7ec3\u6570\u636e\u4e2d\u7684`Age`,`Embarked`\uff0c\u6d4b\u8bd5\u6570\u636e\u4e2d\u7684`Age`,`Fare`,`Embarked`\u3002\u6211\u4eec\u8981\u5c06\u5176\u586b\u5145\u5b8c\u6574\u3002\n\n**3.\u8865\u5168\u6570\u636e\u96c6**\n\n`Age`\u548c`Fare`\u662f\u6570\u503c\u578b\u6570\u636e\uff0c\u53ef\u4ee5\u4f7f\u7528\u5176\u5e73\u5747\u503c\u6765\u8865\u5168\u7a7a\u503c\uff0c\u5c3d\u91cf\u51cf\u5c0f\u8865\u5168\u503c\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002\n\n```python\n# \u4f7f\u7528\u5e73\u5747\u5e74\u9f84\u6765\u586b\u5145\u5e74\u9f84\u4e2d\u7684nan\u503c\nx_train['Age'].fillna(x_train['Age'].mean(), inplace=True)\nx_test['Age'].fillna(x_test['Age'].mean(),inplace=True)\n\n# \u4f7f\u7528\u7968\u4ef7\u7684\u5747\u503c\u586b\u5145\u7968\u4ef7\u4e2d\u7684nan\u503c\nx_test['Fare'].fillna(x_test['Fare'].mean(),inplace=True)\n```\n\n`Embarked`\u662f\u7c7b\u522b\u6570\u636e\uff0c\u53d6\u51fa\u73b0\u6b21\u6570\u6700\u591a\u7684\u7c7b\u522b\u6765\u8865\u5168\u7a7a\u503c\u3002\n\n```python\n# \u4f7f\u7528\u767b\u5f55\u6700\u591a\u7684\u6e2f\u53e3\u6765\u586b\u5145\u767b\u5f55\u6e2f\u53e3\u7684nan\u503c\nprint x_train['Embarked'].value_counts()\nx_train['Embarked'].fillna('S', inplace=True)\nx_test['Embarked'].fillna('S',inplace=True)\n```\n\n\u80fd\u591f\u770b\u5230\uff0c\u51fa\u73b0\u6b21\u6570\u6700\u591a\u7684\u7c7b\u522b\u662f\u2018S\u2019:\n\n```\nS    644\nC    168\nQ     77\n```\n\n**4.\u5c06\u7279\u5f81\u503c\u8f6c\u5316\u4e3a\u7279\u5f81\u5411\u91cf**\n\n\u60f3\u8981\u8fdb\u884c\u8bad\u7ec3\uff0c\u8fd8\u9700\u8981\u5c06\u8fd9\u4e9b\u7279\u5f81\u503c\u8f6c\u5316\u6210\u7279\u5f81\u5411\u91cf\u624d\u884c\u3002\u7c7b\u522b\u7c7b\u578b\u7684\u7279\u5f81\uff0c\u4e5f\u9700\u8981\u8f6c\u5316\u6210\u7c7b\u4f3c\u4e8e`one-hot`\u7684\u683c\u5f0f\u3002\n\n```python\n# \u5c06\u7279\u5f81\u503c\u8f6c\u6362\u6210\u7279\u5f81\u5411\u91cf\ndvec=DictVectorizer(sparse=False)\n\nx_train=dvec.fit_transform(x_train.to_dict(orient='record'))\nx_test=dvec.transform(x_test.to_dict(orient='record'))\n\n# \u6253\u5370\u7279\u5f81\u5411\u91cf\u683c\u5f0f\nprint dvec.feature_names_\n```\n\n\u80fd\u591f\u770b\u5230\uff0c\u8f6c\u6362\u4e4b\u540e\u7684\u7279\u5f81\u5411\u91cf\u7684\u683c\u5f0f\u5927\u81f4\u4e0a\u662f\u8fd9\u6837\u7684\uff1a\n\n```\n['Age', 'Embarked=C', 'Embarked=Q', 'Embarked=S', 'Fare', 'Parch', 'Pclass', 'Sex=female', 'Sex=male', 'SibSp']\n```\n\n\u6bd4\u5982\uff0c\u6211\u4eec\u6253\u5370\u8bad\u7ec3\u6570\u636e\u7684\u7b2c\u4e00\u6761\uff1a\n\n```\nprint x_train[0]\n```\n\n\u5176\u8f93\u51fa\u4e3a\uff1a\n\n```\n[22.    0.    0.    1.    7.25  0.    3.    0.    1.    1.  ]\n```\n\n\u8be5\u7279\u5f81\u5411\u91cf\u7684\u503c\u4e0e\u4e0a\u9762\u7684\u540d\u79f0\u662f\u4e00\u4e00\u5bf9\u5e94\u7684\u3002\n\n\n**5.\u9009\u62e9\u6a21\u578b**\n\nsklearn\u91cc\u9762\u96c6\u6210\u4e86\u5f88\u591a\u7ecf\u5178\u6a21\u578b\uff0c\u672c\u6587\u9009\u4e86\u51e0\u4e2a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002\u8fd9\u4e9b\u6a21\u578b\u5f88\u591a\u90fd\u5177\u5907\u53ef\u8c03\u8282\u7684\u8d85\u53c2\u6570\uff0c\u672c\u6587\u5747\u4f7f\u7528\u9ed8\u8ba4\u914d\u7f6e\u3002\n\n\u5bf9\u6a21\u578b\u7684\u9a8c\u8bc1\u4f7f\u7528\u5341\u500d\u4ea4\u53c9\u9a8c\u8bc1\u3002\n\n```python\n# \u652f\u6301\u5411\u91cf\u673a\nsvc = SVC()\n# \u51b3\u7b56\u6811\ndtc = DecisionTreeClassifier()\n# \u968f\u673a\u68ee\u6797\nrfc = RandomForestClassifier()\n# \u903b\u8f91\u56de\u5f52\nlr = LogisticRegression()\n# \u8d1d\u53f6\u65af\nnb = MultinomialNB()\n# K\u90bb\u8fd1\nknn = KNeighborsClassifier()\n# AdaBoost\nboost = AdaBoostClassifier()\n\nprint 'SVM acc is', np.mean(cross_val_score(svc, x_train, y_train, cv=10))\nprint 'DecisionTree acc is', np.mean(cross_val_score(dtc, x_train, y_train, cv=10))\nprint 'RandomForest acc is', np.mean(cross_val_score(rfc, x_train, y_train, cv=10))\nprint 'LogisticRegression acc is', np.mean(cross_val_score(lr, x_train, y_train, cv=10))\nprint 'NaiveBayes acc is', np.mean(cross_val_score(nb, x_train, y_train, cv=10))\nprint 'KNN acc is', np.mean(cross_val_score(knn, x_train, y_train, cv=10))\nprint 'AdaBoost acc is', np.mean(cross_val_score(boost, x_train, y_train, cv=10))\n```\n\n\u5341\u500d\u4ea4\u53c9\u9a8c\u8bc1\u7684\u5e73\u5747\u7ed3\u679c\u5982\u4e0b\uff1a\n\n```\nSVM acc is 0.726437407786\nDecisionTree acc is 0.777898081943\nRandomForest acc is 0.81717483827\nLogisticRegression acc is 0.795800987402\nNaiveBayes acc is 0.692726705255\nKNN acc is 0.708359153331\nAdaBoost acc is 0.810419929633\n```\n\n\u53ef\u4ee5\u770b\u51fa\uff0c\u5728\u5f53\u524d\u7684\u7279\u5f81\u9009\u62e9\u548c\u6a21\u578b\u914d\u7f6e\u4e0b\uff0c\u968f\u673a\u68ee\u6797\u3001\u903b\u8f91\u56de\u5f52\u548cAdaBoost\u8868\u73b0\u8f83\u597d\uff0cKNN\u548c\u6734\u7d20\u8d1d\u53f6\u65af\u8868\u73b0\u8f83\u5dee\u3002\u8fd9\u6837\u7684\u51c6\u786e\u7387\u548c\u7cbe\u5fc3\u7684\u7279\u5f81\u5de5\u7a0b\u548c\u6a21\u578b\u8c03\u4f18\u76f8\u6bd4\uff0c\u80af\u5b9a\u662f\u5dee\u5f97\u8fdc\u7684\uff0c\u6bd5\u7adf\u5927\u4f6c\u4eec\u90fd\u5df2\u7ecf\u67091.0\u51c6\u786e\u7387\u7684\u4e86\uff0c\u53ef\u6015\u2026\u2026\u4e0d\u8fc7\u672c\u6587\u53ea\u662f\u505a\u4e2a\u5b9e\u8df5\uff0c\u8868\u73b0\u5dee\u70b9\u5012\u662f\u65e0\u6240\u8c13\uff0c\u8fd9\u91cc\u9009\u62e9\u4f7f\u7528AdaBoost\u5b8c\u6210\u540e\u7eed\u5185\u5bb9\u3002\n\n**6.\u8fdb\u884c\u9884\u6d4b**\n\n\u4f7f\u7528AdaBoost\u5206\u7c7b\u5668\u6765\u8fdb\u884c\u751f\u5b58\u9884\u6d4b\uff0c\u5e76\u4fdd\u5b58\u9884\u6d4b\u7ed3\u679c\u3002\n\n```\n# \u8bad\u7ec3\nboost.fit(x_train, y_train)\n# \u9884\u6d4b\ny_predict = boost.predict(x_test)\n# \u4fdd\u5b58\u7ed3\u679c\nresult = {'PassengerId': test_data['PassengerId'],\n          'Survived': y_predict}\nresult = pd.DataFrame(result)\nresult.to_csv('submission.csv',index=False)\n```\n\n\u5728kaggle\u4e0a\u63d0\u4ea4\u4e86\u4e00\u4e0b\uff0c\u51c6\u786e\u7387\u53ea\u67090.75119\u3002\u679c\u7136\uff0c\u65e0\u8111\u83bd\u662f\u4e0d\u884c\u7684=\u3002=\u60f3\u8981\u6a21\u578b\u7684\u6548\u679c\u597d\uff0c\u8ba4\u771f\u7684\u7279\u5f81\u5de5\u7a0b\u548c\u6a21\u578b\u8c03\u4f18\uff0c\u8fd8\u662f\u5c11\u4e0d\u4e86\u7684\u3002\u3002\u3002\n\nemmm,\u4eca\u5929\u7684\u5b9e\u8df5\u5c31\u5230\u8fd9\u91cc\uff0c\u5b8c\u6574\u7684\u4ee3\u7801\u8bf7\u70b9\u51fb[\u8fd9\u91cc(GitHub)](https://github.com/AaronJny/simple_titanic)"
 },
 {
  "repo": "amdp-chauhan/titanic-survival-complete-ml-solution",
  "language": "Jupyter Notebook",
  "readme_contents": "## API-First approach to make Machine Learning solution usable\n\n### Introduction\nIn this application I have solved 'Titanic Survival Prediction problem' and using simple **VotingClassifier** to make predictions. I have trained many other models as well and if you want to then you can view their performance and can choose any of the desired model (by making some changes in `Src/utils/ClassificationModelBuilder.py` file) to make predictions. \n\n### Application Setup\nI have used Python's  `venv` module for creating/managing virtual environment and `flask` framework for API creation. \n\nIf you're not much aware of `venv` environment setup, than you can go through [this](https://docs.python.org/3/tutorial/venv.html) documentation. I learnt from same.\n\nOnce you have `venv` installed and got basic understanding, follow below steps to run this application:\n1. `git clone https://github.com/amdp-chauhan/titanic-survival-complete-ml-solution.git` && `cd titanic-survival-complete-ml-solution`\n2. `python -m venv ./` - It will create a virtual environment in application directory.\n3. `Scripts\\activate.bat` - It will run this virtual environment.\n4. `pip install -r packages.txt` - it will install all required dependencies.\n5. `python application.py` - it will run the application.\n6. `deactivate` - If you want to exit from virtual environment.\n\nUpper commands will work fine in Windows 10, for Linux you can find alternatives in venv documentation.\n\n> Note that in `application.py` file, second import statement is commented out, it is because if it is enabled then it starts retraining classifier models, which is not required if you already have created a final model and data-set is same. Final models exists in `Src/ml-model/voting_classifier_v1.pk` we use same model to make predictions for requested JSON record.\n\n### Making Predictions \nFor predictions I have created an POST API:\n ```\nhttp://{domain}/titanic-survival-classification-model/predict\n ```\n\nIt accepts list of JSON of test records and in return will give you a predicted Survival values in 0/1.\n\nFor example, for below input parameters: \n\n```\n[{\n    \"PassengerId\": 892,\n    \"Pclass\": 3,\n    \"Name\": \"Kelly, Mr. James\",\n    \"Sex\": \"male\",\n    \"Age\": 34.5,\n    \"SibSp\": 0,\n    \"Parch\": 0,\n    \"Ticket\": 330911,\n    \"Fare\": 7.8292,\n    \"Cabin\": \"\",\n    \"Embarked\": \"Q\"\n  },{\n    \"PassengerId\": 893,\n    \"Pclass\": 3,\n    \"Name\": \"Wilkes, Mrs. James (Ellen Needs)\",\n    \"Sex\": \"female\",\n    \"Age\": 47,\n    \"SibSp\": 1,\n    \"Parch\": 0,\n    \"Ticket\": 363272,\n    \"Fare\": 7,\n    \"Cabin\": \"\",\n    \"Embarked\": \"S\"\n  },{\n    \"PassengerId\": 894,\n    \"Pclass\": 2,\n    \"Name\": \"Myles, Mr. Thomas Francis\",\n    \"Sex\": \"male\",\n    \"Age\": 62,\n    \"SibSp\": 0,\n    \"Parch\": 0,\n    \"Ticket\": 240276,\n    \"Fare\": 9.6875,\n    \"Cabin\": \"\",\n    \"Embarked\": \"Q\"\n  },{\n    \"PassengerId\": 895,\n    \"Pclass\": 3,\n    \"Name\": \"Wirz, Mr. Albert\",\n    \"Sex\": \"male\",\n    \"Age\": 27,\n    \"SibSp\": 0,\n    \"Parch\": 0,\n    \"Ticket\": 315154,\n    \"Fare\": 8.6625,\n    \"Cabin\": \"\",\n    \"Embarked\": \"S\"\n  },{\n    \"PassengerId\": 896,\n    \"Pclass\": 3,\n    \"Name\": \"Hirvonen, Mrs. Alexander (Helga E Lindqvist)\",\n    \"Sex\": \"female\",\n    \"Age\": 22,\n    \"SibSp\": 1,\n    \"Parch\": 1,\n    \"Ticket\": 3101298,\n    \"Fare\": 12.2875,\n    \"Cabin\": \"\",\n    \"Embarked\": \"S\"\n  },{\n    \"PassengerId\": 897,\n    \"Pclass\": 3,\n    \"Name\": \"Svensson, Mr. Johan Cervin\",\n    \"Sex\": \"male\",\n    \"Age\": 14,\n    \"SibSp\": 0,\n    \"Parch\": 0,\n    \"Ticket\": 7538,\n    \"Fare\": 9.225,\n    \"Cabin\": \"\",\n    \"Embarked\": \"S\"\n  },{\n    \"PassengerId\": 898,\n    \"Pclass\": 3,\n    \"Name\": \"Connolly, Miss. Kate\",\n    \"Sex\": \"female\",\n    \"Age\": 30,\n    \"SibSp\": 0,\n    \"Parch\": 0,\n    \"Ticket\": 330972,\n    \"Fare\": 7.6292,\n    \"Cabin\": \"\",\n    \"Embarked\": \"Q\"\n  },{\n    \"PassengerId\": 899,\n    \"Pclass\": 2,\n    \"Name\": \"Caldwell, Mr. Albert Francis\",\n    \"Sex\": \"male\",\n    \"Age\": 26,\n    \"SibSp\": 1,\n    \"Parch\": 1,\n    \"Ticket\": 248738,\n    \"Fare\": 29,\n    \"Cabin\": \"\",\n    \"Embarked\": \"S\"\n  },{\n    \"PassengerId\": 900,\n    \"Pclass\": 3,\n    \"Name\": \"Abrahim, Mrs. Joseph (Sophie Halaut Easu)\",\n    \"Sex\": \"female\",\n    \"Age\": 18,\n    \"SibSp\": 0,\n    \"Parch\": 0,\n    \"Ticket\": 2657,\n    \"Fare\": 7.2292,\n    \"Cabin\": \"\",\n    \"Embarked\": \"C\"\n  },{\n    \"PassengerId\": 901,\n    \"Pclass\": 3,\n    \"Name\": \"Davies, Mr. John Samuel\",\n    \"Sex\": \"male\",\n    \"Age\": 21,\n    \"SibSp\": 2,\n    \"Parch\": 0,\n    \"Ticket\": \"A/4 48871\",\n    \"Fare\": 24.15,\n    \"Cabin\": \"\",\n    \"Embarked\": \"S\"\n  },{\n    \"PassengerId\": 902,\n    \"Pclass\": 3,\n    \"Name\": \"Ilieff, Mr. Ylio\",\n    \"Sex\": \"male\",\n    \"Age\": \"\",\n    \"SibSp\": 0,\n    \"Parch\": 0,\n    \"Ticket\": 349220,\n    \"Fare\": 7.8958,\n    \"Cabin\": \"\",\n    \"Embarked\": \"S\"\n  },{\n    \"PassengerId\": 903,\n    \"Pclass\": 1,\n    \"Name\": \"Jones, Mr. Charles Cresson\",\n    \"Sex\": \"male\",\n    \"Age\": 46,\n    \"SibSp\": 0,\n    \"Parch\": 0,\n    \"Ticket\": 694,\n    \"Fare\": 26,\n    \"Cabin\": \"\",\n    \"Embarked\": \"S\"\n}]\n```\nWe will get below output:\n```\n{\n\t\"predictions\": \"[{\\\"PassengerId\\\":892,\\\"Survived\\\":1},{\\\"PassengerId\\\":893,\\\"Survived\\\":1},{\\\"PassengerId\\\":894,\\\"Survived\\\":0},{\\\"PassengerId\\\":895,\\\"Survived\\\":1},{\\\"PassengerId\\\":896,\\\"Survived\\\":1},{\\\"PassengerId\\\":897,\\\"Survived\\\":1},{\\\"PassengerId\\\":898,\\\"Survived\\\":0},{\\\"PassengerId\\\":899,\\\"Survived\\\":0},{\\\"PassengerId\\\":900,\\\"Survived\\\":1},{\\\"PassengerId\\\":901,\\\"Survived\\\":0},{\\\"PassengerId\\\":902,\\\"Survived\\\":1},{\\\"PassengerId\\\":903,\\\"Survived\\\":0}]\"\n}\n```"
 },
 {
  "repo": "techytushar/would-you-have-survived",
  "language": "HTML",
  "readme_contents": "# would-you-have-survived\n\nPredict whether you would have survived if you were on the Titanic Ship, in 1912\n\n\nRead the [CONTRIBUTING.md](https://github.com/techytushar/would-you-have-survived/blob/master/CONTRIBUTING.md), if you want to contribute.\n"
 },
 {
  "repo": "weizhuang1113/Titanic_Survival_Prediction",
  "language": "Jupyter Notebook",
  "readme_contents": "# Kaggle\u6cf0\u5766\u5c3c\u514b\u53f7\u4e58\u5ba2\u751f\u8fd8\u9884\u6d4b\u5206\u6790\n\n- \u76ee\u7684\uff1a\u6cf0\u5766\u5c3c\u514b\u53f7\u4e58\u5ba2\u751f\u8fd8\u9884\u6d4b\u5206\u6790\n\n- [\u6bd4\u8d5b\u5730\u5740](https://www.kaggle.com/c/titanic)\n\n- \u6700\u7ec8\u5f97\u52060.80861\n\n  \n\n"
 },
 {
  "repo": "moksha16/K-fold-Titanic-Dataset",
  "language": null,
  "readme_contents": "# K-fold---Titanic-Dataset"
 },
 {
  "repo": "raqueeb/mlbook-titanic",
  "language": null,
  "readme_contents": "---\ndescription: (\u0986\u09b0 \u098f\u09ac\u0982 \u09aa\u09be\u0987\u09a5\u09a8 \u09aa\u09cd\u09b0\u09cb\u0997\u09cd\u09b0\u09be\u09ae\u09bf\u0982 \u098f\u09a8\u09ad\u09be\u09df\u09b0\u09a8\u09ae\u09c7\u09a8\u09cd\u099f)\n---\n\n# \u09b9\u09be\u09a4\u09c7\u0995\u09b2\u09ae\u09c7 \u09ae\u09c7\u09b6\u09bf\u09a8 \u09b2\u09be\u09b0\u09cd\u09a8\u09bf\u0982\n\n## _\u09ac\u09bf\u09b6\u09cd\u09ac\u09b8\u09c7\u09b0\u09be \u09aa\u09cd\u09b0\u099c\u09c7\u0995\u09cd\u099f '\u099f\u09be\u0987\u099f\u09be\u09a8\u09bf\u0995 '_\n\n> A breakthrough in machine learning would be worth ten Microsofts.\n>\n> \u2014 Bill Gates\n\n{% hint style=\"info\" %}\n_\u0996\u09b8\u09dc\u09be \u09ad\u09be\u09b0\u09cd\u09b8\u09a8: \u09e7.\u09e6\u09e6 \u09b0\u09bf\u09ad\u09bf\u09b6\u09a8 \u09e9\u0964 \u09b8\u09ae\u09df\u09b8\u09c0\u09ae\u09be: \u09ab\u09c7\u09ac\u09cd\u09b0\u09c2\u09df\u09be\u09b0\u09bf -\u09a8\u09ad\u09c7\u09ae\u09cd\u09ac\u09b0 \u09e8\u09e6\u09e7\u09ec_\n\n_\u0996\u09b8\u09dc\u09be \u09ad\u09be\u09b0\u09cd\u09b8\u09a8: \u09e8.\u09e6\u09e6 \u09b0\u09bf\u09ad\u09bf\u09b6\u09a8 \u09ed\u0964 \u09b8\u09ae\u09df\u09b8\u09c0\u09ae\u09be: \u099c\u09c1\u09a8 - \u0985\u0995\u09cd\u099f\u09cb\u09ac\u09b0  \u09e8\u09e6\u09e7\u09ed_\n\n_\u0996\u09b8\u09dc\u09be \u09ad\u09be\u09b0\u09cd\u09b8\u09a8: \u09e9.\u09e6\u09e6 \u09b0\u09bf\u09ad\u09bf\u09b6\u09a8 \u09e7\u09e8\u0964 \u09b8\u09ae\u09df\u09b8\u09c0\u09ae\u09be: \u09a8\u09ad\u09c7\u09ae\u09cd\u09ac\u09b0 - \u099c\u09be\u09a8\u09c1\u09df\u09be\u09b0\u09bf \u09e8\u09e6\u09e7\u09ee_\n{% endhint %}\n\n![&#x9B2;&#x9BE;&#x9B2; &#x9AC;&#x987;&#x99F;&#x9BF;&#x9B0; &#x987;&#x9A8;&#x9CD;&#x99F;&#x9BE;&#x9B0;&#x9A8;&#x9C7;&#x99F; &#x9AD;&#x9BE;&#x9B0;&#x9CD;&#x9B8;&#x9A8; ](.gitbook/assets/123.jpg)\n\n{% hint style=\"info\" %}\n\u09a6\u09cd\u09ac\u09bf\u09a4\u09c0\u09df \u09b8\u0982\u09b8\u09cd\u0995\u09b0\u09a3 \u09aa\u09cd\u09b0\u09bf\u09a8\u09cd\u099f \u098f\u09a1\u09bf\u09b6\u09a8 \u09aa\u09be\u09ac\u09c7\u09a8 [\u098f\u0996\u09be\u09a8\u09c7](https://rokomari.com/book/174186): [https://rokomari.com/book/174186](https://rokomari.com/book/174186)/\n\n\u09aa\u09c1\u09b0\u09be\u09a8\u09cb- [\u09aa\u09cd\u09b0\u09a5\u09ae \u09b8\u0982\u09b8\u09cd\u0995\u09b0\u09a3](https://www.rokomari.com/book/160337/) [https://www.rokomari.com/book/160337/](https://www.rokomari.com/book/160337/)\n{% endhint %}\n\n> \u0997\u09bf\u099f\u09b9\u09be\u09ac \u09b8\u09cd\u0995\u09cd\u09b0\u09bf\u09aa\u09cd\u099f \u09b8\u09ac \u09a0\u09bf\u0995\u09ae\u09a4\u09cb \u099a\u09b2\u099b\u09c7\n\n\\(\u0986\u09b8\u09b2 \"\u099f\u09c7\u09ac\u09bf\u09b2 \u0985\u09ab \u0995\u09a8\u099f\u09c7\u09a8\u09cd\u099f\" \u09ac\u09be\u0981 \u09aa\u09be\u09b6\u09c7\u09b0 \u09aa\u09cd\u09af\u09be\u09a8\u09c7\u0964 \u09ae\u09cb\u09ac\u09be\u0987\u09b2\u09c7\u09b0 \u099c\u09a8\u09cd\u09af \u098f\u0995\u099f\u09c1 \u09b8\u09cd\u0995\u09cd\u09b0\u09b2 \u09a1\u09be\u0989\u09a8 \u0995\u09b0\u09c7 \u099a\u09be\u09aa \u09a6\u09bf\u09a8 \u0993\u09aa\u09b0\u09c7\u09b0 \"\u09b2\u09bf\u09b8\u09cd\u099f\" \u09ac\u09be\u099f\u09a8\\) [\u09a6\u09c7\u0996\u09c1\u09a8 \u09ad\u09bf\u09a1\u09bf\u0993\u0964](https://www.facebook.com/mltraining/videos/757857641090247/)\n\n\u09aa\u09cd\u09b0\u09df\u09cb\u099c\u09a8\u09c0\u09df \u09af\u09cb\u0997\u09be\u09af\u09cb\u0997: [https://github.com/raqueeb](https://github.com/raqueeb) \n\n\u0987\u098a\u099f\u09bf\u098a\u09ac \u099a\u09cd\u09af\u09be\u09a8\u09c7\u09b2 [https://goo.gl/aiWJso](https://goo.gl/aiWJso) \n\n"
 },
 {
  "repo": "bensadeghi/Databricks-DataScience-Titanic",
  "language": "Python",
  "readme_contents": "# Databricks-DataScience-Titanic\nA walk-through of data science basics using PySpark and the [Titanic dataset](https://www.kaggle.com/c/titanic/)\n\n### Content\n- Introduction to Spark and Databricks\n- Extracting Data\n- Cleaning Data\n- What is ML?\n- ML Workflows\n- Model Selection\n- Model Serving\n"
 },
 {
  "repo": "linxinzhe/tensorflow-titanic",
  "language": "Jupyter Notebook",
  "readme_contents": "# Kaggle-TensorFlow-Titanic\nKaggle - Titanic: Machine Learning from Disaster\n\nCompetition Description\n-------\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history. In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\nMore details please visit https://www.kaggle.com/c/titanic\n\nCode Environment\n-------\nYou can use [Anaconda](https://www.anaconda.com/download/) to install the code environment.\n1. conda env create -f environment.yaml\n2. source activate tensorflow\n3. jupyter notebook\n\nImplementation Details\n-------\n1. Data processing.\n2. Data normalization.\n3. Data split into train set, dev(cross validation) set, test set.\n4. Data classification by using Deep Learning implemented in TensorFlow.\n"
 },
 {
  "repo": "SAUVIK/TitanicSurvivalSet",
  "language": "R",
  "readme_contents": "# TitanicSurvivalSet"
 },
 {
  "repo": "savarin/titanic",
  "language": "Python",
  "readme_contents": "# Titanic - Getting 0.799 with Random Forests and Gradient Boosting\n\n## Details\n\nThe materials here build on Section 1-5 the [Kaggle Berlin Introductory Tutorial](https://github.com/savarin/kaggleberlin-introtutorial),\ncomprising parameter-tuned implementations of Random Forests and Gradient Boosting, as\nwell as the ensemble of both models.\n\n## Tutorial\n\nThe tutorial provides a more detailed, step-by-step explanation:  \n- [Section 1-0 - First Cut.ipynb](http://nbviewer.ipython.org/github/savarin/kaggleberlin-introtutorial/blob/master/notebooks/Section%201-0%20-%20First%20Cut.ipynb)\n- [Section 1-1 - Filling-in Missing Values.ipynb](http://nbviewer.ipython.org/github/savarin/kaggleberlin-introtutorial/blob/master/notebooks/Section%201-1%20-%20Filling-in%20Missing%20Values.ipynb)\n- [Section 1-2 - Creating Dummy Variables.ipynb](http://nbviewer.ipython.org/github/savarin/kaggleberlin-introtutorial/blob/master/notebooks/Section%201-2%20-%20Creating%20Dummy%20Variables.ipynb)\n- [Section 1-3 - Parameter Tuning.ipynb](http://nbviewer.ipython.org/github/savarin/kaggleberlin-introtutorial/blob/master/notebooks/Section%201-3%20-%20Parameter%20Tuning.ipynb)\n- [Section 1-4 - Building Pipelines.ipynb](http://nbviewer.ipython.org/github/savarin/kaggleberlin-introtutorial/blob/master/notebooks/Section%201-4%20-%20Building%20Pipelines.ipynb)\n- [Section 1-5 - Final Checks.ipynb](http://nbviewer.ipython.org/github/savarin/kaggleberlin-introtutorial/blob/master/notebooks/Section%201-5%20-%20Final%20Checks.ipynb)\n\nIn addition, further discussion is provided on cross-validation and visualisation:\n- [Appendix A - Cross-Validation.ipynb](http://nbviewer.ipython.org/github/savarin/kaggleberlin-introtutorial/blob/master/notebooks/Appendix%20A%20-%20Cross-Validation.ipynb)\n- [Appendix B - Visualisation.ipynb](http://nbviewer.ipython.org/github/savarin/kaggleberlin-introtutorial/blob/master/notebooks/Appendix%20B%20-%20Visualisation.ipynb)"
 },
 {
  "repo": "HIMIFDA/machine-learning-titanic",
  "language": "Python",
  "readme_contents": "# Simple Implementation of Machine Learning Using Softmax Regression.\n\n\n### Pendahuluan\nPada 14 april 1912 di Southampton, kapal titanic menarik jangkarnya dan melakukan ekspedisi pertamanya menuju New York City. Titanic merupakan kapal terbesar yang beroperasi pada masa itu, mengangkut sekitar 2.224 orang. Kapal yang digadang-gadang tidak bisa tenggelam ini sayangnya malah menabrak gunung es pada pukul 23:40. Kapal ini tenggelam dua jam empat puluh menit kemudian pada pukul 02:20 hari Minggu, 15 April (05:18 GMT) dan mengakibatkan kematian lebih dari 1.500 penumpangnya. Tenggelamnya Titanic adalah salah satu bencana maritim masa damai mematikan sepanjang sejarah.\n\nDari data yang didapat (kita hanya memiliki 1308 data) kita bisa menghitung rata-rata bahwa persentase selamat dari seluruh penumpang hanya sebesar 38% hal ini disebabkan pada saat itu kapal titanic tidak membawa sekoci yang bisa menampung seluruh penumpangnya. Dari 38% persentase selamat itu ternyata semakin tinggi kelasnya maka akan semakin besar persentase selamatnya, kita pun bisa kita bagi lagi menjadi persentase perkelas menjadi:\n\n - Kelas 1 sebesar 61%\n - Kelas 2 sebesar 42%\n - Kelas 3 sebesar 25%\n\nSepertinya pada saat kapal mewah ini tenggelam, penumpang dengan jenis kelamin perempuan lebih diprioritaskan untuk dapat dievakuasi. Hal tersebut dibenarkan setelah kita melihat data dan mengkalkulasikan persentasenya, dari persentase selamat perkelas diatas, kita bisa mengkalkulasikan persetase selamat perjeniskelamin  dan mendapatkan hasil:\n\n - Kelas 1\n       Perempuan:  96%\n       Laki-laki: 34%\n - Kelas 2\n       Perempuan:  88%\n       Laki-laki: 14%\n - Kelas 3\n      Perempuan:  49%\n      Laki-laki: 15%\n\nBeruntunglah mereka yang berusia lebih muda, karna setelah kita melihat dan mengkalkulasikan data lebih dalam lagi, kita mendapatkan kesimpulan bahwa semakin muda umurnya, maka persentase selamatnya akan lebih besar, berikut perhitungan persentase berdasarkan umur kurang dari atau lebih dari 25 tahun:\n\n- Kelas 1\n       Perempuan:\n\t       - Di bawah 25 tahun: 96%\n\t       - Di atas 25 tahun: 96%\n       Laki-laki:\n\t       - Di bawah 25 tahun: 36%\n\t       - Di atas 25 tahun: 33%\n- Kelas 2\n       Perempuan:\n\t       - Di bawah 25 tahun: 93%\n\t       - Di atas 25 tahun: 86%\n       Laki-laki:\n\t       - Di bawah 25 tahun: 36%\n\t       - Di atas 25 tahun: 7%\n- Kelas 3\n      Perempuan:\n\t       - Di bawah 25 tahun: 52%\n\t       - Di atas 25 tahun: 44%\n       Laki-laki:\n\t       - Di bawah 25 tahun: 14%\n\t       - Di atas 25 tahun: 15%\n   \n\n### Machine Learning\n*Machine learning* merupakan subset dari *artificial intelligence*, *machine learning* merupakan teknik mengintruksikan komputer untuk bertindak tanpa perlu di program secara eksplisit.  Atau, penjelasan lebih detailnya seperti ini:\n\n    Sebuah program komputer dikatakan belajar dari pengalaman E yang berhubungan dengan tugas T dan mengukur kinerja P jika kinerjanya pada T, yang diukur dengan P, ditingkatkan dengan pengalaman E.\n    Dalam contoh kasus titanic, apa P, T dan E?\n    \n    Answer\n    P = kemungkinan selamat penumpang titanic.\n    T = menghitung probabilitas keselamatan penumpang titanic.\n    E = algoritme untuk menghitung probabilitas berdasarkan datasets.\n\nDengan *machine learning* kita bisa membuat komputer membuat keputusan berdasarkan data-data yang kita berikan, data-data tersebut akan diproses berdasarkan algoritme tertentu yang pada akhirnya akan mengeluarkan sebuah *output*/hipotesa. Masalah-masalah yang sebelumnya terlihat mustahil untuk dipecahkan, sekarang sudah mulai bisa dipecahkan dengan *machine learning*. dalam dekade terakhir, *machine learning* telah memberi kita *self-driving car*([Tesla Autopilot Self-Driving Car](https://www.youtube.com/watch?v=PUw_DMaQ264&t=12s)), *search engine* ([RankBrain](https://en.wikipedia.org/wiki/RankBrain)), image recognition([Facebbook's Image Recognition](http://www.theverge.com/2016/8/25/12630850/facebook-fair-deepmask-sharpmask-ai-image-recognition)) dan masih banyak lagi.\n\n### Project\nPada project ini, kita akan menghitung probabilitas keselamatan penumpan titanic berdasarkan kelas, umur, dan jenis kelamin. Teknik yang digunakan adalah teknik [*supervised learning*](https://en.wikipedia.org/wiki/Supervised_learning), kita akan melakukan klasifikasi terhadap data-data yang kita punya sehingga bisa mengeluarkan *output* label apakah penumpang tersebut akan selamat atau tidak.\n\nKita menggunakan 3 fitur(x1, x2, x3) untuk membuat hipotesa:\n\n 1. Kelas\n 2. Umur\n 3. Jenis kelamin\n\nAlgoritme yang digunakan adalah [*softmax regression*](http://www.kdnuggets.com/2016/07/softmax-regression-related-logistic-regression.html).\n\nKita menggunakan Python sebagai bahasa pemrogrammannya dan menggunakan beberapa *tools* diantaranya:\n\n - [TensorFlow](https://tensorflow.org)\n - [Sci-kit](http://scikit-learn.org/)\n - [Numpy](http://www.numpy.org/)\n - [Pandas](http://pandas.pydata.org/)\n\n### Instalasi\nPastikan kalian menggunakan Python3 di komputer kalian dan pastikan kalian juga sudah menginstall pip3 di komputer kalian.\n\nPertama, jalankan setup.sh\n    \n    > ./setup.sh\n\n- Kita menjanlankan setup.sh untuk menginstall segala depedencies yang dibutuhkan.\n- Setelah semua depedencies dibutuhkan kita akan melatih model kita\n- Setelah model kita selesai dilatih kita akan menyimpan model kita pada folder /model\n\nSetelah melewati tahap pertama, kita sudah bisa menjalankan aplikasi kita\n\n    > python3 main.py\n\n   \nBuka browser kalian dan akses http://0.0.0.0:5000\n\n### Kontributor\n- [Andrea Damayanti](https://www.facebook.com/Chedhit.4ever?fref=ts)\n- [Pramesti Hatta K.](https://facebook.com/opam22)\n- [Trianto Fajar](https://www.facebook.com/insinyur.sth?fref=ts)\n- [Windi Triyani](https://www.facebook.com/windy.triyani?fref=ts)\n\n### License\n\u00a9 2017 Himpunan Mahasiswa Informatika Unsada\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. \n"
 },
 {
  "repo": "Angad2016/Logistic-Regression-Survival-State",
  "language": null,
  "readme_contents": "# Logistic-Regression-Survival-State\nIn this scenario, we were given data of the titanic's passengers and we had to predict survival rates. Utilised K fold valisation as well. \n"
 },
 {
  "repo": "Currie32/Titanic-Kaggle-Competition",
  "language": "HTML",
  "readme_contents": "# Titanic-Kaggle-Competition\nMy analysis for the 'Titanic: Machine Learning from Disaster' competition, hosted by Kaggle.com\n\nThis was the first competition that I entered on Kaggle.com, and I am pleased to say that my submission scored in the top 9%.\n\nFiles:\nTitanic_Analysis.html - an html file of my analysis.\nTitanic_Analysis.py - a python file of my analysis.\ntest.csv - the test data that was provided by Kaggle.com.\ntrain.csv - the train data that was provided by Kaggle.com.\n"
 },
 {
  "repo": "moeabdol/kaggle-titanic",
  "language": "Python",
  "readme_contents": "Introduction\n--\nMy solution to the Kaggle Titanic competition. Achieving accuracy score of 78% (0.77512).  Note: running the code may last hours. It took around 2 hours of execution time on an early 2014 MacBook Pro 2.3Ghz 8 core machine.\n\nInstructions\n--\n$ python main.py\n\nDependencies\n--\npython 2.7.9\n\npandas 0.15.2\n\nsklearn 0.15.2\n\nmatplotlib 1.4.2\n\nProject Files\n--\nREADME.md                   This readme file describing the project.\n\ntest.csv                    Testing dataset.\n\ntrain.csv                   Training dataset.\n\nmain.py                     Starting execution point of this project.\n\nage.py                      Script handeling the age feature.\n\nembarked.py                 Script handeling the embarked feature.\n\nimportance.py               Script handeling extracting feature importances.\n\ninteraction_features.py     Script handeling the creation of interaction features.\n\nname.py                     Script handeling the name feature.\n\nscale.py                    Script handeling the scaling of features.\n\nsex.py                      Script handeling the sex feature.\n\nfeatrue_importances.png     Figure demonstrating feature importances.\n\nlearning_curves.png         Figure demonstrating the training and testing learning accuracy curves.\n\nroc_curve.png               Figure demonstrating the ROC curve.\n\noptimize.py                 Script handeling the optimization of the classifier's hyperparameters.\n\nlearning_curves.py          Script handeling the creation and ploting of learning curves.\n\npredict.py                  Script handeling the survival prediction of the testing set.\n\nresult.csv                  The output prediction set.\n\nbest_params.csv             Intermediate result of the optimal hyperparameters.\n\nload.py                     Script handeling the loading of training and testing datasets.\n\npreprocess.py               Script handeling the preprocessing and cleaning of datasets.\n"
 },
 {
  "repo": "ritvikmath/Machine-Learning-With-Titanic-Data",
  "language": "Jupyter Notebook",
  "readme_contents": "# Machine Learning with Titanic Data\n\n## Directions to Run Pipeline\n\n1. git clone https://github.com/ritvikmath/Machine-Learning-With-Titanic-Data.git\n\n2. create two empty folders in the working directory called \"model_plots\" and \"model_outputs\"\n\n3. run the pipeline with \"python Titanic.py\"\n\n4. use the command \"python Titanic.py clean\" to restore to initial state (note: will remove all generated files)\n\n\n"
 },
 {
  "repo": "massquantity/Kaggle-Titanic",
  "language": "Jupyter Notebook",
  "readme_contents": "# Kaggle - Titanic: Machine Learning from Disaster\n#### This is a kaggle kernel I wrote for `Titanic` dataset. For your convenience, please view it in [NbViewer](http://nbviewer.jupyter.org/github/massquantity/Kaggle-Titanic/blob/master/Titanic.ipynb)\n------------------------\n>The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew.  This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\n>One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.  Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\n>In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n\nFrom the competition [homepage](http://www.kaggle.com/c/titanic-gettingStarted).\n\n\n\n![Image](http://a3.qpic.cn/psb?/V107khlM1bLYMn/YgCcZdpUwL2f67Nk0LymNVgtogg1Lkol3mKjkSyjrQs!/b/dG0BAAAAAAAA&bo=YAOAAgAAAAARB9E!&rf=viewer_4)\n\n\n## Content:\n1. Data Cleaning\n2. Exploratory Visualization\n3. Feature Engineering\n4. Basic Modeling & Evaluation\n5. Hyperparameters tuning\n6. Ensemble Methods\njk\n"
 },
 {
  "repo": "lydiajessup/titanic-sketch",
  "language": "JavaScript",
  "readme_contents": ""
 },
 {
  "repo": "Johnkamauwanjehia/Data-analysis-Titanic",
  "language": "Python",
  "readme_contents": "# Data-analysis-Titanic\nSimple Data Analysis using python /Titanic data set\nThe dataset was obtained from Kaggle Datasets link:https://www.kaggle.com/broaniki/titanic\n\nData was analysed using juypter notebok.\n\nimport\n1. pandas \n2. numpy \nThen there is use of matplotlib\n"
 },
 {
  "repo": "codebugged/Titanic",
  "language": "Python",
  "readme_contents": "error: no README"
 },
 {
  "repo": "takahiro-777/titanic",
  "language": "Python",
  "readme_contents": "# README\n\n### \u6982\u8981\nkaggle\u306e\u7df4\u7fd2\u7528\u3068\u3057\u3066\u3088\u304f\u4f7f\u308f\u308c\u308btitanic\u306e\u30e2\u30c7\u30eb\u69cb\u7bc9\u306b\u5f53\u305f\u3063\u3066\u5f97\u305f\u77e5\u898b\u3068\u540c\u6642\u306b\u4f5c\u6210\u3057\u305f\u30bd\u30fc\u30b9\u30b3\u30fc\u30c9\u306b\u95a2\u3057\u3066\u307e\u3068\u3081\u308b\u3002  \nhttp://qiita.com/suzumi/items/8ce18bc90c942663d1e6  \n\u3042\u304f\u307e\u3067\u30d9\u30fc\u30b9\u306e\u30b3\u30fc\u30c9\u306a\u306e\u3067\u3001\u4e0a\u8a18\u3092\u53c2\u8003\u306b\u7c21\u5358\u306b\u307e\u3068\u3081\u305f\u3002\n\n### \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u69cb\u6210\n- data/  \n\u5143\u30c7\u30fc\u30bf\u306b\u95a2\u3057\u3066\u307e\u3068\u3081\u3066\u3042\u308b\u3002  \nhttp://qiita.com/suzumi/items/8ce18bc90c942663d1e6  \n\u8a73\u7d30\u306e\u30c7\u30fc\u30bf\u306e\u8aac\u660e\u306b\u95a2\u3057\u3066\u306f\u4e0a\u8a18\u306a\u3069\u304c\u8a73\u3057\u3044  \n\n- src/  \n\u958b\u767a\u306b\u5f53\u305f\u3063\u3066\u30b9\u30af\u30ea\u30d7\u30c8\u3092\u307e\u3068\u3081\u3066\u3042\u308b\u3002  \ntrain.py\u3001test.py\u306f\u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8\u3092\u5229\u7528\u3057\u305f\u30d9\u30fc\u30b9\u306e\u51e6\u7406\u30d5\u30ed\u30fc\uff08\u6b63\u7b54\u7387\u306f62%\u307b\u3069\uff09\u3002  \nrgeos.py\u306fhttps://github.com/rgeos/meetup/blob/master/Titanic.ipynb\n\u3092\u53c2\u8003\u306b\u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u30e2\u30c7\u30eb\u3092\u7d44\u3093\u3060\u3082\u306e\uff08\u6b63\u7b54\u738775%\u307b\u3069\uff09\u3002  \n\n- model/  \n\u4f5c\u6210\u3057\u305f\u30e2\u30c7\u30eb\u3092\u4fdd\u5b58\u3059\u308b  \n\n### \u5b9f\u884c\u65b9\u6cd5\n- \u8a2d\u5b9a\n```\ngit clone https://github.com/takahiro-777/titanic.git\ncd titanic  #\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u30eb\u30fc\u30c8\u306b\u79fb\u52d5\n```\n\n- \u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8  \n\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u30eb\u30fc\u30c8\u4e0b\u3067\u3001  \n```\npython src/train.py  \npython src/test.py  \n```\n\u3067\u5b66\u7fd2\u2192\u4e88\u6e2c\u304c\u53ef\u80fd\u3002\u30e2\u30c7\u30eb\u306b\u95a2\u3057\u3066\u306fmodel\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u3001\u4e88\u6e2c\u7d50\u679c\u306b\u95a2\u3057\u3066\u306foutput\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u4fdd\u5b58\u3055\u308c\u308b\u3002\uff08kaggle\u306e\u63d0\u51fa\u306e\u5f62\u5f0f\u306b\u306a\u3063\u3066\u3044\u308b\uff09  \n\n- \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\n\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u30eb\u30fc\u30c8\u4e0b\u3067\u3001\n```\npython src/rgeos.py\n```\n\u3067\u30e2\u30c7\u30eb\u306e\u4f5c\u6210\u3001\u4e88\u6e2c\u304c\u53ef\u80fd\u3002\u4e88\u6e2c\u7d50\u679c\u306b\u95a2\u3057\u3066\u306foutput\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u4fdd\u5b58\u3055\u308c\u308b\u3002\uff08kaggle\u306e\u63d0\u51fa\u306e\u5f62\u5f0f\u306b\u306a\u3063\u3066\u3044\u308b\uff09\n"
 },
 {
  "repo": "robertolima-dev/MachineLearning-TitanicData",
  "language": "Jupyter Notebook",
  "readme_contents": "# Machine learning training\n\n**Author: Roberto Lima**\n\nLogistic Regression\n\nMachine Learning Training using Python.\n\nThis training was developed on Jupyter Notebook (Anaconda)\n\nhttps://jupyter.org/install\n\n\nLibraries used in this training:\n\n- Numpy\n- Pandas\n- Matplotlib\n- Seaborn\n- Sklearn\n\n"
 },
 {
  "repo": "murufeng/Titanic",
  "language": "R",
  "readme_contents": "# Titanic\nKaggle\u7ade\u8d5b\uff1a\u9884\u6d4b\u6cf0\u5766\u5c3c\u514b\u53f7\u4e2d\u4e58\u5ba2\u7684\u83b7\u6551\u6982\u7387\u4ee5\u53ca\u5b58\u6d3b\u60c5\u51b5\n"
 },
 {
  "repo": "bieri/zmq_titanic",
  "language": "C",
  "readme_contents": ""
 },
 {
  "repo": "ledmaster/TutorialTitanic",
  "language": "Jupyter Notebook",
  "readme_contents": "Material para o tutorial da playlist de v\u00eddeos sobre Machine Learning usando os dados do Titanic\n\nhttps://www.youtube.com/playlist?list=PLwnip85KhroW8Q1JSNbgl06iNPeC0SDkx"
 },
 {
  "repo": "AgarwalGeeks/Titanic-Project",
  "language": "Jupyter Notebook",
  "readme_contents": "# Titanic-Project"
 },
 {
  "repo": "kaburelabs/Titanic-prediction-with-Keras",
  "language": "Jupyter Notebook",
  "readme_contents": "# Titanic-prediction-with-Keras\nMy Kernel from Kaggle\n\nIt's a backup of my kernel on Kaggle https://www.kaggle.com/kernels/scriptcontent/18546554/download\n"
 },
 {
  "repo": "mrc03/Titanic-Survivor-Prediction",
  "language": "Jupyter Notebook",
  "readme_contents": "\n/* Author::  Raj Mehrotra\n   Date:: 24/06/2018\n*/\n\nThe Titanic: Machine Learning from Disaster competiton. With data being provided of varoius passengers traveling on the ship I have used libraries\nlike numpy,pandas to manipulate , explore and analyze the data and libraries like matplotlib and seaborn to visualise the data. Lastly I have used various machine learning \nmodels to make predictions on the formerly cleaned and preprocessed data. Then I used GridSearchCV to optimise the parameters of the various models\n\nFinally I got an accuracy of 0.77990 on the kaggle test set .\n"
 },
 {
  "repo": "sharmaroshan/Titanic-Passenger-Survival-Prediction",
  "language": "Jupyter Notebook",
  "readme_contents": "# Titanic-Passenger-Survival-Prediction\nUsing Classification Techniques, Data reprocessing, Feature Engineering, Feature Extraction and Classification Algorithms from Machine Learning to Predict who can Survive the attack of Tsunami.\n\n# Data Description\n\n# Overview\nThe data has been split into two groups:\n\ntraining set (train.csv)\ntest set (test.csv)\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \u201cground truth\u201d) for each passenger. Your model will be based on \u201cfeatures\u201d like passengers\u2019 gender and class. You can also use feature engineering to create new features.\n\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n\nWe also include gender_submission.csv, a set of predictions that assume all and only female passengers survive, as an example of what a submission file should look like.\n\n# Data Dictionary\n\nVariable\tDefinition\tKey\nsurvival\tSurvival\t0 = No, 1 = Yes\npclass\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\nsex\tSex\t\nAge\tAge in years\t\nsibsp\t# of siblings / spouses aboard the Titanic\t\nparch\t# of parents / children aboard the Titanic\t\nticket\tTicket number\t\nfare\tPassenger fare\t\ncabin\tCabin number\t\nembarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton\nVariable Notes\npclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.\n"
 },
 {
  "repo": "TensShinet/Kaggle-For-Beginners",
  "language": "Jupyter Notebook",
  "readme_contents": "# \u6570\u636e\u79d1\u5b66\u7ade\u8d5b\u5165\u95e8\n\n\n\n\u5728\u6587\u4ef6 `Titanic Data Science Solutions.ipynb` \u7ffb\u8bd1\u4e86\u4e00\u7bc7\u56fd\u5916\u5927\u795e\u7684***[\u6570\u636e\u79d1\u5b66\u7ade\u8d5b\uff08Kaggle\uff09](https://kaggle.com/startupsci/titanic-data-science-solutions/notebook)***\u5165\u95e8\u7684\u6587\u7ae0\u3002\n\n\n\n**\u73b0\u5728\u5f00\u59cb\u603b\u7ed3\u8fd9\u7bc7\u6587\u7ae0\u5230\u5e95\u8bb2\u4e86\u4ec0\u4e48**\n\n\n\n## \u603b\u7ed3\n\n\n\n### \u4e00\u5207\u90fd\u8981\u4ece\u6570\u636e\u5df2\u7ed9\u7684\u7279\u5f81\u5f00\u59cb\n\n```python\ntrain_df.columns.values\n```\n\n\n\n\u901a\u8fc7\u89c2\u5bdf\u8981\u56de\u7b54\u4e0b\u8ff0\u591a\u4e2a\u95ee\u9898\n\n\n\n+ **\u6570\u636e\u96c6\u4e2d\u6709\u54ea\u4e9b\u7279\u5f81\u662f\u53ef\u4ee5\u6709\u7c7b\u522b\u7684\uff1f** \n+ **\u6570\u636e\u96c6\u4e2d\u6709\u54ea\u4e9b\u7279\u5f81\u662f\u6570\u5b57\uff1f**\n+ **\u6709\u54ea\u4e9b\u7279\u5f81\u503c\u5305\u542b\u7a7a\u6570\u636e\uff1f**\n+ **\u6240\u6709\u7279\u5f81\u7684\u6570\u636e\u7c7b\u578b\u662f\u4ec0\u4e48\uff1f**\n+ **\u6bcf\u4e00\u4e2a\u7279\u5f81\u7684\u6570\u503c\u6709\u54ea\u4e9b\u7279\u70b9\uff1f**\n+ **\u6709\u7c7b\u522b\u7684\u7279\u5f81\u7684\u5206\u5e03\u662f\u4ec0\u4e48**\n\n \u56de\u7b54\u5b8c\u4e0a\u8ff0\u95ee\u9898\u4e4b\u540e\uff0c\u6211\u4eec\u9700\u8981\u505a\u51fa\u57fa\u4e8e\u6570\u636e\u5206\u6790\u7684\u5047\u8bbe\u3002\u5176\u4e2d\u6700\u91cd\u8981\u7684\u4e00\u6b65\u662f**\u5224\u65ad\u76f8\u5173\u6027\uff01**\n\n+ \u5728\u6211\u4eec\u5efa\u7acb\u6a21\u578b\u7684\u65e9\u671f\u5c31\u8981\u5224\u65ad\uff0c\u7279\u5f81\u4e0e\u76ee\u6807\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002\n+ \u8865\u5168\u67d0\u4e9b\u7279\u5f81\u503c\n+ \u7ea0\u6b63\u67d0\u4e9b\u7279\u5f81\uff0c\u6bd4\u5982\u5220\u6389\u67d0\u4e9b\u7279\u5f81\uff0c\u6216\u8005\u5bf9\u67d0\u4e9b\u7279\u5f81\u505a\u5904\u7406\n+ \u521b\u5efa\u65b0\u7684\u7279\u5f81\n+ \u6839\u636e\u7c7b\u522b\u6dfb\u52a0\u6211\u4eec\u7684\u5047\u8bbe\n\n\n\n### \u76f8\u5173\u6027\n\n\n\n\u4e3a\u4e86\u56de\u7b54\u7b2c\u4e00\u4e2a\u95ee\u9898\uff0c\u91c7\u7528**\u53ef\u89c6\u5316\u7684\u624b\u6bb5\u5206\u6790\u6570\u636e**\n\n\n\n+ **\u6570\u503c\u7279\u5f81**\u4e0e\u76ee\u6807\u7684\u76f8\u5173\u6027\uff08\u53ef\u5229\u7528\u76f4\u65b9\u56fe\uff09\n+ **\u6709\u5e8f\u7279\u5f81\uff08\u6709\u6570\u5b57\u7f16\u53f7\uff09**\u4e0e\u76ee\u6807\u7684\u76f8\u5173\u6027\uff08\u53ef\u5229\u7528\u76f4\u65b9\u56fe\uff09\n+ **\u5206\u7c7b\u7279\u5f81\uff08\u5b57\u7b26\u4e32\u7f16\u53f7\uff09**\u4e0e\u76ee\u6807\u7684\u76f8\u5173\u6027\uff08pointplot\u3001\u76f4\u65b9\u56fe\uff09\n\n\n\n### \u6574\u7406\u6570\u636e\n\n+ **\u901a\u8fc7\u4e22\u5f03\u6570\u636e\u6765\u6574\u7406**\n+ **\u8865\u5145\u8fde\u7eed\u7279\u5f81\u503c**\n  + \u4e00\u79cd\u7b80\u5355\u7684\u65b9\u6cd5\u662f\u5728\u5747\u503c\u548c\u6807\u51c6\u5dee\u4e4b\u95f4\u751f\u6210\u968f\u673a\u6570\u3002\n  + \u731c\u6d4b\u7f3a\u5931\u503c\u7684\u66f4\u51c6\u786e\u65b9\u6cd5\u662f\u4f7f\u7528\u5176\u4ed6\u76f8\u5173\u7279\u5f81\u3002\n  + \u731c\u6d4b\u7f3a\u5931\u503c\u7684\u66f4\u51c6\u786e\u65b9\u6cd5\u662f\u4f7f\u7528\u5176\u4ed6\u76f8\u5173\u7279\u5f81\u3002\n+ **\u521b\u5efa\u65b0\u7684\u7279\u5f81**\n+ **\u5c06\u6240\u6709\u6570\u636e\u53d8\u6210\u6570\u5b57**\n\n\n\n### \u4f7f\u7528\u5e38\u89c1\u5206\u7c7b\u6a21\u578b\u5206\u7c7b\n\n- Logistic Regression\n- KNN or k-Nearest Neighbors\n- Support Vector Machines\n- Naive Bayes classifier\n- Decision Tree\n- Random Forrest\n- Perceptron\n- Artificial neural network\n- RVM or Relevance Vector Machine\n\n\u6700\u540e\u9009\u4e00\u4e2a\u5206\u7c7b\u6548\u679c\u6700\u597d\u7684\n\n\n\n## \u6700\u540e\n\n\u5f85\u6211\u770b\u5b8c\u300a\u673a\u5668\u5b66\u4e60\u5b9e\u6218\u300b\u518d\u66f4\u65b0\u4e00\u4e9b Scikit-Learn \u548c TensorFLow \u7684\u5b9e\u6218\u7528\u6cd5\u3002\n\n\n\n\n\n\n\n\n\n"
 },
 {
  "repo": "zainsiddiqui/Predicting-Survival-on-Titanic-with-Machine-Learning-and-Graphical-User-Interface",
  "language": "Python",
  "readme_contents": "# Survival of Passengers on Titanic using Machine Learning and Graphical User Interface\n\nThis project consists of a clean and polished Graphical User Interface (GUI) that interacts with 8 Machine Learning models and data visualization tools through the use of different Python libraries. At Rutgers, we learned about Python being a great general purpose language, which allows for great versatility for developers of all specialties. Therefore, we decided to take advantage of Python\u2019s strong support for GUI development as well as its Data Science and Machine Learning capabilities. Using the complex RMS Titanic data set, which includes information about each passengers fate (survived/deceased) according to their economic status, fair, cabin, social class, relatives, gender, port of embarkment and age, we created _8_ different Machine Learning models that learn from the data set and then perform accurate predictions of survival on the testing data provided by the user. In addition, we created an extensive GUI that allows the user to learn and interact with the training and testing data by displaying many different data plots and graphs as well as descriptions about the specifics (advantages and disadvantages) of each Machine Learning model. The user can interact with the GUI through selecting which model to run the testing data on, which then takes them to a screen displaying the prediction results of the testing data as well as the general model accuracy. The screen also includes various buttons that, when selected, display complex and attractive data visualizations on the testing data. The goal for this project was to get a good understanding of Python\u2019s Data Science and Machine Learning support and to learn about GUI development and integration in Python. Upon completing the project, we had an increased appreciation for the power of Machine Learning and its potential as well as the customizability and complexities of GUI development. By partaking in GUI development, Data Manipulation/Visualization creation and Machine Learning development, this project is a clear representation of the power of the Python programming language and its overall integrability.\n\n## Machine Learning Models Implemented:\n- Logistic Regression\n- Stochastic Gradient Descent\n- K Nearest Neighbor\n- Random Forest\n- Naive Bayes\n- Perceptron\n- Linear Support Vector Machine\n- Decision Tree\n\n\n## Dependencies:\n* Numpy\n* Pandas\n* SciKit-Learn\n* SciPy\n* Seaborn\n* Matplotlib\n* Tkinter\n* Pillow\n\n\n## GUI Preview:\n\n![image](https://user-images.githubusercontent.com/39894720/58128517-d30bbd80-7be5-11e9-8fa3-164298563cb7.png)\n\n![image](https://user-images.githubusercontent.com/39894720/58128427-99d34d80-7be5-11e9-83ac-6cfe60096de8.png)\n\n![image](https://user-images.githubusercontent.com/39894720/58125962-ce440b00-7bdf-11e9-9576-6e0bdb5d2659.png)\n\n![image](https://user-images.githubusercontent.com/39894720/58125968-d308bf00-7bdf-11e9-8b76-6c9450c2eaaa.png)\n\n\n\n## Installation:\n1. Download and extract zip file\n\n2. Create virtual environment inside FinalProject directory using\n\"python3 -m venv venv\"\n\n3. Run virtual environment in project directory using\n\"source venv/bin/activate\"\n\n4. Install required dependencies using\n\u201cpip install -r requirements.txt\u201d\n\n5. Run GUI using\n\"python GUI.py\"\n\n6. Exit virtual environment\n\"deactivate\"\n\nNote: User can input their own testing data to predict by changing the \"test.csv\" file with custom information. Do not delete first row of CSV as it contains column headers!\n\n\n## Contributors:\nZain Siddiqui, Krupal Patel, Haneef Pervez\n\n"
 },
 {
  "repo": "aa4933/php-ml-TitanicSurvived",
  "language": "PHP",
  "readme_contents": "# php-ml-TitanicSurvived\n\u5173\u4e8e\u5e94\u7528php-ml\u5e93\u8fdb\u884c\u9884\u6d4b\u6cf0\u5766\u5c3c\u514b\u53f7\u751f\u8fd8\u7387\u7684\u5165\u95e8\u542f\u8499\u6848\u4f8b\uff0fOn the application of php-ml database to predict the Titanic survival rate of entry case enlightenment\n\n\n## \u6559\u7a0b\u5730\u5740\n\nhttp://wow0.cn/2017/11/03/php-ml-apply/\n"
 },
 {
  "repo": "dataworkshop/titanic",
  "language": "Jupyter Notebook",
  "readme_contents": "# titanic\nTitanic: Machine Learning from Disaster\n"
 },
 {
  "repo": "joedayz/titanic-javaee7",
  "language": "Java",
  "readme_contents": "error: no README"
 },
 {
  "repo": "adeshpande3/KaggleTitanic",
  "language": "Python",
  "readme_contents": "# KaggleTitanic\nKaggle Titanic Survival ML Competition\n\nI've been using SciKit Learn to call different ML algorithms that can be used to create a model that predicts survival on the Titanic, given different characteristics on each passenger. Right now, the accuracies (when tested on Kaggle) are shown below:\n\nKNN (k = 17) - **78.95%**\n\nSVM - **76.55%**\n\nLogistic Regression - **76.55%**\n\nRandom Forest (estimators = 100) - **74.61%**\n\nDecision Tree - **70.81%**\n\n\n"
 },
 {
  "repo": "GeekyTheory/Taller-Big-Data-R-Titanic-IEEE",
  "language": "R",
  "readme_contents": "error: no README"
 },
 {
  "repo": "jayadeepj/r-kaggle-titanic",
  "language": "R",
  "readme_contents": "\n# r-kaggle-titanic\n#Titanic Survival Prediction\n\nThis repository contains some of my approaches to the Titanic survival prediction Problem from Kaggle.  The repository includes scripts for feature selection, alternate strategies for data modelling, the original test &amp; train data sets and the visualizations plots generated for the same. All code snippets are written in R.\n\nTitanic survival prediction Problem\nhttp://www.kaggle.com/c/titanic-gettingStarted\n\nIn this popular challenge, the aim is to predict what sorts of people were likely to survive the Titanic disaster based on attributes such as gender, class, ticket details , age category e.t.c . \n\n## Code Example\n\nTODO\n\n## Motivation\n\nOne approach among the listed samples code was submitted to Kaggle.\n\n## Installation\n\nThe datasets can be found in 'data' folder. It includes 2 csv files for train & test respectively.\n\ntrain.csv (59.76 kb)  \ntest.csv (27.96 kb)\n\nFollowing R packages are used.\n\nseqinr: Biological Sequences Retrieval and Analysis\nhttps://cran.r-project.org/web/packages/seqinr/index.html\n\ne1071: Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien\nhttps://cran.r-project.org/web/packages/e1071/index.html\n\n\nparty: A Laboratory for Recursive Partytioning\nhttps://cran.r-project.org/web/packages/party/index.html\n\n\nAmelia: Amelia II: A Program for Missing Data\nhttps://cran.r-project.org/web/packages/Amelia/index.html\n\nggplot2: An Implementation of the Grammar of Graphics\nhttps://cran.r-project.org/web/packages/ggplot2/index.html\n\n\nhttps://cran.r-project.org/web/packages/corrgram/index.html\ncorrgram: Plot a Correlogram\n\n\n## API Reference\n\nFollowing scripts need to be run seprately\n\nexploratory_data_analysis.R  -- All exploratory data analysis approaches analyzing data sets to summarize their main characteristics, building plots, creating map of missing features e.t.c\n\nlogistic_regression_solution.R -- Feature clean up, split of data set into train test, cross validation & survival prediction using Logistic Regression\n\nrandom_forest_solution.R -- Feature clean up, split of data set into train test, cross validation & survival prediction using Random Forest\n\nsupport_vector_machine_solution.R -- Feature clean up, split of data set into train test, cross validation & survival prediction using Support Vector Machine\n\nfinal_baggged_solution.R -- Feature clean up, split of data set into train test, cross validation & survival prediction using all the above methods and then bagging the results.\n\n\n## Plots\n\nSome of the data visualizations are saved as pdfs in plots folder. Code for the same can be found in exploratory_data_analysis.R. The remaining visualizations like missmaps are printed in console itself.\n\nAge_Distribution.pdf\n\nAge_Vs_Survival.pdf\n\nEmbarked_Vs_Survival.pdf\n\nFare_Distribution.pdf\n\nPClass_Distribution_Bar.pdf\n\nsurvival_vs_sex.pdf\n\nsurvived_vs_died.pdf\n\n## Contributors\n\n@jayadeepj\n\n## License\n\n:)"
 },
 {
  "repo": "raphaelbrugier/kaggle-titanic",
  "language": "Scala",
  "readme_contents": "# About\n\nThis is an example of using the Spark-ml library to solve the [Titanic challenge on Kaggle](https://www.kaggle.com/c/titanic).\n\nI obtained a public score of `0.79426` on Kaggle with the current setup.\n\n## Using Zeppelin\n\nYou can start Zeppelin with docker-compose to explore the data with a `titanic` notebook already created.\n\n    docker-compose up -d\n\n## References:\n\n- http://www.ippon.tech/training/big-data-with-spark/\n- https://benfradet.github.io/blog/2015/12/16/Exploring-spark.ml-with-the-Titanic-Kaggle-competition\n- https://www.kaggle.com/mrisdal/titanic/exploring-survival-on-the-titanic\n"
 },
 {
  "repo": "atduskgreg/kaggle-titanic",
  "language": "Processing",
  "readme_contents": "## Kaggle Titanic in OpenCV for Processing\n\nI'm using the [Kaggle Titanic Challenge](http://www.kaggle.com/c/titanic-gettingStarted) as a platform for adding machine learning support to my [OpenCV for Processing](https://github.com/atduskgreg/opencv-processing) library.\n\nOpenCV supports:\n\n* Naive Bayes\n* Adaptive Neural Nets\n* K-Nearest Neighbor\n* AdaBoost\n* SVM\n* Random Decision Forests\n\nand a couple of other techniques.\n\nSo far, I've wrapped:\n\n* KNN\n* AdaBoost\n* Random Decision Forests\n* SVM\n\nwith an in-progress Naive Bayes as well (needs debugging).\n\nThanks to [Rune Madsen](http://github.com/runemadsen) for starting the RDF implementation.\n\n_I've also included an implementation with my libsvm-based [Processing-SVM](https://github.com/atduskgreg/processing-svm) library for comparison with OpenCV's svm implementation. For some reason libsvm produces signficantly better results with the same parameters._"
 },
 {
  "repo": "pbiecek/xaibot-titanic",
  "language": "JavaScript",
  "readme_contents": "# xaibot-titanic\nXAI chatbot explaining your chance of survival on Titanic.\nBot will be released soon.\n\n## Sources\nChatbot is developed in the platform Dialogflow.\nBoth NLU training files and code for the dialogue management might be found [here](bot).\nThe Machine Learning model is accessed via `plumber` API described below.\n\n## start plumber\n\nYou need to have files `titanic.R` and `explain_titanic_rf.rda` in the working folder.\n\nThen in order to execute the `plumber` api you need to call in the screen.\n\n```\nlibrary(\"plumber\")\npmodel <- plumb(\"titanic.R\")\npmodel$run(host = \"0.0.0.0\", port = 8787)\n```\n\n## Try it out\n\nFind a demo here: http://52.31.27.158:8787/__swagger__/\n"
 },
 {
  "repo": "ianozsvald/kaggle_titanic_ipythonnotebook_boilerplate",
  "language": null,
  "readme_contents": "kaggle_titanic_ipythonnotebook_boilerplate\n==========================================\n\nBoilerplate example using IPython Notebook to solve simplest (sex-field only) Titanic challenge for Kaggle (this will get you started wtih the Kaggle competition)\n\nThis is the Titanic challenge:\nhttp://www.kaggle.com/c/titanic-gettingStarted/\n\nFor a longer and far more in-depth analysis using Pandas see: \nhttp://nbviewer.ipython.org/urls/raw.github.com/agconti/kaggle-titanic/master/Titanic.ipynb\n\nYou *must download* the data from Kaggle to run this Notebook.\n\nThis Notebook replicates this example but using Python and an IPython Notebook:\nhttp://www.kaggle.com/c/titanic-gettingStarted/details/getting-started-with-excel\n\nRun the Notebook using:\n\n    $ ipython notebook --pylab=inline  # start the Notebook\n\nGo to the webbrowser, open the \"kaggle_titanic_sexcolumnonly_decision_tree\" example. Work through it, generate the csv output file at the end and upload to Kaggle. You can also view the generated decision tree in-line. The decision tree that you visualise will look like:\n\n![Decision tree example](clf_is_male_forreadme.png \"Simple decision tree using scikit-learn on the Sex column\")\n\nYou will probably need the dependencies too:\n\n    $ pip install -r requirements.txt  # uses pip to get all the Python bits\n\nPlease note that I can't help you setup your machine, you'll have to figure that out yourself. Feel free to send me a Pull Request if you have improvements to the documentation. This tutorial *cannot* say any more about how to solve the Kaggle competition as that would be against the rules.\n"
 },
 {
  "repo": "Msarang7/Machine_Learning_RMS_Titanic",
  "language": "Jupyter Notebook",
  "readme_contents": "# Machine_Learning_RMS_Titanic\nPython Program to predict the survival of passengers of RMS TITANIC VOYAGE that began on 15 April, 1912.\n\nThe program is based on two datasets. The first dataset is for training the Machine and other dataset is used to test how good \nour machine has been trained by training dataset.The test dataset is used to predict the results.\n"
 },
 {
  "repo": "ismailatma/Titanic-Analytics-Deployment",
  "language": "Jupyter Notebook",
  "readme_contents": "# Titanic-Analytics-Deployment\nPOST to this Endpoint URL http://ismailatma.pythonanywhere.com/api from postman or something else with this parameter {\"Pclass\" : 3, \"SibSp\" : 1,\"Parch\" : 0,\"Sex_female\" : 0,\"Sex_male\" : 1}\n"
 },
 {
  "repo": "SuperChenSSS/KaggleForTitanic",
  "language": "Jupyter Notebook",
  "readme_contents": "# Kaggle For Titanic\n## Kaggle\u5165\u95e8\u7ade\u8d5b\n## \u6d89\u53ca\u7b97\u6cd5\uff1a\n- Logistic Regression\n- SVM\n- RandomForest\n- KNeighbors\n\n## \u8be5\u9879\u76ee\u4f7f\u7528jupyter notebook+python3.5\n## \u9879\u76ee\u7ed3\u6784\uff1a\n- input/: \u5b58\u653eData\u6570\u636e\uff0c\u4e5f\u662f\u6e90\u6570\u636e\n- output/: \u5b58\u653e\u7ecf\u8fc7\u5904\u7406\u751f\u6210\u6700\u540e\u7684\u63d0\u4ea4\u6587\u4ef6\uff0c\u7528\u4e8e\u63d0\u4ea4\u68c0\u9a8c\u6b63\u786e\u7387\n- Titanic.ipynb + TitanicProjects: \u5b58\u653e\u5b8c\u6574\u7248\u9879\u76ee\u4ee3\u7801+\u6f14\u793a\u3002TitanProjects\u66f4\u52a0\u7cbe\u7b80\uff0c\u9002\u5408\u5b66\u4e60\uff08\u6b63\u786e\u7387~78%\uff09\n- TitanicZN: \u7ed3\u5408[\u5bd2\u5c0f\u9633\u7684\u535a\u5ba2](http://blog.csdn.net/han_xiaoyang/article/details/49797143)\uff0c\u5bf9\u6587\u7ae0\u4e2d\u7684\u7b97\u6cd5\u518d\u5b9e\u73b0\uff0c\u76ee\u524d\u6b63\u5728\u66f4\u65b0\u4e2d\u3002\u9884\u8ba1TitanicZN\u7684\u51c6\u786e\u7387\u4f1a\u66f4\u9ad8\n\n## \u5982\u4f55\u4f7f\u7528\u9879\u76ee\u6587\u4ef6:\n- \u4f9d\u8d56\u73af\u5883:  python3, pip3, pandas, numpy, matplotlib\n- \u5b89\u88c5jupyter notebook: pip3 install jupyter\n- \u542f\u52a8jupyter notebook: jupyter notebook\n- \u6267\u884c\uff0cEnjoy!\n\n## \u76ee\u524d\u7684\u6700\u9ad8\u6b63\u786e\u7387:79.904%"
 },
 {
  "repo": "nitikasatya16/K-fold---Titanic-Train-",
  "language": null,
  "readme_contents": "# K-fold---Titanic-Train-"
 },
 {
  "repo": "shashwat23/Titanic-Survival-Prediction",
  "language": "Python",
  "readme_contents": "# Titanic-Survival-Prediction\nThis project highlights my approach to the introductory machine learning competition on Kaggle website- Titanic: Machine Learning from Disaster.\n\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nThis project analyses which people were likely to survive. In particular, tools of machine learning have been used to predict which passengers survived the tragedy.\n"
 },
 {
  "repo": "jawad3838/Titanic-Survival-Prediction-Using-R",
  "language": "R",
  "readme_contents": "# Titanic-Survival-Prediction\nPredicting the survival of passengers on RMS Titanic using information about the passengers.\n\n## Pre-requisites\nYou need the following libraries in RStudio or any IDE of your choice:\n* caret\n* rpart\n* rattle\n* RColorBrewer\n* e1071\n* class\n\n## About the Dataset\nThe titanic survival dataset that is available on Kaggle has the following attributes:\n\n* Survival: 0 = No, 1 = Yes \n* pclass (Ticket class):  1 = 1st, 2 = 2nd, 3 = 3rd \n* Sex:  male and female\n* Age: In years \n* sibsp: # of siblings / spouses aboard the Titanic \n* parch: # of parents / children aboard the Titanic \n* ticket: Ticket number \n* fare: Passenger fare \n* cabin: Cabin number \n* embarked (Port of Embarkation): C = Cherbourg, Q = Queenstown, S = Southampton\n\nThe dataset is already split into training and testing sets. The Survival attribute is not present in the test test instead it is present in another file which is labelled gender_submission.\n\n## Different Models Used\n3 different models were used to predict the survival of passengers on the RMS Titanic. The performance of each of these models is shown below via confusion matrix\n\n* Decision Tree\n\nThe generated tree using \"fancyRpartPlot\" is shown below:\n\n![](https://github.com/jawad3838/Titanic-Survival-Prediction-Using-R/blob/master/screenshots/DecisionTree.PNG)\n\n![Confusion Matrix](https://github.com/jawad3838/Titanic-Survival-Prediction-Using-R/blob/master/screenshots/CM_DecisionTree.PNG)\n\n\n* Naive Bayes\n\n![Confusion Matrix](https://github.com/jawad3838/Titanic-Survival-Prediction-Using-R/blob/master/screenshots/CM_NaiveBayes.PNG)\n\n\n* Knn Classifier\n\n![Confusion Matrix](https://github.com/jawad3838/Titanic-Survival-Prediction-Using-R/blob/master/screenshots/CM_knn.PNG)\n"
 },
 {
  "repo": "deadskull7/Titanic-survival-analysis-and-predictions",
  "language": "Jupyter Notebook",
  "readme_contents": "# Titanic-survival-analysis-and-predictions\n\n#### Data being provided of people travelling on titanic , analysis done using matplotlib and seaborn libraries along with pandas manipulation , finally a particular machine learning model after comparison is trained to obtain maximum accuracy on the data which is formerly cleaned and converted to be trained and at last the survival of a person is predicted based on the trained model .\n   \n"
 },
 {
  "repo": "mnassrib/Titanic-logistic-regression-with-python",
  "language": "Jupyter Notebook",
  "readme_contents": "# Titanic-logistic-regression-with-python\nThis kernel was inspired in part by the work of SarahG's analysis that I thank very much for the quality of her analysis. This work represents a deeper analysis by playing on several parameters while using only logistic regression estimator. \n"
 },
 {
  "repo": "nottombrown/IceLightsAndMusic",
  "language": null,
  "readme_contents": "Titanic's End - Ice, Lights, and Music\n=================\n\nProject Repository for Titanic's End Electronics\n\n- [Facebook Group](https://www.facebook.com/groups/1434882210102871/)\n- [Crowdtilt](https://www.crowdtilt.com/campaigns/titanics-end)\n- [Issues](https://github.com/nottombrown/IceLightsAndMusic/issues) (get involved!)\n\n![Sweet Rendering](https://cloud.githubusercontent.com/assets/306655/3239022/981f72f2-f102-11e3-9e56-e604742f7924.jpg)\n\n# MVP\n\nBurning man starts on **Aug 24th**. When we arrive, we must have the following:\n\n- [ ] LEDs along the exterior of the frame\n- [ ] LEDs throughout the interior\n- [ ] A sound system (with small speakers in the interior)\n- [ ] A generator that provides reliable power\n\nWe'd like to keep the entire electronics budget below $5k\n\n# Roadmap\n\nhttps://www.google.com/calendar/embed?src=q6iqtt235h5isporsgrl2aepcs%40group.calendar.google.com&ctz=America/Los_Angeles\n\n# Wiring\n\n![titanics_end_diagram](https://cloud.githubusercontent.com/assets/306655/3368329/eb7fe722-fb72-11e3-9aed-4a7b16cbf9b0.png)\n\nIf you want to contribute, feel free to edit the [omnigraffle file](https://www.dropbox.com/s/9u4l34wucbv1rlj/titanics_end_diagram.graffle)\n\n# LEDs\n\nQuick math for the size of the car\n\n- Height ~3.5m\n- Length ~ 6m\n- Width ~ 2.5m\n\nThis gives us a total perimiter of 17m and a total area of 59.5m^2\n\n## LED strips\nI've had several recommendation for [waterproof WS2812B strips](http://www.aliexpress.com/item/BLACK-PCB-5m-DC5V-WS2812B-led-pixel-srip-IP68-30pcs-WS2812B-M-with-30pixels-reverse-protection/926778326.html)\n\nEach strip has the following specs:\n\n- $30.00 per strip (plus shipping)\n- 150 LEDs per strip\n- 5m per strip\n- 120 degree emitting angle\n- 36 watts per strip\n- 2550 lumens (17 lumens/LED)\n\nIf we get 70 of them and cut them into 50 LED segments, some napkin math would give us:\n\n- 210 strip segments (each 166 cm long and drawing 12 watts)\n- 350 total meters of strips\n- 2520 watts\n- 10,500 total LEDs\n- 178,500 lumens (As bright as about 300 x 60W halogen bulbs)\n- ~$2.5k\n\n## LED Controllers\n\nFadecandy: https://github.com/scanlime/fadecandy\n\nPurchasable on [Adafruit](http://www.adafruit.com/products/1689). Each one has the following specs:\n\n- Can control up to 8 strip segments (each up to 64 pixels long, ours are 50)\n- $25 (plus shipping)\n\nIf we get 26 of them it means:\n\n- We can control 208 segments\n- We have to solder 208 segments\n- It will cost $720\n\n# Nodes\n\nSeems like there are four main LED sections that need power and control\n\n- Left side\n- Right side\n- Front & Interior\n- Back & DJ booth\n\nIn each of those sections, we should have a *node*. The node will contain a power supply and a USB hub, and should be protected so that it does not get damaged.\n\n## USB hubs\n\nWe want a powered hub with at least 8 USB ports.\n\n## Power Supplies\n\nWe need 5V power for each of the LED strips. Micah recommends a [200W Genssi](http://www.ebay.com/itm/GENSSI-5V-40A-200W-Regulated-Switching-Power-Supply-FAST-FREE-SHIPPING-USA-/171123409781).\n\n# Brain\n\nWe should bring an old macbook, which will let us debug things on the fly. If it breaks, it will be replaceable by other macbooks. Uses 85 watts\n\n# Sound System\n\nKan Bros have a large speaker system, I'd estimate it at 2000W\n\n# Generator\n\nI'd like to use inverter generators because they're quiet and efficient. We already have one [3100W Champion](http://www.amazon.com/gp/product/B00BBDCE1S/ref=oh_details_o06_s00_i00?ie=UTF8&psc=1), I think we should get a second. One will be dedicated lights, and the other dedicated audio.\n"
 },
 {
  "repo": "Mineria/Titanic",
  "language": "Python",
  "readme_contents": "error: no README"
 },
 {
  "repo": "aanchan/Titanic-MTLDATA-Theano",
  "language": "Python",
  "readme_contents": "Titanic-MTLDATA-Theano\n======================\n\nLogistic Regression and a single layer neural network for the Kaggle Titanic Competition for MTLData - Oct.-22,2014\n\nThis IPython notebook is a continuation of [work (IPython-NB Link)] (http://nbviewer.ipython.org/github/aanchan/Titanic-MTLDATA/blob/master/TitanicPythonIntro.ipynb) from our previous meetup for the Kaggle Titanic Competition. The solution presented there was using Pandas+SciKit Learn. [Click here](https://github.com/aanchan/Titanic-MTLDATA) for the Github page for that meetup.\n\nContent\n---\n[Data Preparation](http://nbviewer.ipython.org/github/aanchan/Titanic-MTLDATA-Theano/blob/master/Data-Cleaning.ipynb) - A lot of this follows from work during our previous meetup, i.e : Data cleaning using Pandas.\n\n[Logistic Regression using Theano](http://nbviewer.ipython.org/github/aanchan/Titanic-MTLDATA-Theano/blob/master/Theano-Logistic-Regression.ipynb)\n\n[Neural Network Training using Theano](http://nbviewer.ipython.org/github/aanchan/Titanic-MTLDATA-Theano/blob/master/Theano-MLP-Titanic-2.ipynb)\n\nInstalling tools required for code in this tutorial run on your system.\n----\n\t1. Python\n\t2. IPython (Optional since you could run the Python commands from the IPython \n\t   \t   notebook on your native Python interpreter)\n\t3. Numpy\n\t4. Scipy\n\t5. Pandas\n\t6. Theano \n\nInstallation methods for a scientific Python setup vary depending on the Operating System. [Here](http://blog.yhathq.com/posts/setting-up-scientific-python.html) is a great link on completing a setup in Python for scientific purposes. \n\nInstallation instructions for Theano are available from the [Theano website](http://deeplearning.net/software/theano/install.html)\n\nThe Kaggle Titanic Challenge\n----\nRead about it [here](https://www.kaggle.com/c/titanic-gettingStarted)\n\nIntroduction to Python\n----\n[Course from Coursera](https://www.coursera.org/course/interactivepython). This does not require one to download and install Python. They have a version for the course that runs off the browser interactively.\n\n[The best intro I think, from Python Docs](https://docs.python.org/2/tutorial/introduction.html)\n\nIntroduction to the Numpy module in Python\n----\n[The Tentative Numpy Tutorial](http://wiki.scipy.org/Tentative_NumPy_Tutorial) is a good place to start.\n\nIntroduction to Pandas\n---\nThe [Python Pandas Cookbook Lecture Series](http://www.youtube.com/playlist?list=PLyBBc46Y6aAz54aOUgKXXyTcEmpMisAq3) on Youtube by Alfred Essa is a good place to start. Specifically to load our Titanic data set Alfred Essa talks about it [here](https://www.youtube.com/watch?v=lhkchS9gSYk#t=545) in Lesson 1.2.\n\nIntroduction to Theano\n---\nWhile the great tutorial webpages appear on the Theano website, a companion IPython notebook with similar content, and especially a great intro code to Neural Networks is available [here](http://nbviewer.ipython.org/github/craffel/theano-tutorial/blob/master/Theano%20Tutorial.ipynb). \n\nIntroduction to Logistic Regression\n----\n[A Simple Explanation from Duke Medicine](https://www.youtube.com/watch?v=_Po-xZJflPM)\n\n[Logistic Regression for Classification](http://youtu.be/nMcxOiVj7oE)\n\nIntroduction to Neural Networks\n---\n[Logistic and Softmax Regression](http://ufldl.stanford.edu/wiki/index.php/Softmax_Regression) by Prof. Andrew Ng at Stanford.\n\nA clear (and correct) introduction to [Neural Networks](http://ufldl.stanford.edu/wiki/index.php/Neural_Networks) by Prof. Andrew Ng at Stanford.\n\n\n"
 },
 {
  "repo": "MSusik/titanic",
  "language": "Jupyter Notebook",
  "readme_contents": "error: no README"
 },
 {
  "repo": "YingLaiLin/Titanic_Kaggle",
  "language": "Python",
  "readme_contents": "# Titanic_Kaggle\n - \u5173\u4e8e [kaggle](https://www.kaggle.com/c/titanic) \u7684\u5165\u95e8\u6bd4\u8d5b ---- Titanic \u7684\u5b66\u4e60\u8fc7\u7a0b\u7684\u8bb0\u5f55.\n## \u7f16\u7a0b\u5de5\u5177\n- Python 3.6\n### \u6a21\u578b\u8bbe\u8ba1\u4ee5\u53ca\u53c2\u6570\u8c03\u6574\n- [LightGBM](https://github.com/Microsoft/LightGBM)  \n- [scikit-learn](http://scikit-learn.org/stable/index.html)\n### \u6570\u636e\u5206\u6790\n- [matplotlib](https://matplotlib.org/tutorials/introductory/sample_plots.html#sphx-glr-tutorials-introductory-sample-plots-py)\n- [pandas](http://pandas.pydata.org/pandas-docs/stable/genindex.html)\n- [numpy](https://docs.scipy.org/doc/numpy-dev/reference/)\n## \u8d44\u6599\n- [**How am I doing with my score?**\n](https://www.kaggle.com/pliptor/how-am-i-doing-with-my-score)\n- [Predicting Titanic Survivors](https://www.kaggle.com/bismillahkani/predicting-titanic-survivors)\n- [Titanic Data Science Solutions\n](https://www.kaggle.com/startupsci/titanic-data-science-solutions)\n- [How a Data Scientist Beat the Odds](https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy)\n- [Titanic Top 4% with ensemble modeling\n](https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling)\n- [**Applying LightGBM to Titanic dataset**\n](https://www.kaggle.com/shep312/applying-lightgbm-to-titanic-dataset)"
 },
 {
  "repo": "prasants/pyds",
  "language": "Jupyter Notebook",
  "readme_contents": "# Python for Data Science\n\nBasic python skills needed for an Introduction to Data Science, and commencing the process of building a Data Science portfolio.\n\n## Getting Started\n\nSome of the notebooks require external packages like NumPy, Pandas, Matplotlib, Seaborn and Vincent.\n\n### Prerequisites\n\n\n```\nPython 3.x\n```\n\n## Built With\n\n* [Python 3.5.2](https://www.python.org/downloads/release/python-350/)\n\n* [Anaconda 4.2.0](https://www.continuum.io/downloads)\n\n## Versioning\n\n* v0.0.1: Data Exploration and Visualisation\n\n## Authors\n\n* **Prasant Sudhakaran** - [prasants](https://github.com/prasants)\n\n\n## License\n\nThis project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details\n\n## Acknowledgments\n\n* Many fantastic posts on [Stackoverflow](http://stackoverflow.com), linked within the Notebooks.\n"
 },
 {
  "repo": "Shreyas3108/Titanic-EDA-and-Survival-prediction",
  "language": "Jupyter Notebook",
  "readme_contents": "# Titanic-EDA-and-Survival-prediction"
 },
 {
  "repo": "longhowlam/titanicTree",
  "language": "HTML",
  "readme_contents": "# titanicTree\nInteractive Decision tree with Microsoft R\n\nUsing Microsoft R you can create a decision tree and interactively view it to gain insights.\n\n[Interactive decision tree survival Titanic](https://rawgit.com/longhowlam/titanicTree/master/tree.html)\n\nCheers,\nLonghow\n"
 },
 {
  "repo": "lydiajessup/titanic-example",
  "language": "JavaScript",
  "readme_contents": "# titanic-example\n"
 },
 {
  "repo": "tensorinfinity/Titanic",
  "language": "Jupyter Notebook",
  "readme_contents": "\u8fd9\u662f\u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u7684\u6559\u7a0b: a Tutorial to Machine Learning\n\n\u8fd9\u662f [a series of projects]() \u4e2d\u7684\u7b2c\u4e00\u4e2a\u9879\u76ee\uff0c\u4ece\u8fd9\u4e2a\u9879\u76ee\u4e2d\u6211\u4eec\u4f1a\u5b66\u4e60\u5230\u5982\u4f55\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u7ecf\u5178\u7b97\u6cd5\u6765\u5b9e\u73b0\u6cf0\u5766\u5c3c\u514b\u53f7\u5e78\u5b58\u8005\u5206\u6790\u548c\u9884\u6d4b\u3002\n\n\u9700\u8981\u5927\u5bb6\u4e86\u89e3\u673a\u5668\u5b66\u4e60\u7684\u57fa\u672c\u77e5\u8bc6\uff0c\u540c\u65f6\u8981\u638c\u63e1\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u7ecf\u5178\u6a21\u578b\uff0c\u6bd4\u5982logistic regression\uff0csvm \u7b49\u3002\n\n\u9879\u76ee\u4f7f\u7528 `sklearn` \u548c `python3.7`\n\n# \u76ee\u5f55\n\n[**Objective**](https://github.com/tensorinfinity/Titanic#objective)\n\n[**Concepts**](https://github.com/tensorinfinity/Titanic#concepts)\n\n[**Overview**](https://github.com/tensorinfinity/Titanic#overview)\n\n[**Implementation**](https://github.com/tensorinfinity/Titanic#implementation)\n\n[**Training**](https://github.com/tensorinfinity/Titanic#training)\n\n[**Evaluation**](https://github.com/tensorinfinity/Titanic#evaluation)\n\n\n# Objective\n\u6211\u4eec\u9700\u8981\u8bad\u7ec3\u4e00\u4e2a\u673a\u5668\u5b66\u4e60\u7684\u7b97\u6cd5\u6765\u5224\u65ad\u5728\u6cf0\u5766\u5c3c\u514b\u53f7\u4e2d\u54ea\u4e9b\u4eba\u5b58\u6d3b\u7684\u6982\u7387\u66f4\u9ad8\uff0c\u54ea\u4e9b\u4eba\u5b58\u6d3b\u7684\u6982\u7387\u66f4\u4f4e\u3002\n\n\u5728\u8fd9\u4e2a\u9879\u76ee\u4e2d\uff0c\u6211\u4eec\u4e3b\u8981\u4f7f\u7528\u6570\u636e\u53ef\u89c6\u5316\u5206\u6790\u4ee5\u53ca\u7ecf\u5178\u7684\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u6765\u8fdb\u884c\u7ec3\u4e60\u3002\n\n# Concepts\n\n- **\u6570\u636e\u53ef\u89c6\u5316** \u4e00\u822c\u6765\u8bb2\u4f7f\u7528 `matplotlib` \u548c `seaborn` \u5c31\u80fd\u591f\u63d0\u4f9b\u6211\u4eec\u57fa\u672c\u7684\u7ed8\u56fe\u9700\u6c42\uff0c\u6bd4\u8f83\u5e38\u7528\u7684\u56fe\u8868\u6709\uff1a\n    - \u67e5\u770b\u76ee\u6807\u5206\u5e03\u7684\u56fe\u8868\uff0c\u5f53\u5206\u5e03\u4e0d\u5e73\u8861\u7684\u65f6\u5019\uff0c\u53ef\u4ee5\u901a\u8fc7\u8fd9\u4e2a\u56fe\u6807\u89c2\u5bdf\u5230\n    - \u6563\u70b9\u56fe\uff0c\u53ef\u4ee5\u89c2\u5bdf\u5230\u7279\u5f81\u7684\u5206\u5e03\u8d8b\u52bf\u4ee5\u53ca\u662f\u5426\u6709outlier\u7684\u5b58\u5728\n    - \u591a\u4e2a\u53d8\u91cf\u4e4b\u95f4\u7684\u76f8\u5173\u56fe\u56fe\u7247\uff0c\u53ef\u4ee5\u770b\u51fa\u4e00\u5b9a\u7684\u4f9d\u8d56\u5173\u7cfb\n\n- **\u6570\u636e\u9884\u5904\u7406** \u5bf9\u4e8e\u5f97\u5230\u7684\u6570\u636e\uff0c\u5728\u8fdb\u884c\u6784\u9020\u7279\u5f81\u4e4b\u524d\uff0c\u6211\u4eec\u9700\u8981\u5bf9\u6570\u636e\u8fdb\u884c\u9884\u5904\u7406\uff0c\u56e0\u4e3a\u6570\u636e\u5b58\u5728\u7740\u7f3a\u5931\u7b49\u95ee\u9898\uff0c\u4e00\u822c\u7684\u9884\u5904\u7406\u6b65\u9aa4\u5982\u4e0b\uff1a\n    - \u5408\u5e76\u4e0d\u540c\u6587\u4ef6\u5939\u4e2d\u7684\u7279\u5f81\n    - \u5904\u7406\u786e\u5b9e\u6570\u636e\n    - \u5904\u7406\u79bb\u7fa4\u70b9\n    - \u5c06\u4e00\u4e9b\u7279\u5f81\u8f6c\u6362\u6210 Categorical Variable\n\n\u8fd9\u90e8\u5206\u7684\u5904\u7406\u7b56\u7565\u4f9d\u8d56\u4e8e\u6570\u636e\u53ef\u89c6\u5316\u4e2d\u7684\u5206\u6790\u548c\u7ed3\u679c\u3002\n\n- **\u4ea4\u53c9\u9a8c\u8bc1** \u8fd9\u5bf9\u4e8e\u6a21\u578b\u7684\u9009\u62e9\u662f\u4e00\u4e2a\u975e\u5e38\u91cd\u8981\u7684\u6b65\u9aa4\uff0c\u80fd\u591f\u77e5\u9053\u6a21\u578b\u662f\u5426\u8fc7\u62df\u5408\u4e86\uff0c\u662f\u4e0d\u662f\u80fd\u591f\u771f\u6b63generalize\u5230\u6d4b\u8bd5\u96c6\u4e0a\u3002\u5f53\u6570\u636e\u7684\u5206\u5e03\u968f\u673a\u5747\u8861\u7684\u60c5\u51b5\u4e0b\uff0c**5-Fold CV** \u5c31\u5df2\u7ecf\u8db3\u591f\u4e86\uff0c\u5982\u679c\u4e0d\u653e\u5fc3\uff0c\u53ef\u4ee5\u4f7f\u7528 **10-Fold**\u3002\n\n<div align=center>\n<img src='assets/cross.png' width='400'>\n</div>\n\n- **\u683c\u70b9\u641c\u7d22** \u6709\u7684\u6a21\u578b\u6709\u5f88\u591a\u8d85\u53c2\u6570\u9700\u8981\u8c03\u6574\uff0c\u6bd4\u5982\u51b3\u7b56\u6811\u4e2d\u6211\u4eec\u9700\u8981\u8bbe\u7f6e\u6700\u5927\u6df1\u5ea6\uff0c\u5206\u88c2\u6240\u9700\u6700\u5c0f\u8282\u70b9\u6570\u7b49\u7b49\uff0c\u8fd9\u4e9b\u53c2\u6570\u7684\u4e0d\u540c\u7ec4\u5408\u975e\u5e38\u7684\u591a\uff0c\u4f7f\u7528\u683c\u70b9\u641c\u7d22\u5c31\u662f\u6211\u4eec\u5c06\u6240\u6709\u53ef\u80fd\u7684\u53c2\u6570\u90fd\u5217\u51fa\u6765\uff0c\u7136\u540e\u7a77\u4e3e\u6240\u6709\u7684\u7ec4\u5408\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\uff0c\u5f97\u5230\u6027\u80fd\u6700\u597d\u7684\u53c2\u6570\u7ec4\u5408\u3002\n\n<div align=center>\n<img src='assets/grid.png' width='200'>\n</div>\n\n- **Ensemble** \u6a21\u578b\u878d\u5408\u662f\u673a\u5668\u5b66\u4e60\u91cc\u9762\u975e\u5e38\u5e38\u89c1\u7684\u6280\u672f\uff0c\u662f\u5c06\u591a\u4e2a\u4e0d\u540c\u7684\u5f31\u5206\u7c7b\u5668\u96c6\u6210\u5f97\u5230\u4e00\u4e2a\u5f3a\u5206\u7c7b\u5668\uff0c\u53ef\u4ee5\u540c\u65f6\u964d\u4f4e\u6a21\u578b\u7684 Bias \u548c Variance\uff0c\u5728\u63d0\u9ad8\u6a21\u578b\u8868\u73b0\u7684\u540c\u65f6\u53c8\u80fd\u964d\u4f4e\u8fc7\u62df\u5408\u7684\u98ce\u9669\u3002Ensemble\u7684\u6548\u679c\u8981\u597d\uff0c\u4e0d\u540c\u7684\u5f31\u5206\u7c7b\u5668\u4e4b\u95f4\u76f8\u5173\u6027\u5c3d\u53ef\u80fd\u5c0f\uff0c\u8fd9\u6837\u80fd\u591f\u589e\u52a0\u96c6\u6210\u6a21\u578b\u7684\u591a\u6837\u6027\uff0c\u540c\u65f6\u5f31\u5206\u7c7b\u5668\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u4e0d\u80fd\u592a\u5927\uff0c\u5426\u5219\u4f1a\u62c9\u4f4e\u96c6\u6210\u6a21\u578b\u7684\u6027\u80fd\u3002\u5e38\u89c1\u7684 Ensemble \u65b9\u6cd5\u6709\u4e0b\u9762\u51e0\u79cd\uff1a\n    - Bagging: \u5bf9\u8bad\u7ec3\u96c6\u8fdb\u884c\u968f\u673a\u91c7\u6837\u5f97\u5230\u4e0d\u540c\u7684\u5b50\u96c6\u6765\u8bad\u7ec3\u5f31\u5206\u7c7b\u5668\uff0c\u7136\u540e\u5bf9\u5f31\u5206\u7c7b\u5668\u8fdb\u884c\u6295\u7968\u3002\n    - Boosting: \u8fed\u4ee3\u8bad\u7ec3\u5f31\u5206\u7c7b\u5668\uff0c\u6839\u636e\u4e0a\u4e00\u6b21\u8fed\u4ee3\u4e2d\u9884\u6d4b\u9519\u8bef\u7684\u60c5\u51b5\u8fdb\u884c\u4fee\u6b63\u8bad\u7ec3\u6837\u672c\u7684\u6743\u91cd\u3002\n    - Blending: \u7528\u4e0d\u76f8\u5173\u7684\u8bad\u7ec3\u96c6\u8fdb\u884c\u4e0d\u540c\u5f31\u5206\u7c7b\u5668\u7684\u8bad\u7ec3\uff0c\u6700\u540e\u5c06\u5b83\u4eec\u7684\u8f93\u51fa\u53d6\u52a0\u6743\u5e73\u5747\u5f97\u5230\u6700\u540e\u7684\u9884\u6d4b\u7ed3\u679c\u3002\n    - Stacking\n\n<div align=center>\n<img src='assets/stacking.jpg' width='800'>\n</div>\n\n\n# Overview\n\u5728\u8fd9\u4e2a\u90e8\u5206\uff0c\u6211\u4eec\u5c55\u793a\u4e00\u4e0b\u6574\u4f53\u7684\u7ed3\u6784\uff0c\u5982\u679c\u4f60\u5f88\u719f\u6089\u8fd9\u4e2a\u90e8\u5206\uff0c\u4f60\u53ef\u4ee5\u76f4\u63a5\u8df3\u5230[implementation](https://github.com/tensorinfinity/Titanic#implementation)\n\n## \u6570\u636e\u53ef\u89c6\u5316\u5206\u6790\n\u9996\u5148\uff0c\u6211\u4eec\u5bf9\u6570\u636e\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u5206\u6790\u5176\u4e2d\u7684\u5173\u7cfb\u3002\n\n\u9996\u5148\u6211\u4eec\u5bf9\u6570\u636e\u4e2d\u6240\u6709\u7279\u5f81\u8fdb\u884c\u7f3a\u5931\u503c\u7684\u53ef\u89c6\u5316\uff0c\u53ef\u4ee5\u53d1\u73b0\u5e74\u9f84\u548c\u8239\u8231\u53f7\u7801\u786e\u5b9e\u6bd4\u8f83\u591a\uff0c\u5176\u4e2d\u5e74\u9f84\u5927\u6982\u7f3a\u5c11\u4e94\u5206\u4e4b\u4e00\uff0c\u800c\u8239\u8231\u53f7\u7801\u5927\u6982\u53ea\u898110%\u4e0d\u5230\uff0c\u6240\u4ee5\u540e\u9762\u6211\u4eec\u4f1a\u5c3d\u91cf\u53bb\u586b\u8865\u5e74\u9f84\u7684\u7279\u5f81\uff0c\u540c\u65f6\u53bb\u6389\u8239\u8231\u53f7\u7801\u8fd9\u4e00\u4e2a\u7279\u5f81\u3002\n\n<div align=center>\n<img src='assets/missing.png' width='400'>\n</div>\n\n\u4e0b\u9762\u6211\u4eec\u5206\u6790\u5b58\u6d3b\u7684\u4eba\u548c\u9047\u96be\u7684\u4eba\u7684\u6bd4\u4f8b\uff0c\u53d1\u73b0\u5b58\u6d3b\u7684\u4eba\u5927\u6982\u53ea\u6709\u9047\u96be\u7684\u4eba\u7684\u4e8c\u5206\u4e4b\u4e00\uff0c\u8fd9\u91cc\u53ef\u4ee5\u8bf4\u660e\u8bad\u7ec3\u96c6\u6709\u4e00\u70b9\u7c7b\u522b\u4e0d\u5747\u8861\uff0c\u4e24\u4e2a\u7c7b\u522b\u7684\u6570\u636e\u5e76\u4e0d\u662f\u4e00\u6837\u591a\u7684\u3002\n\n<div align=center>\n<img src='assets/surv.png' width='400'>\n</div>\n\n\n\u63a5\u7740\u6211\u4eec\u8003\u8651\u6027\u522b\u548c\u5b58\u6d3b\u5e94\u8be5\u5b58\u5728\u4e00\u5b9a\u7684\u5173\u7cfb\uff0c\u5e94\u8be5\u5973\u6027\u5e78\u5b58\u7684\u4eba\u6570\u66f4\u591a\uff0c\u901a\u8fc7\u4e0b\u9762\u7684\u53ef\u89c6\u5316\u6211\u4eec\u53d1\u73b0\u4e86\u786e\u5b9e\u662f\u8fd9\u6837\u7684\uff0c\u5728\u9047\u96be\u7684\u4eba\u6570\u4e2d\uff0c\u5927\u591a\u6570\u662f\u7537\u6027\uff0c\u800c\u5e78\u5b58\u7684\u4eba\u6570\u4e2d\uff0c\u5927\u591a\u662f\u5973\u6027\u3002\n\n<div align=center>\n<img src='assets/surv_sex.png' width='400'>\n</div>\n\n\u63a5\u7740\u6211\u4eec\u8003\u8651\u8239\u8231\u7b49\u7ea7\u548c\u5e78\u5b58\u8005\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u901a\u8fc7\u4e0b\u9762\u7684\u56fe\u6211\u4eec\u53d1\u73b0\u5e78\u5b58\u8005\u4e2d\u6bcf\u79cd\u8239\u8231\u7b49\u7ea7\u5927\u591a\u4e00\u81f4\uff0c\u800c\u9047\u96be\u7684\u4eba\u4e2d\uff0c\u5927\u591a\u6570\u662f\u4e09\u7b49\u8239\u8231\u3002\n\n<div align=center>\n<img src='assets/surv_pclass.png' width='400'>\n</div>\n\n\u7136\u540e\u6211\u4eec\u67e5\u770b\u4e00\u4e0b\u5e74\u9f84\u5206\u5e03\uff0c\u53d1\u73b0\u5927\u591a\u6570\u4eba\u96c6\u4e2d\u572820-40\u5c81\u4e4b\u95f4\u3002\n\n<div align=center>\n<img src='assets/age.png' width='400'>\n</div>\n\n\u63a5\u7740\u6211\u4eec\u67e5\u770b\u4e0a\u8239\u7684\u4eba\u4e2d\u7684\u4eb2\u5c5e\u5173\u7cfb\uff0c\u53d1\u73b0\u5927\u591a\u6570\u4eba\u90fd\u662f0\u4e2a\u4eb2\u5c5e\u6216\u8005\u4e00\u4e2a\u4eb2\u5c5e\uff0c\u8bf4\u660e\u5927\u591a\u6570\u90fd\u662f\u5355\u72ec\u51fa\u884c\u3002\n\n<div align=center>\n<img src='assets/sib.png' width='400'>\n</div>\n\n\u63a5\u7740\u6211\u4eec\u67e5\u770b\u4e00\u4e0b\u7968\u4ef7\u5206\u5e03\uff0c\u53ef\u4ee5\u770b\u5230\u5927\u591a\u6570\u4eba\u7684\u7968\u4ef7\u6bd4\u8f83\u4fbf\u5b9c\u3002\n\n<div align=center>\n<img src='assets/fare.png' width='400'>\n</div>\n\n\n## \u6570\u636e\u9884\u5904\u7406\n\u9996\u5148\u6211\u4eec\u9700\u8981\u5bf9\u6570\u636e\u4e2d\u7684\u7f3a\u5931\u503c\u548c\u4e00\u4e9b\u975e\u6570\u503c\u7279\u5f81\u8fdb\u884c\u9884\u5904\u7406\u3002\u5bf9\u4e8e\u7f3a\u5931\u503c\u6211\u4eec\u53ef\u4ee5\u8003\u8651\u4e24\u7c7b\u5904\u7406\u65b9\u6cd5\uff0c\u4e00\u7c7b\u662f\u4f7f\u7528\u7279\u5f81\u7684\u5747\u503c\u6216\u8005\u662f\u9891\u6570\u6700\u5927\u7684\u7279\u5f81\u53bb\u586b\u5145\u7f3a\u5931\u503c\uff0c\u7b2c\u4e8c\u7c7b\u662f\u5b9a\u4e49\u4e00\u4e2a\u51fd\u6570\uff0c\u6839\u636e\u67d0\u4e9b\u7279\u6027\u6765\u786e\u5b9a\u7f3a\u5931\u503c\u3002\u5bf9\u4e8e\u975e\u6570\u503c\u7279\u5f81\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u5176\u8f6c\u6362\u4e3acategorical\u7279\u5f81\uff0c\u6bd4\u5982\u6027\u522b\uff0c\u5c31\u53ef\u4ee5\u53d8\u6210\u4e00\u4e2a\u4e8c\u503c\u7279\u5f81\uff0c\u7537\u6027\u662f0\uff0c\u5973\u6027\u662f1\u3002\u9664\u6b64\u4e4b\u5916\uff0c\u8fd8\u9700\u8981\u6254\u6389\u4e00\u4e9b\u65e0\u6548\u7684\u7279\u5f81\uff0c\u6bd4\u5982\u5927\u591a\u6570\u7684\u6570\u636e\u90fd\u7f3a\u5c11\u7684\u7279\u5f81\u3002\n\n## \u6a21\u578b\u8bad\u7ec3\u548c\u9884\u6d4b\n\u6211\u4eec\u91c7\u7528`sklearn`\u91cc\u9762\u5b9e\u73b0\u597d\u7684\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\uff0c\u51e0\u4e4e\u6240\u6709\u7ecf\u5178\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u91cc\u9762\u90fd\u6709\u5b9e\u73b0\uff0c\u6bd4\u5982svm\uff0crandom forest \u7b49\u7b49\uff0c\u7136\u540e\u7528\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u5bf9\u6d4b\u8bd5\u96c6\u8fdb\u884c\u9884\u6d4b\u3002\u5728\u6a21\u578b\u8bad\u7ec3\u7684\u65f6\u5019\u53ef\u4ee5\u91c7\u7528\u4ea4\u53c9\u9a8c\u8bc1\u548c\u683c\u70b9\u641c\u7d22\u7b49\u65b9\u6cd5\uff0c\u540c\u65f6\u4e5f\u53ef\u4ee5\u91c7\u7528Ensemble\u4e2d\u7684\u65b9\u6cd5\u3002\n\n\n# Implementation\n## \u6570\u636e\u51c6\u5907\n\u6570\u636e\u5df2\u7ecf\u5b58\u653e\u5728\u672c\u4ed3\u5e93\u4e2d\uff0c\u4e5f\u53ef\u4ee5\u901a\u8fc7[\u6bd4\u8d5b\u754c\u9762](https://www.kaggle.com/c/titanic/data)\u8fdb\u884c\u6570\u636e\u4e0b\u8f7d\u3002\u5c06\u4e0b\u8f7d\u597d\u7684\u6570\u636e\u653e\u5728\u9879\u76ee\u6587\u4ef6\u5939\u4e2d\u5373\u53ef\u3002\n\n\u91cc\u9762\u4e00\u5171\u670911\u79cd\u4e0d\u540c\u7684\u4fe1\u606f\n```python\n{'PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'}\n```\n\n\u901a\u8fc7 `baseline.ipynb`\uff0c\u6211\u4eec\u53ef\u4ee5\u5f97\u5230\u4e0b\u9762\u7684\u6570\u636e\u5c55\u793a\n<div align=center>\n<img src='assets/data_show.jpg' width='800'>\n</div>\n\n\n## \u6570\u636e\u9884\u5904\u7406\n\u5bf9\u4e8e\u6570\u636e\u7f3a\u5931\uff0c\u6211\u4eec\u53ef\u4ee5\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u53bb\u586b\u5145\u5747\u503c\n\n```python\ntest_feat['Fare'].fillna(test['Fare'].mean(), inplace=True)\n```\n\u8fd9\u91cc\u6211\u4eec\u5bf9\u6d4b\u8bd5\u96c6\u4e2d\u7684`Fare`\u7279\u5f81\u8fdb\u884c\u4e86\u586b\u5145\n\n\u5982\u679c\u8fd9\u4e2a\u503c\u6709\u4e00\u4e9b\u7279\u70b9\uff0c\u90a3\u4e48\u6211\u4eec\u53ef\u4ee5\u5b9a\u4e49\u4e00\u4e2a\u51fd\u6570\u6765\u5b9e\u73b0\uff0c\u6bd4\u5982\u8003\u8651\u5230\u5e74\u9f84\u548c\u8239\u8231\u7b49\u7ea7\u53ef\u80fd\u6709\u4e00\u5b9a\u7684\u5173\u7cfb\n<div align=center>\n<img src='assets/pclass_age.png' width='500'>\n</div>\n\n\u6211\u4eec\u53d1\u73b0\u8239\u8231\u7b49\u7ea7\u9ad8\u7684\u4eba\u4e00\u822c\u5e74\u9f84\u6bd4\u8f83\u5927\uff0c\u6240\u4ee5\u53ef\u4ee5\u7528\u4e0b\u9762\u7684\u51fd\u6570\u53bb\u5b9e\u73b0\u7f3a\u5931\u503c\u7684\u586b\u5145\n\n```python\ndef infer_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age):\n        if Pclass == 1:\n            return 37\n        elif Pclass == 2:\n            return 29\n        else:\n            return 24\n    else:\n        return Age\n\ntrain['Age'] = train[['Age', 'Pclass']].apply(infer_age, axis=1)\n```\n\n\u5bf9\u4e8e\u975e\u6570\u503c\u7279\u5f81\uff0c\u6211\u4eec\u9700\u8981\u8f6c\u6362\u6210categorical\u7684\u5f62\u5f0f\uff0c\u53ef\u4ee5\u4f7f\u7528\u4e0b\u9762\u7684\u4ee3\u7801\n```python\nsex = pd.get_dummies(train['Sex'])\n```\n\u63a5\u7740\u6211\u4eec\u5f97\u5230\u4e86\u4e0b\u9762\u7684\u6548\u679c\n<div align=center>\n<img src='assets/dummy.png' width='300'>\n</div>\n\n\n## \u6a21\u578b\u4ee5\u53ca\u8bad\u7ec3\n\u5f97\u5230\u5904\u7406\u597d\u7684\u6570\u636e\u4e4b\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u5f00\u59cb\u8bad\u7ec3\u6a21\u578b\uff0c\u793a\u4f8b\u4ee3\u7801\u91cc\u9762\u4f7f\u7528\u4e86\u6700\u7b80\u5355\u7684`logistic regression`\uff0c\u6240\u6709\u7684\u4ee3\u7801\u90fd\u5728`baseline.ipynb`\u4e2d\u3002 \n\n\n## \u7ed3\u679c\u5206\u6790\n\u7528\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u5728\u9a8c\u8bc1\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u53ef\u4ee5\u5f97\u5230precision\uff0crecall\uff0cf1 score\u7b49\u6570\u503c\u5206\u6790\u7ed3\u679c\n\n<div align=center>\n<img src='assets/result.png' width='400'>\n</div>\n\n\n# Evaluation\n\u6211\u4eec\u7684\u6d4b\u8bd5\u811a\u672c\u4e5f\u5728 `baseline.ipynb` \u4e2d\u5b9e\u73b0\u4e86\uff0c\u901a\u8fc7\u8f93\u5165\u6240\u6709\u6d4b\u8bd5\u96c6\u7684\u6570\u636e\uff0c\u7ecf\u8fc7\u6a21\u578b\u5f97\u5230\u9884\u6d4b\u7684\u7ed3\u679c\uff0c\u7136\u540e\u8fdb\u884c\u7ed3\u679c\u7684\u63d0\u4ea4\u3002\n\n\u6211\u4eec\u4f1a\u5728\u672c\u5730\u521b\u5efa\u4e00\u4e2a\u9884\u6d4b\u7684\u7ed3\u679c `submission.csv`\uff0c\u6211\u4eec\u5c06\u8fd9\u4e2a\u6587\u4ef6\u63d0\u4ea4\u5230 kaggle\uff0c\u53ef\u4ee5\u5f97\u5230\u7c7b\u4f3c\u4e0b\u9762\u7684\u6bd4\u8d5b\u7ed3\u679c\u3002\n\n<div align=center>\n<img src='assets/submission.jpg' width='800'>\n</div>\n\n\u901a\u8fc7\u8fd9\u4e2a\u7ed3\u679c\u53ef\u4ee5\u5224\u65ad\u4f60\u7684\u6a21\u578b\u6027\u80fd\u7684\u597d\u574f\u3002"
 },
 {
  "repo": "gauss256/titanic",
  "language": "Jupyter Notebook",
  "readme_contents": "# Visualization Techniques\nFrom the Vancouver Kaggle Meetup of 2016-03-24. This is an exploration of visualization techniques, as applied to the Iris Flower data set and the data from the Titanic learner competition.\n\n## Prerequisites\nThese notebooks should run with the out-of-the-box [Anaconda distribution](https://www.continuum.io/downloads) for Python 2.7. Open a terminal window in the directory containing the files and issue the command `ipython notebook`.\n"
 },
 {
  "repo": "jmonterrubio/Titanic-Data-Investigation",
  "language": "HTML",
  "readme_contents": "# Titanic-Data-Investigation\n\nThis project resolves some questions about the titanic incident using a small dataset provided by Udacity's Nanodegree for Project 2.\n\nThe final html document was created using Anaconda (https://www.continuum.io/downloads) from source [Titanic Data Investigation](Titanic Data Investigation.ipynb)\n"
 },
 {
  "repo": "ranvirm/scala-spark-titanic-example-project",
  "language": "Scala",
  "readme_contents": "## Overview\nThis project serves as an example of a scala-spark project using the Kaggle Titanic dataset\n\n## Usage\n1. Clone repository to local directory\n2. cd into directory\n3. Compile the project with [sbt](https://www.scala-sbt.org/) using:\n```\nsbt package\n```\n4. Train model pipeline using:\n ```\n spark-submit --class ModelTrain --master local[*] --driver-memory 4G target/scala-2.11/scalasparktitanicproject_2.11-1.0.jar\n ```\n5. Train model pipeline using:\n ```\n spark-submit --class ModelPredict --master local[*] --driver-memory 4G target/scala-2.11/scalasparktitanicproject_2.11-1.0.jar\n ```\n\n## Notes\nPredictions data will be saved as a csv file in the predictions directory found in project root dir"
 },
 {
  "repo": "ravinderpayal/Titanicturer",
  "language": "JavaScript",
  "readme_contents": "error: no README"
 },
 {
  "repo": "tsundokum/mozgi-titanic",
  "language": "Python",
  "readme_contents": "mozgi-titanic\n=============\n\nTitanic at Kaggle by mozgi"
 },
 {
  "repo": "andythai/project-titanic",
  "language": "C++",
  "readme_contents": "# Project Titanic\n<b>Contributors:</b><br>\nAndy Thai<br>\nTian Yang<br>\nYunbo Chen<br>\n<br>\n<b>Description:</b><br>\nProject Titanic is a ship simulator game / program where a player controls a ship on water and attempts to avoid collision with terrain, while trying to pick up as many buoys as possible. This project has been written in C++ using the OpenGL and OpenAL libraries, with GLSL shaders.<br>\n<br>\n<b>Links:</b><br>\nDemo video: <a href=\"https://www.youtube.com/watch?v=E_oUR8v9Y5A\">YouTube</a><br>\nBinaries may require installation of the <a href=\"https://www.openal.org/\">OpenAL SDK</a> to run.<br>\n<br>\n<b>Technical features:</b><br>\n\u2022 Toon Shading<br>\n\u2022 Bounding Box Collision Detection<br>\n\u2022 Procedural Terrain Generation<br>\n\u2022 Sound Effects<br>\n\u2022 Water Effects<br>\n\u2022 Particle Effects<br>\n<br>\n<b>Screenshots:</b><br>\n![Bounding Boxes](https://4.bp.blogspot.com/-FFn-X6n-bLs/WEj3Hff8BuI/AAAAAAAAAWk/vlJCHyuSYyc0unVYJhZXwRKDJId-hJdHwCK4B/s320/SS1.png)\n![Procedural Terrain](https://1.bp.blogspot.com/-fGv0s3hoRAc/WEj3ImxYnTI/AAAAAAAAAWs/VyURO6RBYxoEZE3UgspIpE5LjibGfptJACK4B/s320/SS2.png)<br>\n![General View](https://1.bp.blogspot.com/-tMkcQD7dK7g/WEj3LDcyabI/AAAAAAAAAW0/jBIUjxemWmUHdeDS02KiqYkKDTg9c7xeQCK4B/s320/SS3.png)\n![Procedural generation early example](https://3.bp.blogspot.com/-UUEkjyGWh6E/WEPB8XycaZI/AAAAAAAAAVY/-J8rTl1Vj546hKjVfIbk6FLHOd2A0wziQCK4B/s320/procedural_t.png)<br>\n"
 },
 {
  "repo": "dataistanbul/kaggle_warmup",
  "language": "HTML",
  "readme_contents": "# titanic\nKaggle 101 Titanic Data Set Analysis\n"
 },
 {
  "repo": "mayank408/Data-Science-Hands-On",
  "language": "Jupyter Notebook",
  "readme_contents": "error: no README"
 },
 {
  "repo": "apg400/TitanicML",
  "language": "Python",
  "readme_contents": "Titanic Kaggle competition\n\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  \nOn April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 \nout of 2224 passengers and crew. This sensational tragedy shocked the international community \nand led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. \nAlthough there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, \nsuch as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. \nIn particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n"
 },
 {
  "repo": "Ffisegydd/titanic",
  "language": "Python",
  "readme_contents": "titanic\n=======\n\nKaggle competition - [Titanic: ML from disaster](https://www.kaggle.com/c/titanic-gettingStarted)\n"
 },
 {
  "repo": "filipefilardi/titanic-decision-tree",
  "language": "Jupyter Notebook",
  "readme_contents": "# Titanic Decision Tree\n\n### Data\n\nThe dataset used is included as `data/titanic_data.csv` and is provided [here](https://www.kaggle.com/c/titanic/data) by Kaggle.\n\n**Features**\n- `pclass` : Passenger Class (1 = 1st, 2 = 2nd, 3 = 3rd)\n- `name` : Name\n- `sex` : Sex\n- `age` : Age in years\n- `sibsp` : Number of Siblings/Spouses Aboard\n- `parch` : Number of Parents/Children Aboard\n- `ticket` : Ticket Number\n- `fare` : Passenger Fare\n- `cabin` : Cabin number\n- `embarked` : Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\n\n**Target Variable**\n- `survival` : Survival (0 = No, 1 = Yes)\n"
 },
 {
  "repo": "EdmundHee/kaggle-titanic",
  "language": "Jupyter Notebook",
  "readme_contents": ""
 },
 {
  "repo": "SeanLee97/titanic_disaster",
  "language": "Jupyter Notebook",
  "readme_contents": "\n# \u6570\u636e\u5206\u6790\u2014\u6cf0\u5766\u5c3c\u514b\u4e4b\u707e\n\n[@seanlee](https://github.com/SeanLee97)\n\n\u76ee\u5f55\uff1a\n1. \u52a0\u8f7d\u6570\u636e\uff0c\u5206\u6790\u6570\u636e\n2. \u6570\u636e\u6e05\u6d17\n3. \u7279\u5f81\u5de5\u7a0b\n4. \u6a21\u578b\u9884\u6d4b\n\n## 1. \u52a0\u8f7d\u6570\u636e\uff0c\u5206\u6790\u6570\u636e\u7684\u7ec4\u6210\n\n\n```python\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random as rd\n```\n\n\n```python\ndf = pd.read_csv('./data/train.csv')\n# \u8f93\u51fa\u6570\u636e\u4fe1\u606f\nprint(df.info())\n```\n\n    <class 'pandas.core.frame.DataFrame'>\n    RangeIndex: 891 entries, 0 to 890\n    Data columns (total 12 columns):\n    PassengerId    891 non-null int64\n    Survived       891 non-null int64\n    Pclass         891 non-null int64\n    Name           891 non-null object\n    Sex            891 non-null object\n    Age            714 non-null float64\n    SibSp          891 non-null int64\n    Parch          891 non-null int64\n    Ticket         891 non-null object\n    Fare           891 non-null float64\n    Cabin          204 non-null object\n    Embarked       889 non-null object\n    dtypes: float64(2), int64(5), object(5)\n    memory usage: 83.6+ KB\n    None\n\n\n\u53ef\u4ee5\u770b\u5230\uff0c\u6570\u636e\u603b\u5171\u670912\u4e2a\u5c5e\u6027\uff0c\u5206\u522b\u662f\n1. 'PassengerId': \u4e58\u5ba2\u7f16\u53f7\n2. 'Survived'\uff1a \u662f\u5426\u5b58\u6d3b\uff081\u5b58\u6d3b\uff0c0\u6b7b\u4ea1\uff09\n3. 'Pclass'\uff1a \u5ba2\u8231\u7b49\u7ea7 \uff081\uff0c 2\uff0c 3\uff09\n4. 'Name'\uff1a \u4e58\u5ba2\u59d3\u540d\n5. 'Sex'\uff1a \u4e58\u5ba2\u6027\u522b\n6. 'Age'\uff1a \u4e58\u5ba2\u5e74\u9f84 \n7. 'SibSp'\uff1a \u4e58\u5ba2\u5144\u5f1f\u59d0\u59b9/\u914d\u5076\u6570\n8. 'Parch'\uff1a \u4e58\u5ba2\u7236\u6bcd/\u5b50\u5973\u6570\n9. 'Ticket': \u8239\u7968\u53f7\n9. 'Fare'\uff1a \u8239\u7968\u4ef7\u683c\n10. 'Cabin'\uff1a \u8231\u53f7\n11. 'Embarked'\uff1a\u767b\u9646\u6e2f\u53e3\n\n\u5f97\u5230\u5c5e\u6027\u540e\u6211\u4eec\u5173\u5fc3\u7684\u662f\u627e\u5230\u548c\u7528\u6237\u5b58\u6d3b\u76f8\u5173\u7684\u5c5e\u6027\uff0c\u4e5f\u5c31\u662f\u54ea\u4e9b\u5c5e\u6027\u5f71\u54cdSurvived\u5c5e\u6027\u3002\n\n\u4ece\u7ed9\u51fa\u7684info\u8868\u770b\u5230\uff0c\u5b58\u5728\u6570\u636e\u7f3a\u5931\u7684\u60c5\u51b5\uff0c\u5982\uff1a**Age, Cabin, Embarked**\uff0c**\u5f80\u5f80\u6570\u636e\u7f3a\u5931\u7684\u90e8\u5206\u662f\u63d0\u9ad8\u6574\u4e2a\u6a21\u578b\u7684\u51c6\u786e\u7387\u7684\u5173\u952e**\uff0c\u6211\u4eec\u8981\u505a\u7684\u662f\u6316\u6398\u51fa\u4e22\u5931\u7684\u6570\u636e\uff0c\u5c06\u5176\u5f71\u54cd\u52a0\u5165\u5230\u7ed3\u679c\u9884\u6d4b\u4e2d\n\n### 1.1 \u5148\u770b\u503c\u7684\u5206\u5e03\n\n\n```python\ndf.describe()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>714.000000</td>\n      <td>891.000000</td>\n      <td>891.000000</td>\n      <td>891.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>446.000000</td>\n      <td>0.383838</td>\n      <td>2.308642</td>\n      <td>29.699118</td>\n      <td>0.523008</td>\n      <td>0.381594</td>\n      <td>32.204208</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>257.353842</td>\n      <td>0.486592</td>\n      <td>0.836071</td>\n      <td>14.526497</td>\n      <td>1.102743</td>\n      <td>0.806057</td>\n      <td>49.693429</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.420000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>223.500000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>20.125000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>7.910400</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>446.000000</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>28.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>14.454200</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>668.500000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>38.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>31.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>891.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>80.000000</td>\n      <td>8.000000</td>\n      <td>6.000000</td>\n      <td>512.329200</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\ndescibe() \u7ed9\u51fa\u4e86\u603b\u6570\uff0c\u5747\u503c\uff0c\u6807\u51c6\u503c\u7b49\u6570\u636e\uff0c\u770b\u4e86\u5934\u6709\u70b9\u6655\u5427\uff0c\u597d\u5427\u8fd9\u91cc\u6211\u4eec\u53ea\u662f\u5927\u6982\u770b\u4e00\u4e0b\u503c\u7684\u8303\u56f4\uff0c\u6700\u540e\u7684\u6570\u503c\u7684\u89c4\u6574\u5316\u624d\u4f1a\u7528\u5230\n\n## 2. \u6570\u636e\u7684\u6e05\u6d17\n\n\u63a5\u4e0b\u6765\u9010\u4e2a\u63ed\u5f00\u6570\u636e\u7684\u9762\u7eb1\uff01\n\n### Survived\n\n\u8ba9\u6211\u4eec\u5148\u770b\u770b\u5b58\u6d3b\u5360\u6bd4\u5427\n\n\n```python\ndf.Survived.value_counts().plot(kind='bar')\nprint(df.Survived.value_counts().values)\nplt.show()\n```\n\n    [549 342]\n\n\n\n![png](./static/output_10_1.png)\n\n\n### PassengerId\n\n\u4e58\u5ba2\u7f16\u53f7\uff0c\u4e00\u822c\u6765\u8bf4\u7f16\u53f7\u662f\u987a\u5e8f\u751f\u6210\u7684\uff0c\u5982\u679c\u662f\u987a\u5e8f\u751f\u6210\u7684\u8bdd\uff0c\u90a3\u4e48\u6570\u636e\u5c31\u6ca1\u6709\u591a\u5927\u7684\u610f\u4e49\u3002\n\n\u4f46\u662f\u4e5f\u6709\u5176\u4ed6\u60c5\u51b5\uff0c\u6bd4\u5982\u94f6\u884c\u6392\u53f7\uff0c\u91d1\u3001\u94f6\u3001\u666e\u5361\u7684\u7f16\u53f7\u4e00\u822c\u662f\u4e0d\u4e00\u6837\u7684\uff0c\u5982\u679c\u662f\u6309\u7167\u8fd9\u79cd\u65b9\u5f0f\u8bf4\u660ePassagerId\u4e2d\u5305\u542b\u4e86\u4e0d\u540c\u7b49\u7ea7\u4eba\u7fa4(\u611f\u89c9\u6709\u70b9\u7528\u8bcd\u4e0d\u5f53)\u7684\u542b\u4e49\uff0c\u6240\u4ee5\u4e00\u822c\u662f\u6309\u533a\u95f4\u6765\u5904\u7406\u3002\u5230\u5e95\u5c5e\u4e8e\u54ea\u79cd\u5462\uff1f\u6211\u4eec\u5148\u5206\u6790\u5206\u6790\n\n\n```python\nplt.title('PassagerId')\nplt.plot(df.PassengerId)\nplt.show()\n```\n\n\n![png](./static/output_12_0.png)\n\n\n\u597d\u5427\uff0c\u53ef\u4ee5\u770b\u5230\u7f16\u53f7\u662f\u987a\u5e8f\u751f\u6210\u7684\uff0c\u6240\u4ee5\uff0c\u6211\u4eec\u5148\u653e\u4e0b\u8fd9\u4e2a\u5c5e\u6027\n\n### Pclass\n\nPclass\u662f\u8231\u4f4d\u7684\u7b49\u7ea7\uff0c\u5c31\u597d\u6bd4\u98de\u673a\u5206\u5934\u7b49\u8231\uff0c\u5546\u52a1\u8231\uff0c\u666e\u901a\u3002\u4e00\u822c\u6765\u8bf4\u8d8a\u9ad8\u7ea7\u7684\u8231\u4f4d\u63d0\u4f9b\u7684\u5b89\u5168\u63aa\u65bd\u53ef\u80fd\u4f1a\u6bd4\u8f83\u5230\u4f4d\uff0c\u6240\u4ee5\u8fd9\u5e94\u8be5\u662f\u4e00\u4e2a\u5f71\u54cd\u56e0\u7d20\uff0c\u5230\u5e95\u662f\u4e0d\u662f\u5462\uff1f\u8ba9\u6211\u4eec\u5206\u6790\u5206\u6790\u518d\u4e0b\u5b9a\u8bba\n\n\n```python\nplt.title('Pclass')\nplt.xlabel('class')\nplt.ylabel('persons')\nprint(df.Pclass.value_counts())\ndf.Pclass.value_counts().plot(kind='bar')\nplt.show()\n```\n\n    3    491\n    1    216\n    2    184\n    Name: Pclass, dtype: int64\n\n\n\n![png](./static/output_15_1.png)\n\n\n\u4e0a\u9762\u662f\u5404\u8231\u4f4d\u4eba\u6570\u5206\u5e03\u3002\u73b0\u5728\u8ba9\u6211\u770b\u770b\u5404\u8231\u4f4d\u5b58\u6d3b\u4eba\u6570\n\n\n```python\nfig = plt.figure()\nalive = df.Pclass[df.Survived == 1].value_counts()\ndie = df.Pclass[df.Survived == 0].value_counts()\nprint('alive')\nprint(alive.values)\nprint('die')\nprint(die.values)\n_df = pd.DataFrame({'alive': alive, 'die': die})\n_df.plot(kind='bar', stacked=True)\nplt.show()\n```\n\n    alive\n    [136 119  87]\n    die\n    [372  97  80]\n\n\n\n    <matplotlib.figure.Figure at 0x7f7fc905b978>\n\n\n\n![png](./static/output_17_2.png)\n\n\n\u53ef\u4ee5\u770b\u52303\u4e2d\u6b7b\u4ea1\u5360\u6bd4\u6700\u9ad8\uff0c1\u6700\u5c11\uff0c3\u5e94\u8be5\u662f\u666e\u901a\u8231\uff0c1\u5e94\u8be5\u662f\u8d35\u5bbe\u8231\uff0c\u6240\u4ee5\u8231\u4f4d\u7b49\u7ea7\u5bf9\u5b58\u6d3b\u9020\u6210\u5f71\u54cd\n\n### Cabbin\n\n\u8231\u53f7\u7f3a\u5931\u6bd4\u8f83\u4e25\u91cd\uff0c \u5148\u5206\u6790\u8231\u53f7\u7f3a\u5931\u548c\u4e0d\u7f3a\u5931\u5b58\u6d3b\u5360\u6bd4\n\n\n```python\ncabin_notnull = df.Survived[pd.notnull(df.Cabin)].value_counts()\ncabin_null = df.Survived[pd.isnull(df.Cabin)].value_counts()\n\n_df = pd.DataFrame({'Not NAN': cabin_notnull, 'NAN': cabin_null})\n_df.plot(kind='bar', stacked=True)\nplt.show()\n```\n\n\n![png](./static/output_20_0.png)\n\n\n\u53ef\u4ee5\u770b\u5230\u6b7b\u4ea1\u7684\u4e58\u5ba2\u4e2d\u4e22\u5931\u8231\u53f7\u7684\u5360\u5927\u591a\u6570\uff0c\u4ece\u76f4\u89c2\u4e0a\u53ef\u4ee5\u8ba4\u4e3a\u8231\u53f7\u6570\u636e\u662f\u5426\u4e22\u5931\u4e0e\u5b58\u6d3b\u6709\u5173\u8054\uff0c\u4f46\u7531\u4e8e\u8231\u53f7\u5927\u90e8\u5206\u7f3a\u5931\uff0c\u672a\u77e5\u6027\u592a\u591a\uff0c\u4e00\u4e9b\u5185\u90e8\u7684\u5173\u7cfb\u4e0d\u80fd\u5f97\u5230\u786e\u5b9a\uff0c\u6240\u4ee5\u8fd9\u4e2a\u5c5e\u6027\u8fd8\u5f97\u9700\u8981\u6316\u6398\u3002\n\n\n### Embarked\n\n\u767b\u9646\u6e2f\u53e3\u548c\u5b58\u6d3b\u7387\u5206\u6790\n\n\n```python\nalive = df.Embarked[df.Survived == 1].value_counts()\ndie = df.Embarked[df.Survived == 0].value_counts()\n\n_df = pd.DataFrame({'alive': alive, 'die': die})\n_df.plot(kind='bar', stacked = True)\nplt.show()\n```\n\n\n![png](./static/output_25_0.png)\n\n\n\u53ef\u4ee5\u770b\u5230\u767b\u9646\u6e2f\u53e3\u548c\u5b58\u6d3b\u7387\u8fd8\u662f\u6709\u4e00\u5b9a\u5173\u8054\u7684\uff0c\u800c\u4e14\u767b\u9646\u6e2f\u53e3\u6570\u636e\u5b58\u5728\u90e8\u5206\u7f3a\u5931\uff0c\u6211\u4eec\u5e94\u8be5\u8981\u6316\u6398\u7f3a\u5931\u7684\u90e8\u5206\u7684\u5f71\u54cd\n\n### Name \n\n\u56e0\u4e3a\u7537\u751f\u5973\u751f\u7684\u59d3\u540d\u4e00\u822c\u4e0d\u540c\uff0c\u7ed9\u4e00\u4e2a\u59d3\u540d\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u53ef\u4ee5\u5206\u8fa8\u51fa\u6027\u522b\u3002\u867d\u7136\u5206\u51fa\u4e86\u6027\u522b\uff0c\u4f46\u662fMiss\u548cMrs.\u90fd\u662ffemale\u79f0\u547c\u4e0d\u540c\u53ef\u80fd\u5e74\u9f84\u4e5f\u4e0d\u540c\uff0c\u800c\u5e74\u9f84\u53c8\u5b58\u5728\u90e8\u5206\u7f3a\u5931\uff0c\u56e0\u6b64\u6211\u4eec\u9700\u8981\u5c06\u540d\u5b57\u8003\u8651\u5728\u5185\u3002\u5177\u4f53\u7684\u5904\u7406\u5728\u7279\u5f81\u5de5\u7a0b\u90e8\u5206\u7ed9\u51fa\u3002\n\u540d\u5b57\u4e2d\u90fd\u542b\u6709Mrs, Miss, Mr\u8fd9\u4e9b\u6807\u8bc6\u6027\u522b\u7684\u5173\u952e\u8bcd\uff0c\u5176\u5b9e\u4e5f\u5c31\u662f\u76f4\u63a5\u7ed9\u51fa\u4e86\u6027\u522b\uff0c\u6240\u4ee5Name\u5c5e\u6027\u7684\u4ef7\u503c\u53ef\u4ee5\u8ba4\u4e3a\u5f52\u5230\u4e86Sex\u4e0a\n\n### Age\n\n\n```python\nfig = plt.figure(figsize=(16, 4))\n\nplt.subplot2grid((1, 3),(0, 0))\nplt.title('age & survived')\nplt.xlabel('age')\nplt.ylabel('survived')\nplt.yticks([0,1])\nplt.scatter(df.Age, df.Survived)\n\n# \u5404\u5e74\u9f84\u5206\u5e03\nplt.subplot2grid((1, 3), (0, 1))\nage_0_20 = len(df.Age[(df.Age.values >= 0) & (df.Age.values < 20)])\nage_20_40 = len(df.Age[(df.Age.values >= 20) & (df.Age.values < 40)])\nage_40_ = len(df.Age[(df.Age.values >= 40)])\nage_list = (age_0_20,age_20_40,age_40_)\nplt.title('age range')\nplt.ylabel('persons')\nplt.xlabel('age')\nplt.xticks((0,1, 2),('<20','20~40', '>=40'))\nplt.bar(left=(0,1,2),height=age_list, align='center', color=('blue', 'orange', 'green'))\n\n# \u5404\u7b49\u7ea7\u5e74\u9f84\u5206\u5e03\nplt.subplot2grid((1, 3), (0, 2))\nline1 = df.Age[df.Pclass==1].plot(kind='kde')\nline2 = df.Age[df.Pclass==2].plot(kind='kde')\nline3 = df.Age[df.Pclass==3].plot(kind='kde')\n#_df = pd.DataFrame({'pclass1_age': line1, 'pclass2_age': line2, 'pclass3_age': line3})\n#_df.plot(kind='kde')\nplt.legend(('pclass1_age', 'pclass2_age', 'pclass3_age'))\nplt.xlabel('age')\n\nplt.show()\n```\n\n\n![png](./static/output_31_0.png)\n\n\n\u56fe1\uff1a\u53ef\u4ee5\u770b\u5230\u7684\u4fe1\u606f\u611f\u89c9\u4e0d\u662f\u5f88\u591a\uff0c\u6bd4\u8f83\u660e\u663e\u7684\u662f60\uff5e80\u4e4b\u95f4\u5b58\u6d3b\u7387\u6bd4\u8f83\u591a\uff0c\n\n\u56fe2\uff1a\u53ef\u4ee5\u770b\u523020~40\u5e74\u9f84\u5c42\u5360\u6bd4\u6700\u9ad8\uff0c\u4e5f\u5c31\u662f\u4e2d\u9752\u5e74\u5360\u6bd4\u6700\u9ad8\n\n\u56fe3\uff1a\u53ef\u4ee5\u770b\u5230\u5404\u7b49\u7ea7\u8231\u4f4d\u5e74\u9f84\u5360\u6bd4\u3002\u53ef\u4ee5\u770b\u5230\u4e00\u7b49\u8231\u4e2d\u5e74\u9f8440\u5c81\u5de6\u53f3\u7684\u6bd4\u8f83\u591a\uff0c\u4e8c\u7b49\u8231\u4e2d30\u5de6\u53f3\u7684\u6bd4\u8f83\u591a\uff0c\u4e09\u7b49\u823120\u5de6\u53f3\u6bd4\u8f83\u591a\uff0c\u8d22\u5bcc\u7684\u914d\u6bd4\u4e5f\u7b26\u5408\u73b0\u5b9e\u751f\u6d3b\u89c4\u5f8b\u3002\n\n\u7531\u6b64\u53ef\u4ee5\u77e5\u9053\uff0c\u5e74\u9f84\u53ef\u4ee5\u76f4\u63a5\u5f71\u54cd\u5b58\u6d3b\u7387\uff08\u9752\u4e2d\u5e74\u5404\u65b9\u9762\u751f\u7406\u673a\u80fd\u5f3a\uff0c\u5b58\u6d3b\u7387\u9ad8\uff09\uff0c\u4e5f\u53ef\u4ee5\u95f4\u63a5\u5f71\u54cd\uff0c\u56e0\u4e3a\u5e74\u9f84\u548c\u6240\u4e58\u7684\u8231\u4f4d\u7b49\u7ea7\u6709\u4e00\u5b9a\u7684\u5173\u8054\u3002\n\n**\u4e4b\u524d\u7684\u5206\u6790\u5df2\u7ecf\u77e5\u9053\u5e74\u9f84\u5b58\u5728\u7f3a\u5931\u60c5\u51b5\uff0c\u5982\u4f55\u51cf\u5f31\u7f3a\u5931\u90e8\u5206\u7684\u5f71\u54cd\u6211\u4eec\u7559\u5230\u540e\u9762**\n\n### Sex\n\n\u4fd7\u8bdd\u8bf4\u201c\u5973\u58eb\u4f18\u5148\uff0c\u7537\u58eb\u9760\u8fb9\u201d\uff0c\u53ef\u60f3\u800c\u77e5\u5e74\u9f84\u5e94\u8be5\u4e5f\u662f\u4e00\u4e2a\u5f71\u54cd\u56e0\u7d20\uff0c\u6211\u4eec\u5206\u6790\u5206\u6790\n\n\n```python\nmale = len(df.Sex[df.Sex == 'male'])\nfemale = len(df.Sex[df.Sex == 'female'])\nsex_list = (male, female)\nplt.ylabel('persons')\nplt.xticks((0, 1),('male','female'))\nplt.bar(left=(0,1),height=sex_list, align='center', color=('blue', 'orange'))\nplt.show()\n\nmale = df.Survived[df.Sex == 'male'].value_counts()\nfemale = df.Survived[df.Sex == 'female'].value_counts()\n_df = pd.DataFrame({'male': male, 'female': female})\n_df.plot(kind='bar', stacked=True)\nplt.show()\n```\n\n\n![png](./static/output_34_0.png)\n\n\n\n![png](./static/output_34_1.png)\n\n\n\u56fe1\uff1a\u7537\u58eb\u6bd4\u5973\u58eb\u591a\n\n\u56fe2\uff1a\u5b58\u6d3b\u4e2d\u5973\u58eb\u5360\u591a\u6570\uff0c\u6b7b\u4ea1\u4e2d\u7537\u58eb\u5360\u591a\u6570\n\n\u75311\uff0c2\u53ef\u4ee5\u77e5\u9053\u6027\u522b\u5bf9\u5b58\u6d3b\u7387\u4e5f\u6709\u5f71\u54cd\n\n### SibSp\n\n\u8239\u4e0a\u4e58\u5ba2\u7684\u5144\u5f1f\u59d0\u59b9/\u914d\u5076\u6570\n\n\n```python\nalive = df.SibSp[df.Survived == 1].value_counts()\ndie = df.SibSp[df.Survived == 0].value_counts()\n_df = pd.DataFrame({'alive': alive, 'die': die})\n_df.plot(kind='bar', stacked=True)\nplt.show()\n```\n\n\n![png](./static/output_37_0.png)\n\n\n### Parch\n\n\u8239\u4e0a\u4e58\u5ba2\u7684\u7236\u6bcd/\u5b50\u5973\u6570\n\n\n```python\nalive = df.Parch[df.Survived == 1].value_counts()\ndie = df.Parch[df.Survived == 0].value_counts()\n_df = pd.DataFrame({'alive': alive, 'die': die})\n_df.plot(kind='bar', stacked=True)\nplt.show()\n```\n\n\n![png](./static/output_39_0.png)\n\n\n\u6211\u4eec\u5c06SibSp\u548cParch\u7ed3\u5408\u5728\u4e00\u8d77\u770b\u770b\u6709\u6ca1\u6709\u89c4\u5f8b\n\n## 3. \u7279\u5f81\u5de5\u7a0b (feature engineering)\n\n\u63a5\u4e0b\u6765\u8981\u505a\u7684\u662f\u5bf9\u8fd9\u4e9b\u5c5e\u6027\u505a\u7279\u5f81\u5de5\u7a0b\uff0c\u4e5f\u662f\u6700\u9700\u8981\u8111\u6d1e\u548c\u6700\u5173\u952e\u7684\u4e00\u6b65\u3002\n\n\u8fd9\u4e00\u6b65\u6211\u4eec\u4e3b\u8981\u5904\u7406\uff1a\n* \u5c06\u5b57\u7b26\u6570\u636e\u6570\u503c\u5316   \n   \u6570\u503c\u5316\u4e00\u822c\u4e24\u79cd\u65b9\u5f0f\n   \u4ee5\u6027\u522b\uff08gender\uff09\uff1a\u7537\u5973\uff0c \u4e3a\u4f8b\u89e3\u91ca\n   * \u8fde\u7eed\u503c\u8868\u793a\uff1a\u5373gender\u6709\u4e24\u503c\uff0c1\u4ee3\u8868\u7537\uff0c0\u4ee3\u8868\u5973\u8868\u793a\u6027\u522b\n   * \u4e8c\u8fdb\u5236\u8868\u793a\uff1a\u9700\u8981\u65b0\u589e\u5b57\u6bb5gender_male, gender_female\u75280\u548c1\u4ee3\u8868\u662f\u5426\u9009\u62e9\u8be5\u503c\n   \n\n* \u5bf9\u7f3a\u5931\u503c\u7684\u5904\u7406\n   \u5bf9\u4e8e\u7f3a\u503c\u7684\u60c5\u51b5\uff0c\u5e38\u89c1\u7684\u5904\u7406\u65b9\u6cd5\uff1a\n   * \u82e5\u7f3a\u503c\u7684\u6837\u672c\u5360\u603b\u6570\u7684\u6bd4\u4f8b\u6781\u9ad8\uff0c\u53ef\u4ee5\u8003\u8651\u820d\u5f03\u3002\n   * \u53ef\u4ee5\u91c7\u7528\u4e2d\u4f4d\u6570\uff0c\u5e73\u5747\u503c\uff0c\u4f17\u6570\u6765\u586b\u5145\u7f3a\u5931\u503c\n   * \u5982\u679c\u7f3a\u503c\u7684\u6837\u672c\u9002\u4e2d\uff0c**\u79bb\u6563\u578b**\u7279\u5f81\u5c5e\u6027\uff08\u6bd4\u5982\u8bf4\u7c7b\u76ee\u5c5e\u6027\uff09,\u90a3\u5c31\u628aNaN\u4f5c\u4e3a\u4e00\u4e2a\u65b0\u7c7b\u522b\uff0c\u52a0\u5230\u7c7b\u522b\u7279\u5f81\u4e2d\n   * \u5982\u679c\u7f3a\u503c\u7684\u6837\u672c\u9002\u4e2d\uff0c**\u8fde\u7eed\u578b**\u7279\u5f81\u5c5e\u6027\uff0c\u53ef\u4ee5\u8003\u8651\u7ed9\u5b9a\u4e00\u4e2a\u6b65\u957f\uff0c\u76ee\u7684\u662f\u628a\u5b83**\u79bb\u6563\u5316**\uff0c\u7136\u540e\u628aNaN\u4f5c\u4e3a\u4e00\u4e2a\u7c7b\u578b\u52a0\u5230\u5c5e\u6027\u7c7b\u76ee\u4e2d\n   * \u56de\u5f52\u9884\u6d4b\u51fa\u7f3a\u5931\u503c\n\n\n```python\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = preprocessing.StandardScaler()\n\ntrain_df = df\ntest_df = pd.read_csv('./data/test.csv')\ndf = pd.concat([train_df, test_df])\n\n# \u4ece\u5b57\u7b26\u4e32\u4e2d\u63d0\u53d6\u51fa\u6570\u5b57\u4e32\ndef FindNumber(item):\n    match = re.compile(\"([\\d]+$)\").search(item)\n    if match:\n        return match.group()\n    else:\n        return 0\n    \n# \u83b7\u53d6\u5b57\u6bcd\ndef getLetter(item):\n    match = re.compile(\"([a-zA-Z]+)\").search(item)\n    if match:\n        return match.group()\n    else:\n        return 'U'\n```\n\n### \u5bf9Cabin\u8fdb\u884c\u7279\u5f81\u5de5\u7a0b\n\n\n```python\ndf['Cabin'][df.Cabin.isnull()] = 'U0'\n    \ndf['CabinLetter'] = df['Cabin'].map( lambda x : getLetter(x))\ndf['CabinLetter'] = pd.factorize(df['CabinLetter'])[0]\ndf['CabinNum'] = df['Cabin'].map( lambda x : FindNumber(x)).astype(int)\ndf['CabinNum'] = scaler.fit_transform(df['CabinNum'].reshape(-1 ,1))\n#df['CabinNum']\n```\n\n### Ticket\n\n\u5c06Ticket\u4e2d\u7684\u6570\u5b57\u90e8\u5206\u4f5c\u4e3a\u6570\u503c\n\n\n```python\n#print(df.Ticket)\ndf['TicketNum'] = df.Ticket.map( lambda x : FindNumber(x)).astype(int)\ndf['TicketNum'] = scaler.fit_transform(df['TicketNum'].reshape(-1 ,1))\n#print(df.TicketNum)\n```\n\n### Sex\n\n1\u66ff\u6362male, 0 \u66ff\u6362female\n\n\n```python\ndf['Gender'] = np.where(df['Sex'] == 'male', 1, 0)\n#df['Gender']\n```\n\n### Fare\n\n\u9009\u62e9\u4e2d\u4f4d\u6570\u4f5c\u4e3a\u672a\u77e5\u503c\u7684\u586b\u5145\u503c\n\n\n```python\ndf['Fare'][ np.isnan(df['Fare']) ] = df['Fare'].median()\ndf['Fare'] = scaler.fit_transform(df['Fare'].reshape(-1 ,1))\n```\n\n### Embarked\n\n\u5bf9\u4e8e\u767b\u9646\u6e2f\u53e3\u4e3a\u7a7a\u65f6\uff0c\u5c06\u5176\u8d4b\u503c\u4e3a\u51fa\u73b0\u6b21\u6570\u6700\u591a\u7684\u6e2f\u53e3,\u5e76\u5c06\u6e2f\u53e3\u6570\u503c\u5316\n\n\n```python\ndf.Embarked[df.Embarked.isnull()] = df.Embarked.dropna().mode().values\n# \u6570\u503c\u5316\ndf['Embarked'] = pd.factorize(df['Embarked'])[0]\n#df['Embarked']\n```\n\n### Pclass\n\n\u5bf9\u7f3a\u5931\u503c\u7684\u5904\u7406\u548cEmbarked\u4e00\u6837\uff0c\u7528\u51fa\u73b0\u6b21\u6570\u6700\u591a\u7684\u503c\u66ff\u4ee3\n\n\n```python\ndf.Pclass[ df.Pclass.isnull() ] = df.Pclass.dropna().mode().values\n\ndf['Pclass'] = scaler.fit_transform(df['Pclass'].reshape(-1 ,1))\n```\n\n### Name\n\ndf['Appellation']  -  \u8bb0\u5f55\u79f0\u547c\uff0cMr, Mrs, Miss ... , \u5e76\u6570\u503c\u5316\n\n\n```python\ndf['Appellation'] = df['Name'].map(lambda x: re.compile(\", (.*?)\\.\").findall(x)[0])\n\ndf['Appellation'][df.Appellation == 'Jonkheer'] = 'Master'\ndf['Appellation'][df.Appellation.isin(['Ms','Mlle'])] = 'Miss'\ndf['Appellation'][df.Appellation == 'Mme'] = 'Mrs'\ndf['Appellation'][df.Appellation.isin(['Capt', 'Don', 'Major', 'Col', 'Sir'])] = 'Sir'\ndf['Appellation'][df.Appellation.isin(['Dona', 'Lady', 'the Countess'])] = 'Lady'\n\n# \u6570\u503c\u5316\ndf['AppellationNum'] = pd.factorize(df['Appellation'])[0]\n```\n\n\n```python\ndf['SibSp'] = scaler.fit_transform(df['SibSp'].reshape(-1 ,1))\ndf['Parch'] = scaler.fit_transform(df['Parch'].reshape(-1 ,1))\n```\n\n### Age\n\nAge \u7531\u4e4b\u524d\u7684\u5206\u6790\u53ef\u77e5\u662f\u4e00\u4e2a\u91cd\u8981\u5f71\u54cd\u56e0\u7d20\uff0c\u5f88\u591a\u5c5e\u6027\u53ef\u4ee5\u5f71\u54cd\u5230\u5e74\u9f84\uff0c\u6240\u4ee5\u5728\u8fd9\u91cc\u6211\u4eec\u7528**\u56de\u5f52\u53bb\u9884\u6d4b\u7f3a\u5931\u7684\u5e74\u9f84\u3002**\n\n\n```python\nage_df = df[['Age', 'Embarked','Fare', 'Parch', 'SibSp', 'AppellationNum','Pclass', 'CabinLetter', 'CabinNum']]\nX = age_df.loc[(df.Age.notnull())].values[:, 1::]\ny = age_df.loc[(df.Age.notnull())].values[:, 0]\n\nrgr = RandomForestRegressor(n_estimators=2000, n_jobs=-1)\nrgr.fit(X, y)\n\npreds = rgr.predict(age_df.loc[ (df.Age.isnull()) ].values[:, 1::])\ndf.loc[ (df.Age.isnull()), 'Age' ] = preds\ndf['Age'] = scaler.fit_transform(df['Age'].reshape(-1 ,1))\n```\n\n\n```python\ndf_choosed = df[['Survived', 'Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'TicketNum', 'CabinLetter', 'CabinNum', 'Gender', 'AppellationNum']]\n\ntrain_df = df_choosed[:train_df.shape[0]] \ntest_df  = df_choosed[train_df.shape[0]:]\n```\n\n## 4. \u6a21\u578b\n\n* \u7b97\u6cd5\u7684\u9009\u62e9\n    \u8fd9\u7c7b\u6570\u636e\u6211\u4eec\u4e00\u822c\u4f7f\u7528\u96c6\u6210\u7684\u65b9\u6cd5\u4f5c\u4e3a\u5206\u7c7b\u5668\uff0c\u968f\u673a\u68ee\u6797,GDBT, XGBoost\u4f1a\u6709\u8f83\u597d\u7684\u6548\u679c\n* \u53c2\u6570\u7684\u9009\u62e9\n    \u4e3a\u4e86\u627e\u5230\u6700\u4f73\u53c2\u6570\u6211\u4eec\u4f7f\u7528\u7f51\u683c\u641c\u7d22\u6765\u5bfb\u627e\u6700\u4f73\u53c2\u6570\uff08\u5982\u679c\u6570\u636e\u91cf\u8f83\u5927\u53ef\u4ee5\u9009\u62e9\u968f\u673a\u8c03\u53c2\uff09\n* \u6a21\u578b\u7684\u8c03\u6574\n    * \u8fc7\u62df\u5408\uff1a \u5982\u679c\u51fa\u73b0\u4e86\u9ad8\u65b9\u5dee\uff0c\u4f4e\u504f\u5dee\u7684\u60c5\u51b5\u90a3\u4e48\u53ef\u80fd\u51fa\u73b0\u4e86\u8fc7\u62df\u5408\uff0c\u8fc7\u62df\u5408\u7684\u89e3\u51b3\u65b9\u6cd5\u4e3b\u8981\u6709\uff1a\n        * \u66f4\u591a\u7684\u8bad\u7ec3\u6570\u636e\n        * \u6570\u636e\u6b63\u5219\u5316\n        * \u7f51\u7edc\u4e0d\u5b9c\u8fc7\u6df1\n    * \u6b20\u62df\u5408\uff1a \u5982\u679c\u51fa\u73b0\u4e86\u9ad8\u504f\u5dee\uff0c\u4f4e\u65b9\u5dee\u7684\u60c5\u51b5\u90a3\u4e48\u53ef\u80fd\u51fa\u73b0\u4e86\u6b20\u62df\u5408\uff0c\u6b20\u62df\u5408\u997f\u89e3\u51b3\u65b9\u6cd5\u4e3b\u8981\u6709\uff1a\n        * \u52a0\u6df1\u7f51\u7edc\u997f\u6df1\u5ea6\n        * \u8bad\u7ec3\u65f6\u95f4\u52a0\u957f\uff08\u795e\u7ecf\u7f51\u7edc\uff09\n\n\n```python\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\ntrain_data = train_df.as_matrix()\ntest_data = test_df.as_matrix()\n\nX_train, y_train = train_data[:, 1::], train_data[:, 0]\nX_test, y_test = test_data[:, 1::], test_data[:, 0]\n\nclf = RandomForestClassifier(n_jobs=-1, oob_score=True)\n\nparams = { \n    \"n_estimators\" : [200, 500, 1000, 2000, 5000, 10000],\n    \"max_features\" : [3, 4, 5], \n    \"min_samples_split\" : [4, 6, 8, 10]\n}\ngs= GridSearchCV(clf, params)\ngs.fit(X_train, y_train)\nprint('Done !')\n```\n\n    Done !\n\n\n\n```python\ntrain_score = gs.score(X_train, y_train)\nprint('train accuracy: ', train_score)\n```\n\n    train accuracy:  0.930415263749\n\n\n\n```python\ny_pred = gs.predict(X_test)\n\npassengerId = df['PassengerId'][train_df.shape[0]:]\n\n# out to file\nresult = pd.DataFrame({'PassengerId': passengerId.as_matrix(), 'Survived':y_pred.astype(np.int32)})\nresult.to_csv(\"./result.csv\", index=False)\nresult\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>892</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>893</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>894</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>895</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>896</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>897</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>898</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>899</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>900</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>901</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>902</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>903</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>904</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>905</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>906</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>907</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>908</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>909</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>910</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>911</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>912</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>913</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>914</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>915</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>916</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>917</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>918</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>919</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>920</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>921</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>388</th>\n      <td>1280</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>389</th>\n      <td>1281</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>390</th>\n      <td>1282</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>391</th>\n      <td>1283</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>392</th>\n      <td>1284</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>393</th>\n      <td>1285</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>394</th>\n      <td>1286</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>395</th>\n      <td>1287</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>396</th>\n      <td>1288</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>397</th>\n      <td>1289</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>398</th>\n      <td>1290</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>399</th>\n      <td>1291</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>400</th>\n      <td>1292</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>401</th>\n      <td>1293</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>402</th>\n      <td>1294</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>403</th>\n      <td>1295</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>404</th>\n      <td>1296</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>405</th>\n      <td>1297</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>406</th>\n      <td>1298</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>407</th>\n      <td>1299</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>408</th>\n      <td>1300</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>409</th>\n      <td>1301</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>410</th>\n      <td>1302</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>411</th>\n      <td>1303</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>412</th>\n      <td>1304</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>413</th>\n      <td>1305</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>414</th>\n      <td>1306</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>415</th>\n      <td>1307</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>416</th>\n      <td>1308</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>417</th>\n      <td>1309</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>418 rows \u00d7 2 columns</p>\n</div>\n\n\n\n## \u7ed3\u679c\n\n\u4e0a\u8ff0baseline\u63d0\u4ea4\u5230kaggle\u540e\u51c6\u786e\u7387\u4e3a0.78468, \u7ed3\u679c\u8fd8\u4e0d\u662f\u5f88\u7406\u60f3\uff0c\u4e0d\u8fc7\u6539\u8fdb\u7684\u5730\u65b9\u8fd8\u662f\u6709\u633a\u591a\u7684\u3002\n\n\u5206\u6790\u7ed3\u679c\u53ef\u4ee5\u77e5\u9053\u6a21\u578b\u5b58\u5728\u4e86\u8fc7\u62df\u5408\uff0c\u63a5\u4e0b\u6765\u6211\u4eec\u8981\u505a\u7684\u5c31\u662f\u5bf9baseline\u8fdb\u884c\u53c2\u6570\u8fdb\u884c\u53cd\u590d\u8c03\u6574\u3002\n\n**No Free Lunch** \u5b9a\u7406\u544a\u8bc9\u6211\u4eec\u6ca1\u6709\u4e00\u4e2a\u6a21\u578b\u662f\u5305\u6253\u5929\u4e0b\u7684\uff0c\u53cd\u590d\u8c03\u53c2\u751a\u81f3\u66f4\u6362\u6a21\u578b\u662f\u5173\u952e\uff01\n"
 },
 {
  "repo": "chrisgschon/titanic-django",
  "language": "Jupyter Notebook",
  "readme_contents": "# \ud83d\udef3 titanic-django\n\nA simple example of how Django can be used to serve predictions through an API.\n\n# \u25b6\ufe0f Quickstart\n\n  \n\n## \ud83d\udc0d Create environment\n\n  Open up a terminal in the repositories root and run these commands to align your environment:\n\n``` conda create -n titanic-django ```\n\n  \n\n``` conda activate titanic-django ```\n\n  \n\n``` conda install pip ```\n\n``` pip install -r requirements.txt ```\n\n  \n## \ud83d\ude80 Run local Django server\n\nThis will host the API.\n\n- Open a terminal and navigate to the titanicapi project folder (first level, not the second). Then run:\n\n  \n\n``` conda activate titanic-django ```\n\n  \n\n(making sure you are executing from within the new environment)\n\n  \n\nNext run\n\n  \n\n``` python manage.py runserver ```\n\n  \n\nThis will run a local development server of the django app. The model's API is now ready to call!\n\n  \n\n## \ud83d\udcf2 Call the API\n\n  \n\n- Run demo.ipynb in the titanic-django conda environment once more (or any environment with requests and pandas installed)\n\n  \n\n- You should be getting results from your server!\n\n  \n\nThat's it! Congrats on your first Django model API!\n\n  \n  \n\n## \u26cf How to build a similar django project from scratch\n\n \n\n- Run the following in your desired project directory:\n\n  \n\n``` django-admin startproject titanicapi ```\n\n  \n\n``` python manage.py migrate ```\n\n  \n\n``` django-admin startapp api ```\n\n  \n\n- Add api to installed apps in main settings\n\n  \n\n- Add two files to api app directory:\n\n  \n\n1. functions.py\n\n- here you write your functions for loading the serialized model and classifying passengers\n\n2. urls.py\n\n- here you specify the URL on which your API will sit. This will allow you to send post requests with feature data to get predictions.\n\n  \n\n- Create APIView class in view.py with a post function to handle post requests. E.g.\n\n  \n\n```get_classification(APIView)```\n\n  \n\n- Add api urls to project root urls (titanicapi/urls.py) so that the main app knows that the api urls exist. E.g.\n\n  \n\n``` path('api/', include('api.urls')) ```\n\n- Try running your server to see if it likes your setup. Generally Django logs are very good.\n\n``` python manage.py runserver ```"
 },
 {
  "repo": "esskeetit0817/project-titanic",
  "language": "Jupyter Notebook",
  "readme_contents": "# project-titanic\n\u4f7f\u7528Numpy\u548cPandas\u5bf9\u6cf0\u5766\u5c3c\u514b\u6570\u636e\u96c6\u6570\u636e\u96c6\u8fdb\u884c\u5206\u6790\uff0c\u4ee5\u5f97\u51fa\u7ed3\u8bba\uff1a\u54ea\u4e9b\u56e0\u7d20\u8ba9\u751f\u8fd8\u7387\u66f4\u9ad8\u3002\n"
 },
 {
  "repo": "49nord/titanic-rs",
  "language": "Rust",
  "readme_contents": "# Nmea 0183 GGA Parser\n\nThis is a parser for the GGA sentence of the NMEA 0183 protocol.\n\nMore sentences may be added in the future, but are *not planned* at the moment.\nAccepts data as described [here](http://www.catb.org/gpsd/NMEA.html#_gga_global_positioning_system_fix_data).\n\n## Setup\n\nAdd this to `Cargo.toml`:\n\n```toml\n[dependencies]\ntitanic = \"0.1.0\"\n```\n\nThen put this in your crate root:\n\n```rust\nextern crate titanic;\n```\n\n## Usage\n\n[`GgaParser`](./parser/struct.GgaParser.html) can be used like an\n[`Iterator`](https://doc.rust-lang.org/std/iter/trait.Iterator.html).\nCalling `next` on `GgaParser` **blocks** until\n\n- it finds `'$'`,\n- reaches EOF\n- an I/O error occurs.\n\n`'$'` signals the beginning of a new sentence. If the new sentence is of the\ntype GGA, it will be parsed if possible. The parser iterates over\n`Result<GgaSentence, ParseError>`.\n\nEOF signals the end of the iterator.\n\n```rust\n# extern crate titanic;\n# use std::io::Cursor;\nuse titanic::GgaParser;\n\nlet data = Cursor::new(\"$GPGGA,142212.000,1956.9418,S,06938.0163,W,1,3,5.74,102.1,M,47.9,M,,*57\");\nlet parser = GgaParser::new(data).unwrap();\n\nfor gga in parser {\n    let gga = gga.unwrap();\n    println!(\n       \"Time: {}, we are here: {}\u00b0, {}\u00b0\",\n       gga.utc.format(\"%H:%M:%S\"),\n       gga.lat.unwrap(),\n       gga.long.unwrap()\n   );\n}\n// Prints \"Time: 14:22:12, we are here: -19.94903\u00b0, -69.633605\u00b0\"\n```"
 },
 {
  "repo": "ayooshkathuria/Kaggle-Titanic-Challenge",
  "language": "Python",
  "readme_contents": "### Kaggle Competition | Titanic Machine Learning from Disaster\n\n>The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew.  This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\n>One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.  Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\n>In this contest, we ask you to complete the analysis of what sorts of people were likely to survive.  In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n\n>This Kaggle Getting Started Competition provides an ideal starting place for people who may not have a lot of experience in data science and machine learning.\"\n\nFrom the competition [homepage](http://www.kaggle.com/c/titanic-gettingStarted).\n\nThe following contains my solution to the Titanic Challenge.\n"
 },
 {
  "repo": "Green-Wood/Kaggle-Titanic-ML",
  "language": "Python",
  "readme_contents": "**Titanic ML Kaggle**\n\nUsing python to solve and predict whether you would survive in Titanic"
 },
 {
  "repo": "ShauryaBhandari/Kaggle-Titanic-Dataset",
  "language": "Python",
  "readme_contents": "# Kaggle-Titanic-Dataset\nSolution to Kaggle's Titanic Dataset using various ML algorithms \nThe goal is to predict the survival or the death of a given passenger based on 12 feature such as sex, age, etc.  \n\n\n### Problem Statement:\nThis is a binary classification to detect the survival or death of a passenger onboard the Titanic. \nThe model predicts predicts the death or survival of a new passenger. \n\n### Introduction: \nRMS Titanic was a British passenger liner that sank in the North Atlantic Ocean in the early hours of 15 April 1912, after colliding with an iceberg during its maiden voyage from Southampton to New York City. There were an estimated 2,224 passengers and crew aboard, and more than 1,500 died, making it one of the deadliest commercial peacetime maritime disasters in modern history. RMS Titanic was the largest ship afloat at the time it entered service and was the second of three Olympic-class ocean liners operated by the White Star Line. \nIt was built by the Harland and Wolff shipyard in Belfast. \nThomas Andrews, her architect, died in the disaster.\n\n### Dataset:\nThe Titanic dataset can be downloaded from the Kaggle website which provides separate train and test data. \nThe train data consists of 891 entries and the test data 418 entries. It has a total of 12 features. \n\n### Exploratory data analysis:\n\nAs in different data projects, we'll first start diving into the data and build up our first intuitions.\nIn this section, we'll be doing four things.\n\nData extraction: We'll load the dataset and have a first look at it.\n\nCleaning: We'll fill in missing values.\n\nPlotting: We'll create some interesting charts that'll (hopefully) spot correlations and hidden insights out of the data.\n\nAssumptions: We'll formulate hypotheses from the charts.\n\n### Modelling:\n\nIn this part, we use our knowledge of the passengers based on the features we created and then build a statistical model. \nYou can think of this model as a box that crunches the information of any new passenger and decides whether or not he survives.\nA variety of ML algorithms were used and models created for SVM, KNN, Logistic Regression etc.\nSteps:\n1) Break the combined dataset in train set and test set.\n\n2) Use the train set to build a predictive model.\n\n3) Evaluate the model using the train set.\n\n4) Test the model using the test set and generate and output file and import it to another csv file (attached above).\n\n5) Compare the performance of various models and choose the best fit. In this case, Decision Tree model performed the best followed by KNN and SVM.\n\nThank you for visiting! \n\n"
 },
 {
  "repo": "yaswanthpalaghat/Titanic-Dataset-Logistic-Regression",
  "language": "Jupyter Notebook",
  "readme_contents": "# Titanic-Dataset-Logistic-Regression"
 },
 {
  "repo": "shubhammittal009/Kaggle_titanic_challenge",
  "language": "Jupyter Notebook",
  "readme_contents": "error: no README"
 },
 {
  "repo": "raninho/titanic_survival_exploration",
  "language": "HTML",
  "readme_contents": "# Project 0: Introduction and Fundamentals\n## Titanic Survival Exploration\n\n### Install\n\nThis project requires **Python 2.7** and the following Python libraries installed:\n\n- [NumPy](http://www.numpy.org/)\n- [Pandas](http://pandas.pydata.org)\n- [matplotlib](http://matplotlib.org/)\n- [scikit-learn](http://scikit-learn.org/stable/)\n\nYou will also need to have software installed to run and execute an [iPython Notebook](http://ipython.org/notebook.html)\n\nUdacity recommends our students install [Anaconda](https://www.continuum.io/downloads), a pre-packaged Python distribution that contains all of the necessary libraries and software for this project.\n\n### Code\n\nTemplate code is provided in the notebook `titanic_survival_exploration.ipynb` notebook file. Additional supporting code can be found in `titanic_visualizations.py`. While some code has already been implemented to get you started, you will need to implement additional functionality when requested to successfully complete the project.\n\n### Run\n\nIn a terminal or command window, navigate to the top-level project directory `titanic_survival_exploration/` (that contains this README) and run **one** of the following commands:\n\n```bash\njupyter notebook titanic_survival_exploration.ipynb\n```\nor\n```bash\nipython notebook titanic_survival_exploration.ipynb\n```\n\nThis will open the iPython Notebook software and project file in your web browser.\n\n## Data\n\nThe dataset used in this project is included as `titanic_data.csv`. This dataset is provided by Udacity and contains the following attributes:\n\n- `survival` : Survival (0 = No; 1 = Yes)\n- `pclass` : Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n- `name` : Name\n- `sex` : Sex\n- `age` : Age\n- `sibsp` : Number of Siblings/Spouses Aboard\n- `parch` : Number of Parents/Children Aboard\n- `ticket` : Ticket Number\n- `fare` : Passenger Fare\n- `cabin` : Cabin\n- `embarked` : Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n"
 },
 {
  "repo": "multivacplatform/multivac-kaggle-titanic",
  "language": "Scala",
  "readme_contents": "# Machine Learning from Disaster (Kaggle) \n[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/multivacplatform/multivac-kaggle-titanic/blob/master/LICENSE) [![Build Status](https://travis-ci.org/multivacplatform/multivac-kaggle-titanic.svg?branch=master)](https://travis-ci.org/multivacplatform/multivac-kaggle-titanic) [![Multivac Discuss](https://img.shields.io/badge/multivac-discuss-ff69b4.svg)](https://discourse.iscpif.fr/c/multivac) [![Multivac Channel](https://img.shields.io/badge/multivac-chat-ff69b4.svg)](https://chat.iscpif.fr/channel/multivac)\n\nThis repo is just for learning purposes to anyone who is new to Machine Learning by Apache Spark.\nhttps://www.kaggle.com/c/titanic\n\n## Environment and Tests\n* Scala 2.11.x\n* Apache Spark 2.2\n* Tests locally and in Cloudera (CDH 5.12)\n\n## How-To\n* sbt update\n* sbt \"run local\" - This runs the code on your local machine\n* sbt pacakge - to use the JAR by spark-submit \n* You can set ParamGrid values for cross validation inside ParamGridParameters.scala \n\n## Re-used Codes\n\n* [Exploring spark.ml with the Titanic Kaggle competition](https://benfradet.github.io/blog/2015/12/16/Exploring-spark.ml-with-the-Titanic-Kaggle-competition)\n* [Titanic: Machine Learning from Disaster (Kaggle)](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/19095846306138/45566022600459/8071950455163429/latest.html)\n\n## Code of Conduct\n\nThis, and all github.com/multivacplatform projects, are under the [Multivac Platform Open Source Code of Conduct](https://github.com/multivacplatform/code-of-conduct/blob/master/code-of-conduct.md). Additionally, see the [Typelevel Code of Conduct](http://typelevel.org/conduct) for specific examples of harassing behavior that are not tolerated.\n\n## Useful Links\n\n* [Building Classification model using Apache Spark](http://vishnuviswanath.com/spark_lr.html)\n* [Revisit Titanic Data using Apache Spark](https://6chaoran.wordpress.com/2016/08/13/__trashed/)\n* [Would You Survive the Titanic? A Guide to Machine Learning in Python](https://blog.socialcops.com/engineering/machine-learning-python/)\n\n## Copyright and License\n\nCode and documentation copyright (c) 2017-2019 [ISCPIF - CNRS](http://iscpif.fr). Code released under the [MIT license](https://github.com/multivacplatform/multivac-kaggle-titanic/blob/master/LICENSE).\n"
 },
 {
  "repo": "Donges-Niklas/Classification-Titanic-Dataset",
  "language": "Jupyter Notebook",
  "readme_contents": "# Binary Classification with the Titanic Dataset\nThis project deepened my machine learning knowledge significantly and I strengthened my ability to apply concepts that I learned from textbooks, blogs and various other sources, on a different type of problem. This project had a heavy focus on the data preparation part, since this is what data scientists work on most of their time.\n\nI started with the data exploration where I got a feeling for the dataset, checked about missing data and learned which features are important. During this process I used seaborn and matplotlib to do the visualizations. During the data preprocessing part, I computed missing values, converted features into numeric ones, grouped values into categories and created a few new features. Afterwards I started training 8 different machine learning models, picked one of them (random forest) and applied cross validation on it. Then I explained how random forest works, took a look at the importance it assigns to the different features and tuned it's performace through optimizing it's hyperparameter values. Lastly I took a look at it's confusion matrix and computed the models precision, recall and f-score, before submitting my predictions on the test-set to the Kaggle leaderboard.\n\nTo see the project, just open the Jupyter Notebook: [\"binary_classification_titanic.ipynb\"](https://github.com/Donges-Niklas/Classification-Titanic-Dataset/blob/master/binary_classification_titanic.ipynb).\n\n\n\n\n"
 },
 {
  "repo": "dragonet1/kaggle-titanic-ensemble-models",
  "language": "Jupyter Notebook",
  "readme_contents": "# kaggle-titanic, final submit is 0.80383. my code include many others ideas, i mentioned it in ./code/titanic.ipnb\n"
 },
 {
  "repo": "pankhurinagpal/Titanic-Crash-Analysis",
  "language": "Jupyter Notebook",
  "readme_contents": "# Titanic-Crash-Analysis\nI have split this project into 2 parts- performing an exploratory data analysis and building a logistic regression model to predict if a passenger would be able to survive a Titanic Crash.\n\n![titanic](https://unsplash.com/photos/ZjSdIwwcjgk/download?force=true)\n\n## Motivation\nEveryone knows about the huge unfornate incident of Titanic crash, we also have the dataset containing the information of all the passengers present during the crash readily available on the web. Why don't we simply use it to predict what kind of passengers were given high priority to rescue first? I wanted my first project on GitHub to be a really meaningful one and not just anything. I initially performed this whole piece of code in R (while I was learning) but then writing it on Python was a completely different experience.\n\n## Pre-requisites\n- Basics of Python programming (refer: https://www.pythonforbeginners.com/basics/break-and-continue-statements)\n- Basics of Machine Learning Algorithms, more specifically Logistic Regression Model (refer: https://www.datacamp.com/community/tutorials/understanding-logistic-regression-python)\n\n## Libraries used\n\n- numPy : for basic and high level algebra\n- pandas : for data manipulation and analysis\n- matplotlib.pyplot : for generating plots and histograms\n- seaborn : for high-level interface for drawing attractive and informative statistical graphics\n- warnings : to essentially ignore the warnings \n- sklearn : for performing logistic regression \n\n### Refer to my full code in the name: Titanic crash survival.ipynb - https://github.com/pankhurinagpal/Titanic-Crash-Analysis/blob/master/Titanic%20crash%20survival.ipynb\n\n### Supporting CSV file for data set:  titanic.csv - https://github.com/pankhurinagpal/Titanic-Crash-Analysis/blob/master/titanic.csv\n\nHappy coding!\n"
 },
 {
  "repo": "ankitgokhroo68368/Data-analytics-on-titanic-dataset",
  "language": "Jupyter Notebook",
  "readme_contents": "# Data-analytics-on-titanic-dataset\nData analysis and data visualisation titanic dataset and prepare a model by using logistic regression algorithm and predict the passenger survived or not in the tragedy.\nwe are given 2 datasets one is train dataset and another is test dataset.\nwe train the train dataset and than predict survival of people in test dataset.\nwe have certain columns in dataset :\nSibSp\t means is no. of sibbings,\nParch\t means no. of parents,\nEmbarked is  Port of Embarkation,\nPclass is person class in the ship,\n"
 },
 {
  "repo": "gittb/Titanic-Dataset-Full-Connected-Demo",
  "language": "Python",
  "readme_contents": "error: no README"
 },
 {
  "repo": "dnlsql/01_AnaliseDadosTitanic_Python",
  "language": "Jupyter Notebook",
  "readme_contents": "# An\u00e1lise dos Dados do Titanic\nO Conjunto de dados foi o \"titanic-data-6.csv\". Esse conjunto de dados tem por informa\u00e7\u00e3o alguns dados dos que foram sobrevivente ou mortos.<br />\n\nForam respondidas as seguintes perguntas:<br />\n\n1 - Entre as pessoas que sobreviveram e as pessoas mortas, quais tiveram um maior n\u00famero?<br />\n2 - Qual grupo por faixa et\u00e1ria, teve a maior taxa de sobreviv\u00eancia?<br />\n3 - Entre as classes que haviam no Titanic, qual sofreu maior perda?<br />\n4 - Por sexo, qual teve maior perda significativa?<br />\n\nBreve Descri\u00e7\u00e3o:<br />\n\nDepois de carregar os dados do csv, foi poss\u00edvel observar os dados e assim constatei que os campos (Sobrevivente, Idade, ClasseBilhete, Sexo), seriam ideais para identificar os tipos de pessoas que tenham morrido ou sobrevivido ao infeliz acidente.<br />\n\nLimpezas:<br />\n\nFoi verificado que o campo de ano contia valores nulos e com isso, todo valor nulo foi ajustado a m\u00e9dia entre eles. Havia tamb\u00e9m registros com o tipo float e o mesmo foi convertido para inteiro;<br />\n\nOs campos foram renomeados;<br />\n\nVerificado se existem registros duplicados;<br />\n\nUma nova coluna para foi criada para categorizar a fa\u00edxa et\u00e1ria;<br />\n\nO campo Sexo foi ajustado para male == M e female == F.<br />\n\nGr\u00e1ficos:<br />\n\nEstes foram feitos todos com gr\u00e1fico de barras e tentando ao m\u00e1ximo trazer ao analista, maior clareza em rela\u00e7\u00e3o aos dados utilizados e respondendo de forma simples e objetiva as perguntas que foram sugeridas.\n"
 },
 {
  "repo": "kheraAniruddh/Machine-learning-examples",
  "language": "Jupyter Notebook",
  "readme_contents": "# Machine-learning-stuff\nVarious usecases and models.\n\n1. Check out the famous # Titanic example\nReally good starting point on how feature engineering and selection really boost predictions and compare with cool models.\n\n2. Ada.m.py how I implemented from scratch Adaboost using coordinate descent on spam vs. ham data\n\n3. csr.ipynb demonostrates how to handle sparse data \n\n4. Salarypredictor\n\n5. implementing poly kernel svms and comparing Xvalidation accuracy spam vs. ham dataset\n"
 },
 {
  "repo": "ahmernajar/Titanic-Machine-Learning-from-Disaster-Kaggle-Competition",
  "language": "Python",
  "readme_contents": "# Titanic-Machine-Learning-from-Disaster-Kaggle-Competition\n\nThis is a famous Titanic Machine Learning problem on Kaggle\nGo through the train file to understand the how i solve this problem\nI have used to two algorithms Rnadom Forest and Decision Tree (you can use one of these) to get the best possible accuracy \n\n\n# DATASET \nDataset can be downloaded here\nhttps://www.kaggle.com/c/titanic\n\n\n# REQUIRMENTS\nNumpy,\nPandas,\nscikit learn,\nMatplotlib\n"
 },
 {
  "repo": "rajatsharma369007/Titanic_Survival_Exploration_with_Decision_Trees",
  "language": "Jupyter Notebook",
  "readme_contents": "# Titanic Survival Exploration with Decision Trees\n\n## Introduction\nIn this notebook, I have implemented a decision tree model that will help to predict the survival chances of a person, based on the features like passenger class, sex, age, place of embarkation etc.\n\n## Dataset\nHave a look at the dataset. [click here](https://github.com/rajatsharma369007/Titanic_Survival_Exploration_with_Decision_Trees/blob/master/titanic_data.csv)  \nBelow is the info. related to attributes of the dataset.  \n\n<img src=\"https://github.com/rajatsharma369007/Titanic_Survival_Exploration_with_Decision_Trees/blob/master/dataset_info.png\" width=\"700px\">\n\n## Installation\n* numpy  \n<code>conda install numpy</code>\n* pandas  \n<code>conda install pandas</code>\n* scikit-learn  \n<code>conda install -c anaconda scikit-learn</code>\n\n## Accuracy\nTrain Score = 0.8918  \nTest Score = 0.8435\n\n## Issue/Bug\nPlease open issues on github to report bugs or make feature requests.\n\n## Contribution\nIf you are interested in improving the code, please open an issue first to describe the task you are planning to do. For small fixes (a few lines of change) feel free to open pull requests directly.\n"
 },
 {
  "repo": "PaulYangSz/MyJavaLogisticRegression",
  "language": "Java",
  "readme_contents": "# MyJavaLogisticRegression\nLearn Logistic Regression and implemented by Java\n\n*This repository is used to practice learned knowledge about Java and Logistic Regression*\n## \u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7528\u5230\u7684\u8d44\u6599 \n- \u6570\u636e\u6316\u6398\u548c\u673a\u5668\u5b66\u4e60\u65b9\u9762\uff1a \n    - \u300a\u673a\u5668\u5b66\u4e60\u300b \u300a\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5\u300b \u300a\u673a\u5668\u5b66\u4e60\u5b9e\u6218\u300b  \n    - \u5bd2\u5c0f\u9633\u7684Blog[*\u903b\u8f91\u56de\u5f52\u521d\u6b65*](http://blog.csdn.net/han_xiaoyang/article/details/49123419) \n    - \u6d1e\u5ead\u5c0f\u54e5\u7684Blog[*Logistic\u56de\u5f52\u603b\u7ed3*](http://blog.csdn.net/dongtingzhizi/article/details/15962797)\n- Java\u5b66\u4e60\u65b9\u9762\uff1a \n    - \u300aHead First Java\u300b \u300aCore Java\u300b\n## \u4e0b\u9762\u5c06\u4ecb\u7ecd\u4e0b\u6574\u4e2a\u5de5\u7a0b\u7684\u5185\u5bb9\u3001\u601d\u8def\u4ee5\u53ca\u63a5\u4e0b\u6765\u672a\u5b8c\u6210\u7684\u4e8b\n### data\u6587\u4ef6\u5939 \n\u8fd9\u4e2a\u6587\u4ef6\u5939\u4e2d\u5b58\u653e\u4e86\u8bad\u7ec3\u6570\u636e\u4ee5\u53ca\u6d4b\u8bd5\u6570\u636e\u3002\u5176\u4e2d\uff1a\n- HxyLinearData1.txt\u548cHxyNonlinearData2.txt \n    - \u8fd9\u4e2a\u662f\u4ece[*\u903b\u8f91\u56de\u5f52\u521d\u6b65*](http://blog.csdn.net/han_xiaoyang/article/details/49123419) \u4e2d\u4e0b\u8f7d\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u540e\u9762\u5728Java\u4ee3\u7801\u4e2d\u4e5f\u5bf9\u8fd9\u4e2a\u8bad\u7ec3\u96c6\u505a\u4e86\u9884\u6d4b\u3002 \n- kaggle\u6587\u4ef6\u5939\n    - train.csv \u548c test.csv\u662f\u4ecekaggle\u7f51\u7ad9\u4e0a\u4e0b\u8f7d\u7684Titanic\u95ee\u9898\u7684\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\n    - HxyTitanicDataProc.py\u662f\u4ece\u5bd2\u5c0f\u9633\u7684Blog[*\u903b\u8f91\u56de\u5f52\u5e94\u7528\u4e4bKaggle\u6cf0\u5766\u5c3c\u514b\u4e4b\u707e*](http://blog.csdn.net/han_xiaoyang/article/details/49797143)\u4e2d\u63d0\u53d6\u7684\u5bf9train.csv\u7684\u6570\u636e\u7279\u5f81\u5de5\u7a0b\u5904\u7406Python\u4ee3\u7801\u3002*\uff08\u8fd9\u90e8\u5206\u540e\u7eed\u53ef\u4ee5\u8003\u8651\u7528Java\u5b9e\u73b0\u4e4b\uff09*\n### src\u6587\u4ef6\u5939 \n\u8fd9\u4e2a\u6587\u4ef6\u5939\u5305\u542b\u4e86\u6240\u6709\u903b\u8f91\u56de\u5f52\u7684Java\u4ee3\u7801\uff0c\u4e0b\u9762\u6309\u7167\u81ea\u5df1\u7684\u89e3\u51b3\u95ee\u9898\u601d\u8def\u6765\u9010\u4e2a\u4ecb\u7ecd\u5404\u4e2a\u6587\u4ef6\u3002\n#### Main.java\u7684\u76ee\u7684\u662f\u5b9e\u73b0\u4e00\u4e2a\u5165\u53e3\u53ef\u4ee5\u6309\u7167\u6b65\u9aa4\u6765\u5bf9Demo\u6570\u636e\u96c6\u8fdb\u884c\u5b66\u4e60\u548c\u9884\u6d4b\u3002\u5177\u4f53\u601d\u8def\u662f\uff1a\n- Step1 \u4f7f\u7528\u4e00\u4e2aFileHelper\u7c7b\u6765\u5b9e\u73b0csv\u683c\u5f0f\u6570\u636e\u7684\u8bfb\u53d6\u548c\u4fdd\u5b58\n- Step2 \u4f7f\u7528MapFunction\u7c7b\u6765\u6620\u5c04\u8bfb\u53d6\u6570\u636e\u5230\u4e00\u4e2a\u7ebf\u6027\u6216\u8005\u975e\u7ebf\u6027\u7684\u51fd\u6570\u4e0a\n- Step3 \u521b\u5efa\u4e00\u4e2aOptiMethod\uff0c\u7136\u540e\u7528\u4e0a\u9762\u7684MapFunction\u548c\u8fd9\u4e2aOptiMethod\u7ec4\u6210\u7684Model\u6765\u5bf9MapFunction\u7684\u53c2\u6570Theta[]\u8fdb\u884c\u4f18\u5316\uff0c\u6700\u7ec8\u751f\u6210\u4e00\u4e2a\u5b66\u4e60\u4e4b\u540e\u7684Model\n- Step4 \u4f7f\u7528\u8fd9\u4e2a\u8bad\u7ec3\u597d\u7684Model\u5bf9\u8bad\u7ec3\u96c6\u8fdb\u884c\u9884\u6d4b\u5f97\u5230\u6700\u7ec8\u7684\u7ed3\u679c\n#### FilerHelper.java\n\u8fd9\u4e2a\u7c7b\u662f\u7b2c\u4e00\u4e2a\u88ab\u5b9e\u73b0\u7684\uff0c\u76ee\u7684\u5c31\u662f\u4ececsv\u683c\u5f0f\u7684\u6587\u4ef6\u4e2d\u8bfb\u53d6\u6570\u636e\uff0c\u4fdd\u5b58\u5728\u81ea\u5df1\u7684ArrayList\u4e2d\u3002\n#### MapFunction.java\n\u8fd9\u4e2a\u7c7b\u5c06FileHelper\u4e2d\u7684\u4fdd\u5b58\u6570\u636e\uff0c\u8f6c\u5316\u4e3a\u81ea\u5df1\u7684X\u548cY\u6570\u7ec4\u4e2d\uff0c\u5e76\u6839\u636eX\u7684\u6700\u5927\u5e42\u503cdegree\u6620\u5c04\u51fa\u4e00\u7ef4\u7ebf\u6027\u6216\u8005\u591a\u7ef4\u975e\u7ebf\u6027\u7684\u51fd\u6570\uff0c\u5176\u4e2d**\u65b9\u6cd5genXjOfTheta(int[] xiPwerV, int curDegree, int curXIdx)** \u7684\u5b9e\u73b0\u8fc7\u7a0b\u6700\u4e3a\u6709\u610f\u601d\uff0c\u867d\u7136\u51fd\u6570\u7684\u6709\u6548\u884c\u53ea\u6709\u533a\u533a\u4e0d\u523010\u884c\uff0c\u4f46\u662f\u5374\u82b1\u4e86\u6211\u4e00\u665a\u4e0a\u7684\u65f6\u95f4\u624d\u5199\u51fa\u6765\u3002\u8fd9\u4e2a\u65b9\u6cd5\u7684\u76ee\u7684\u5c31\u662f\u6839\u636e\u9009\u53d6\u4f5c\u4e3a\u7279\u5f81\u7684X\u7684\u4e2a\u6570\u4ee5\u53cadegree\u7684\u503c\u6765\u751f\u6210\u4e00\u4e2a\u5168\u5c55\u5f00\u7684\u591a\u9879\u5f0f\u3002\u8ba9Theta[]\u4e0e\u8fd9\u4e2a\u591a\u9879\u5f0f\u5bf9\u5e94\u8d77\u6765\u3002\n#### LinearMapFunction.java\u548cNonlinearMapFunction.java \n\u8fd9\u4e24\u4e2a\u7c7b\u662f\u5bf9MapFunction\u62bd\u8c61\u7c7b\u7684\u7ee7\u627f\uff0c\u672c\u6765\u662f\u60f3\u5728\u8fd9\u4e24\u4e2a\u5404\u81ea\u7684\u5b50\u7c7b\u4e2d\u5b8c\u6210\u5404\u81ea\u7684\u5173\u952e\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u60f3\u5206\u6b65\u8d70\u5148\u505a\u4e2a\u7ebf\u6027\u7b80\u5355\u7684\u540e\u9762\u5728\u5bf9\u975e\u7ebf\u6027\u7684\u5b9e\u73b0\uff0c\u4f46\u662f\u540e\u9762\u5728\u771f\u6b63\u5b9e\u73b0\u4ee3\u7801\u7684\u65f6\u5019\u4e0d\u592a\u60f3\u628a\u7ebf\u6027\u7684\u7b80\u5355\u4ee3\u7801\u5355\u72ec\u5b9e\u73b0\u51fa\u6765\u4e86\uff0c\u5c31\u4e00\u53e3\u6c14\u5b9e\u73b0\u4e86**\u65b9\u6cd5genXjOfTheta**\uff0c\u6240\u4ee5\u73b0\u5728\u770b\u8d77\u6765Linear\u548cNonlinear\u8fd9\u4e24\u4e2a\u7c7b\u4e2d\u5404\u81ea\u5b9e\u73b0\u7684\u4e1c\u897f\u5df2\u7ecf\u6ca1\u6709\u4ec0\u4e48\u592a\u5927\u5dee\u522b\u4e86\u3002\u751a\u81f3\u53ef\u4ee5\u629b\u53bb\u4e0d\u8981\u4e86\u3002\u4e0d\u8fc7\u4e3a\u4e86\u5728Main\u4e2d\u770b\u8d77\u6765\u66f4\u597d\u770b\u4e00\u4e9b\uff0c\u6211\u8fd8\u662f\u5148\u4fdd\u7559\u4e0b\u6765\u4e86\u3002\n#### OptiMethod.java\n\u8fd9\u4e2a\u62bd\u8c61\u7c7b\u4e3b\u8981\u662f\u4f5c\u4e3a\u5176\u4ed6\u4f18\u5316\u65b9\u6cd5\u7684\u62bd\u8c61\u63a5\u53e3\n#### GradientDecent.java\n\u5b9e\u73b0\u4e86OptiMethod\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u6cd5\u6765\u6c42\u6700\u4f18\u89e3\u3002\n#### MyMathApi.java\n\u8fd9\u4e2a\u4e3b\u8981\u5b9e\u73b0\u4e86\u4e00\u4e9b\u9759\u6001\u7684\u6570\u5b66\u8ba1\u7b97\u516c\u5f0f\uff0c\u5305\u62ecSigmoid\u3001C(m, n)\u548c\u5bf9\u6570\u7ec4\u6c42\u548c\u7b49\u3002\u65b9\u4fbf\u5176\u4ed6\u7c7b\u4f7f\u7528\n#### LogiRegModel.java\n\u662f\u4f7f\u7528\u4e00\u4e2a\uff08\u7ebf\u6027\u6216\u975e\u7ebf\u6027\uff09\u7684MapFunction\u4ee5\u53ca\u5bf9\u8fd9\u4e2a\u51fd\u6570\u7684\u7cfb\u6570Theta[]\u6c42\u6700\u4f18\u89e3\u7684\u4e00\u4e2aOptiMethod\u6784\u5efa\u7ec4\u6210\u4e00\u4e2amodel\uff0c\u5728\u8c03\u7528startOpti\u540e\uff0c\u5f00\u59cb\u5bf9model\u7684\u53c2\u6570\u8fdb\u884c\u8bad\u7ec3\u3002\n#### TitanicMain.java \n\u8fd9\u4e2a\u662f\u53e6\u5916\u4e00\u4e2aMain\u5165\u53e3\uff0c\u7528\u6765\u5904\u7406Kaggle-Titanic\u7684\u9884\u6d4b\u95ee\u9898\uff0c\u4e0d\u8fc7train.csv\u7684\u6570\u636e\u7279\u5f81\u5904\u7406\u91c7\u7528\u7684\u662f\u522b\u4eba\u7684Python\u4ee3\u7801\u505a\u7684\u3002\u540e\u9762\u53ef\u4ee5\u8003\u8651\u7528Java\u5b9e\u73b0\u4e4b\u3002\n\n\u4e0a\u9762\u57fa\u672c\u4e0a\u5c31\u662f\u5bf91.0\u7248\u672c\u7684\u4e3b\u8981\u4ecb\u7ecd\u3002\u63a5\u4e0b\u6765\u8ba8\u8bba\u4e0b\u540e\u9762\u8981\u505a\u7684\u4e8b\u3002\n### \u63a5\u4e0b\u6765\u7684\u5185\u5bb9\n#### \u589e\u52a0\u5176\u4ed6\u4f18\u5316\u65b9\u6cd5\uff0c\u6bd4\u5982\u725b\u987f\u6cd5\uff0c\u62df\u725b\u987f\u6cd5\u7b49\n#### \u6570\u636e\u7279\u5f81\u5de5\u7a0b\u7684\u7422\u78e8\n#### \u5b66\u4e60Kaggle\u7684kernel\u548cdiscussion\n#### \u4f18\u5316Java\u4ee3\u7801\u5305\u62ec\u6ce8\u91ca\u7684\u6dfb\u52a0\u7b49\n#### \u8fd8\u6709\u4e00\u4e9b\u7761\u89c9\u524d\u60f3\u5230\u7684\uff0c\u4f46\u662f\u9192\u6765\u540e\u60f3\u4e0d\u8d77\u6765\u7684\u3002\u3002\u3002\n\n## V2.0\u7248\u672c\u7684\u66f4\u65b0\n\u4e0a\u9762\u662fV1.0\u7248\u672c\u7684\u4e3b\u8981\u5de5\u4f5c\u3002\u4e3b\u8981\u662f\u5b9e\u73b0\u4e86GD\u65b9\u6cd5\u7684LR\u3002\u540e\u9762\u5bf9\u4e00\u7ef4\u7ebf\u6027\u548c\u4e8c\u7ef4\u975e\u7ebf\u6027\u7684\u5f71\u5c04\u51fd\u6570\u90fd\u505a\u4e86\u5b9e\u9a8c\u3002\u5728\u6570\u636e\u7279\u5f81\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\uff0c\u53d1\u73b0\u6ca1\u6709\u592a\u591a\u7684\u533a\u522b\uff0c\u800c\u4e14\u8fd8\u662f\u7ebf\u6027\u7684\u901f\u5ea6\u5feb\u800c\u4e14\u5728Titanic\u95ee\u9898\u4e0a\u8868\u73b0\u7684\u66f4\u597d\u3002V2.0\u7248\u672c\u4e3b\u8981\u662f\u52a0\u5165\u4e86AbaBoost\u3002\n### AbaBoost\u4ecb\u7ecd\nAbaBoost\u662f\u4e00\u79cdensemble learning\u7b97\u6cd5\u3002\u7b80\u5355\u8bf4\u5c31\u662f\u4ea7\u751f\u4e00\u7cfb\u5217\u7684\u57fa\u5206\u7c7b\u6a21\u578b\uff0c\u53ca\u6bcf\u4e2a\u57fa\u6a21\u578b\u7684\u9884\u6d4b\u6743\u91cd\u7cfb\u6570\uff0c\u7136\u540e\u7efc\u5408\u51fa\u6700\u7ec8\u7684\u9884\u6d4b\u7ed3\u679c\u3002\u5176\u4e2d\u6bcf\u4e2a\u57fa\u6a21\u578b\u90fd\u662f\u4ece\u4e0a\u4e00\u4e2a\u57fa\u6a21\u578b\u7684\u7ed3\u679c\u4e2d\u91cd\u65b0\u8ba1\u7b97\u8bad\u7ec3\u96c6\u7684\u6743\u91cd\u5206\u5e03\u5f97\u5230\u3002\u4e5f\u5c31\u662f\u8bf4\u5bf9\u4e8e\u4e0a\u4e00\u6b21\u7684\u9884\u6d4b\uff0c\u8c03\u6574\u5347\u9ad8\u9884\u6d4b\u9519\u8bef\u7684\u90a3\u6761X_i\u7684\u8bad\u7ec3\u6743\u91cd\u7cfb\u6570\uff0c\u8ba9\u4e0b\u4e00\u6b21\u4ea7\u751f\u7684\u57fa\u6a21\u578b\u5bf9\u9519\u8bef\u6570\u636e\u7684\u8bad\u7ec3\u91cd\u70b9\u7167\u987e\u3002\n### AdaBoost\u5b9e\u73b0\n\u5bf9\u4e8eAdaBoost\u7b97\u6cd5\u7684\u5b9e\u73b0\u4e3b\u8981\u662f\u5728\u6587\u4ef6AdaBoostModel.java\u4e2d\u3002\u5728\u8fd9\u4e2a\u7c7b\u91cc\u9762\u5b9e\u73b0\u4e86\u57fa\u4e8e\u4e00\u4e2aLR\u6a21\u578b\u548c\u6700\u5927\u57fa\u6a21\u578b\u4e2a\u6570\u7684\u6784\u9020\u65b9\u6cd5\uff0c\u4ee5\u53ca\u4ece\u5747\u5300\u5206\u5e03\u5f00\u59cb\u8bad\u7ec3\u57fa\u6a21\u578b\uff0c\u9010\u6b65\u66f4\u65b0\u8bad\u7ec3\u96c6\u7684\u6bcf\u6761\u6570\u636e\u7684\u6743\u91cd\u5206\u5e03\uff0c\u518d\u7528\u66f4\u65b0\u6743\u91cd\u540e\u7684\u8bad\u7ec3\u96c6\u5b66\u4e60\u51fa\u4e0b\u4e00\u4e2a\u57fa\u6a21\u578b\uff0c\u76f4\u5230\u8fbe\u5230\u6700\u5927\u57fa\u6a21\u578b\u4e2a\u6570\u3002\u4e0d\u8fc7\u4e2d\u95f4\u52a0\u5165\u4e86\u4e00\u4e9b\u9650\u5236\uff0c\u6bd4\u5982\u9519\u8bef\u7387\u9ad8\u4e8e0.5\u65f6\u5c31\u505c\u6b62\u7ee7\u7eed\u4ea7\u751f\u57fa\u6a21\u578b\uff08\u5176\u5b9e\u4f4e\u4e8e0.5\u5c31\u662f\u8bf4\u8fd9\u4e2a\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u8981\u53cd\u7740\u7528\uff0c\u4e0d\u8fc7\u611f\u89c9\u610f\u4e49\u4e0d\u5927\uff09\uff0c\u800c\u4e14\u4e3a\u4e86\u8ba9\u57fa\u6a21\u578b\u7684\u9884\u6d4b\u6743\u91cd\u6709\u4e00\u5b9a\u7684\u5b9e\u9645\u610f\u4e49\uff0c\u5c3d\u91cf\u4f18\u5316\u81f3\u9519\u8bef\u7387\u4e0d\u9ad8\u4e8e0.45.\n\u5177\u4f53\u7684\u7b97\u6cd5\u6b65\u9aa4\u53c2\u8003\u300a\u7edf\u8ba1\u65b9\u6cd5\u5b66\u4e60\u300b\u4ee5\u53ca\u300a\u673a\u5668\u5b66\u4e60\u300b\u3002\n### \u8c08\u8c08\u5728Titanic\u4e0a\u7684\u7ed3\u679c\n\u6a21\u578b\u6700\u7ec8\u5728Titanic\u95ee\u9898\u4e0a\u5e76\u6ca1\u6709\u5e26\u6765\u4ec0\u4e48\u591a\u5927\u7684\u8fdb\u6b65\uff08\u751a\u81f3\u4e0d\u5982\u5355\u6b211000\u8fed\u4ee3\u7684LR\uff0c\u4e0d\u8fc7\u6bd4\u5355\u6b21100\u8fed\u4ee3\u7684\u8981\u597d\u3002\uff09\uff0c\u603b\u7ed3\u4e0b\u6765\u5c31\u662f\u8fd9\u4e2a\u6570\u636e\u7279\u5f81\u5df2\u7ecf\u88ab\u5355\u6b21\u7684LR\u5b66\u4e60\u5230\u6781\u9650\u4e86\u3002\u518d\u591a\u51e0\u6b21LR\u4e5f\u4e0d\u4f1a\u6709\u592a\u9ad8\u7684\u8fdb\u6b65\u3002\u5982\u679c\u60f3\u8981\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u90a3\u4e48\u4e00\u662f\u6539\u53d8\u6570\u636e\u7279\u5f81\u7684\u9009\u53d6\uff0c\u91cd\u5efa\u7279\u5f81\u5de5\u7a0b\u3002\u4e8c\u662f\u66f4\u6362\u5b66\u4e60\u65b9\u6cd5\u6765\u66ff\u4ee3LR\u3002\n"
 },
 {
  "repo": "felixagbavor/Logistic-Regression",
  "language": "Python",
  "readme_contents": "<banner><b>Logistic Regression</b></banner>\n<body>\n  <p>A complete impelementation of the Logistic regression classification algorithm</p>\n  <p>This algorithm is developed to solve Kaggle's Titanic problem using Logistic regression as the classifier.\nThe algorithm employs feature engineering by the mean-std method. </p>\n\n  <p>Dataset can be found <a href='https://www.kaggle.com/c/titanic/data'>here</a></p>\n</body>\n"
 },
 {
  "repo": "rebeccabilbro/titanic",
  "language": "Jupyter Notebook",
  "readme_contents": "# Titanic\nDownload, explore, and wrangle the Titanic passenger manifest dataset with an eye toward developing a predictive model for survival.\n\nThis tutorial is based on the Kaggle Competition,[\"Predicting Survival Aboard the Titanic\"](https://www.kaggle.com/c/titanic)\n\n![RMS Titanic , Ocean Liner, (1912)](https://github.com/rebeccabilbro/titanic/blob/master/images/Cd51-1000g.gif)\n_Licensed under CC BY-SA 3.0 via Wikimedia Commons: \"Cd51-1000g\" by Boris Lux_\n\n## STEP ONE: EXPLORATORY ANALYSIS\nStart by cloning this repository.\n\n__Anaconda users__: you should have everything you need, but _if_ you find you are missing anything, type this into the command line:\n\n    conda install -c https://conda.anaconda.org/blaze <package>\n\n__Others__: make sure the required libraries are installed by using:\n\n    pip install -r requirements.txt    \n\nThen look inside the data folder and open ```train.csv``` to check out the dataset we'll be exploring today.  \n\nTo start the lab, open up the iPython Notebook file: ```titanic_wrangling.ipynb```.\n\n\n### Things to think about\n1. How to explore a new dataset?\n2. What to look for in tabular data?\n3. What visualization tools can you use to help you explore?\n4. What is the end goal of data wrangling? Why are we even doing this?\n5. What to clean and how to clean it?\n\n\nSee also:     \n[Baby steps to performing exploratory analysis in Python](http://www.analyticsvidhya.com/blog/2014/08/baby-steps-python-performing-exploratory-analysis-python/)     \n[Data munging using Pandas](http://www.analyticsvidhya.com/blog/2014/09/data-munging-python-using-pandas-baby-steps-python/)\n\n\n## STEP TWO: MACHINE LEARNING FROM DISASTER\n_(You will do this portion in the Machine Learning course.)_      \n\nThe iPython Notebook for this class is called \"titanicML_workshop.ipynb.\" To get it, navigate in the command line to the titanic repository that you cloned for the last class, and try:\n\n    git stash\n    git pull origin master    \n\nIf you haven't already installed Scikit-learn, do that now.    \n\n__Anaconda users__: you already have Scikit-learn! _If_ you ever find you are missing anything, type this into the command line:\n\n    conda install -c https://conda.anaconda.org/blaze <package>\n\nEveryone else, make sure Scikit-learn is installed:\n\n__WINDOWS USERS__:\n\n    pip install -U scikit-learn\n\n__MAC OSX USERS__:\n\n\tpip install -U numpy scipy scikit-learn\n\n__LINUX w/ Python 2__: \t\n\n\tsudo apt-get install build-essential python-dev python-setuptools \\\n                     python-numpy python-scipy \\\n                     libatlas-dev libatlas3gf-base\n\tsudo apt-get install python-matplotlib\n\n__LINUX w/ Python 3__:\n\n \tsudo apt-get install build-essential python3-dev python3-setuptools \\\n \t\t\t\t\t python3-numpy python3-scipy \\\n                     libatlas-dev libatlas3gf-base\n    sudo apt-get install python-matplotlib\n\n\nProblems with installation? Check out: http://scikit-learn.org/stable/install.html\n\nIf you get hung up with the installation or the repo update, you can also get the gist:     https://gist.github.com/rebeccabilbro/d40599f4ec96aa21dc48\n\n\n### Key Concepts    \n__Machine Learning__    \n\n__Classification__    \n\n__Cross-Validation__    \nhttp://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html\n\n__Model Evaluation__    \n    -Scores    \n    -Classification reports     \n    -Visualization tools     \n    -Precision recall           \n\n### Key Tools in Scikit-Learn    \n__Linear Regression__    \nhttp://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n\n__Random Forests__    \nhttp://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n\n__Support Vector Machines__    \nhttp://scikit-learn.org/stable/modules/svm.html\n\n### Sources\nThis tutorial is based on the following tutorials for Kaggle's titanic competition:\n       https://www.kaggle.com/mlchang/titanic/logistic-model-using-scikit-learn/run/91385\n       https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests\n       https://github.com/savarin/pyconuk-introtutorial/tree/master/notebooks\n"
 },
 {
  "repo": "mdimercurio/TitanicR",
  "language": "R",
  "readme_contents": "TitanicR\n========\n\nThis project will be presented during MTL Data's Hangout 'Demystifying Machine Learning' on September 25th, 2014\n\nIt is a quick introduction to predictive analytics using the example of Kaggle's Titanic challenge. The current version of the code includes a logistic regression and a decision tree.\n\nKaggle Challenge: Titanic - Machine Learning from a Disaster \n\n[Link to the Kaggle Competition's page](http://www.kaggle.com/c/titanic-gettingStarted)\n\nFrom Trevor's blog:\n\n[Getting started with R](http://trevorstephens.com/post/72916401642/titanic-getting-started-with-r)\n\n[Feature exploration](http://trevorstephens.com/post/72920580937/titanic-getting-started-with-r-part-2-the)\n\n[Logistic Regression in R](http://data.princeton.edu/R/glms.html)\n\nGetting started with R\n========\n[Install R](http://cran.at.r-project.org/)\n\n[Install R-Studio](http://www.rstudio.com/products/rstudio/download/)\n"
 },
 {
  "repo": "aruizga7/TitanicShinyApplication",
  "language": "R",
  "readme_contents": "error: no README"
 },
 {
  "repo": "ramp-kits/titanic",
  "language": "Jupyter Notebook",
  "readme_contents": "# RAMP starting kit on the Titanic dataset\n\nAuthors: Alexandre Gramfort & Balazs Kegl\n\n[![Build Status](https://travis-ci.org/ramp-kits/titanic.svg?branch=master)](https://travis-ci.org/ramp-kits/titanic)\n\nGo to [`ramp-worflow`](https://github.com/paris-saclay-cds/ramp-workflow) for more help on the [RAMP](http:www.ramp.studio) ecosystem.\n\nInstall ramp-workflow (rampwf), then execute\n\n```\nramp_test_submission\n```\n\nto test the starting kit submission (`submissions/starting_kit`) and\n\n```\nramp_test_submission --submission=random_forest_20_5\n```\n\nto test `random_forest_20_5` or any other submission in `submissions`.\n\nGet started on this RAMP with the [dedicated notebook](titanic_starting_kit.ipynb).\n"
 },
 {
  "repo": "rahulravindran0108/Titanic-Survival-Exploraton",
  "language": "HTML",
  "readme_contents": "# Titanic-Survival-Exploration\n\nThis repository contains project file for Project 0 - Titanic Survival Exploration as part of Udacity's Machine Learning Nanodegree.\n\n## Description\n\nIn this optional project, you will create decision functions that attempt to predict survival outcomes from the 1912 Titanic disaster based on each passenger\u2019s features, such as sex and age. You will start with a simple algorithm and increase its complexity until you are able to accurately predict the outcomes for at least 80% of the passengers in the provided data. This project will introduce you to some of the concepts of machine learning as you start the Nanodegree program.\n"
 },
 {
  "repo": "eddelbuettel/data-examples",
  "language": "R",
  "readme_contents": "error: no README"
 },
 {
  "repo": "jamessdixon/Kaggle.Titanic",
  "language": "F#",
  "readme_contents": "# Kaggle.Titanic\nF# implementation of Kaggle's Titanic Challenge.  Designed to be a dojo\n\nUsed FSharp and numl\n"
 },
 {
  "repo": "jasonicarter/DAND_Titanic_Data_Analysis",
  "language": "Jupyter Notebook",
  "readme_contents": "# Udacity\n## Data Analyst Nanodegree\n\" **Data Scientist was rated by Glassdoor as the # 1 Best Job in America for 2016.\nJoin 1,606 other students in our Data Analyst Nanodegree!**\n\nWe built this program with expert analysts and scientists at leading technology companies\nto ensure you master the exact skills necessary to build a career in data science.\n\nLearn to clean up messy data, uncover patterns and insights, make predictions using machine learning,\nand clearly communicate critical findings. \"\n\n**Co-Created by:**\n- facebook\n- mongoDB\n- Zipfian Academy\n\n## Projects\n\n### [Test a Perceptual Phenomenon](https://github.com/jasonicarter/DAND_Stroop_Effect)\n\nUse descriptive statistics and a statistical test to analyze the Stroop effect,\na classic result of experimental psychology. Give your readers a good intuition for the data\nand use statistical inference to draw a conclusion based on the results.\n\n> Well done by showing the key test statistics correctly step by step. Conclusion drawn is well aligned wit the test result.\n>\n> -- [Udacity Review](https://github.com/jasonicarter/DAND_Stroop_Effect/blob/master/UdacityReview.pdf)\n\n### [Investigate a Dataset](https://github.com/jasonicarter/DAND_Titanic_Data_Analysis)\n\nChoose one of Udacity's curated datasets and investigate it using NumPy and Pandas.\nGo through the entire data analysis process, starting by posing a question and finishing by sharing your findings.\n\n> You have postulated several meaningful questions and have provided a thoughtful investigation in turn. Asking the right questions is, arguably, the most important part in data analysis. Well done!\n>\n> Truly outstanding work on the visualizations. The plots are varied and intuitively make sense with the idea being portrayed.\n>\n> -- [Udacity Review](https://github.com/jasonicarter/DAND_Titanic_Data_Analysis/blob/master/UdacityReview.pdf)\n\n### [Wrangle OpenStreetMap Data](https://github.com/jasonicarter/DAND_OpenStreetMap_Data_MongoDB)\n\nChoose any area of the world in [OpenStreeMap](https://www.openstreetmap.org) and use data munging techniques,\nsuch as assessing the quality of the data for validity, accuracy, completeness, consistency and uniformity,\nto clean the OpenStreetMap data for a part of the world that you care about.\n\n> An outstanding work demonstrating a variety of auditing and cleaning procedures.\n>\n> I must say, terrific work on the formatting and presentation of this project. The information is concisely provided and presented in an aesthetically pleasing manner without overwhelming the reader with large blocks of code and unnecessary detail.\n>\n> -- [Udacity Review](https://github.com/jasonicarter/DAND_OpenStreetMap_Data_MongoDB/blob/master/UdacityReviews.pdf)\n\n### [Explore and Summarize Data](https://github.com/jasonicarter/DAND_Prosper_Loan_Data_EDA_R)\n\nUse R and apply exploratory data analysis techniques to explore relationships in one variable to multiple variables\nand to explore a selected data set for distributions, outliers, and anomalies.\n\n> Awesome work! There isn't much I can suggest to improve this project further.\n>\n> -- [Udacity Review](https://github.com/jasonicarter/DAND_Prosper_Loan_Data_EDA_R/blob/master/UdacityReview.pdf)\n\n### [Identify Fraud from Enron Email](https://github.com/jasonicarter/DAND_Enron_Email_Data_Machine_Learning)\n\nPlay detective and put your machine learning skills to use by building an algorithm to identify Enron Employees\nwho may have committed fraud based on the public Enron financial and email dataset.\n\n### [Make Effective Data Visualization](https://github.com/jasonicarter/DAND_Data_Visualization_D3)\n\nCreate a data visualization from a data set that tells a story or highlights trends or patterns in the data.\nUse either dimple.js or d3.js to create the visualization. Your work should be a reflection of the theory\nand practice of data visualization, such as visual encodings, design principles, and effective communication.\n\n> This is a very slick chart design. Nice job finding a way to concentrate a lot of information into a compact, clean and readable space. I really like the introductory information below the chart title as well as how the neighborhoods were sorted by housing population ratio. The neighborhood map also ads a nice exploratory element to the chart.\n\n> -- [Udacity Review](https://github.com/jasonicarter/DAND_Data_Visualization_D3/blob/master/UdacityReviews.pdf)\n\n### [Design an A/B Test](https://github.com/jasonicarter/DAND_AB_Testing)\n\nMake design decisions for an A/B test, including which metrics to measure and how long the test should be run.\nAnalyze the results of an A/B test that was run by Udacity and recommend whether or not to launch the change.\n"
 },
 {
  "repo": "FunXExcel/Kaggle_Titanic_R",
  "language": "R",
  "readme_contents": "It's very easy to make some words **bold** and other words *italic* with Markdown. You can even [link to Google!](http://google.com)\n"
 },
 {
  "repo": "vikramvinay/Titanic-Machine-Learning-from-Disaster",
  "language": "Jupyter Notebook",
  "readme_contents": "# Titanic-Machine-Learning-from-Disaster\n\n![](http://media.giphy.com/media/1Nk9bIidJVTy0/giphy.gif)\n\n\n\nExplains how to begin with kaggle very first time .\n\n\n##Overview\nThis is the code for the one who just begin with kaggle's Titanic competition.This basically explains all the basic concept ,\nHow to approach the any competition over kaggle and how to start with it.\n\n##Dependencies\n* Classfication Algorithms\n* Numpy\n* Pandas\n\n##Usage\n\nRun this using [jupyter notebook](http://jupyter.readthedocs.io/en/latest/install.html). Just type `jupyter notebook` in the main directory and the code will pop up in a browser window. \n\n##Result\nThe output of the code is to predict the survival(0-Die,1-Survive) of the passenger who borded on the Titanic.\n\n"
 },
 {
  "repo": "zifeo/Titanic",
  "language": "Jupyter Notebook",
  "readme_contents": "# Titanic\n\nBuilding on the top of recent advances in the field of signal processing on graphs (Schuman et al., 2013) and deep learning on irregular domains (Bronstein et al., 2017), we investigate the performance of standard machine learning methods and the relevance of graph based convolutional neural networks to perform binary classification in this specific case (layered data). The new method provide a convenient way of getting rotational invariance over the data (Defferrard et al., 2017) and set up a flexible framework for structured pooling.\n\nSee the [project report](./project.ipynb) (notebook).\n\n## Getting started\n\n```shell\n7z e data.7z \npip3 install -r requirements.txt\njupyter notebook\n# or\njupyter lab\n```\n\n## References\n\n- TORRES, Ramon, SNOEIJ, Paul, GEUDTNER, Dirk, et al. GMES Sentinel-1 mission. Remote Sensing of Environment, 2012, vol. 120, p. 9-24.\n- SHUMAN, David I., NARANG, Sunil K., FROSSARD, Pascal, et al. The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. IEEE Signal Processing Magazine, 2013, vol. 30, no 3, p. 83-98.\n- BRONSTEIN, Michael M., BRUNA, Joan, LECUN, Yann, et al. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 2017, vol. 34, no 4, p. 18-42.\n- DEFFERRARD, Micha\u00ebl, BRESSON, Xavier, et VANDERGHEYNST, Pierre. Convolutional neural networks on graphs with fast localized spectral filtering. In : Advances in Neural Information Processing Systems. 2016. p. 3844-3852.\n- NGUYEN Ha Q., DO Minh N, et al. Downsampling of Signal on Graphs Via Maximum Spanning Trees. IEEE Transactions on Signal Processing, 2015, vol. 63, no 1.\n- DORFLER Florain, BULLO Francesco. Kron reduction of graphs with applications to electrical networks. 2011.\n\n## License\n\nThe project is licensed under MIT. Datasets follow their respective licensing schemes and are not assimilated to the processing. Some functions are courtesy of [Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering](https://github.com/mdeff/cnn_graph) (MIT) or adapted from [PyGSP: Graph Signal Processing in Python](https://github.com/epfl-lts2/pygsp/) (BSD).\n"
 },
 {
  "repo": "codelibra/kaggle-titanic",
  "language": "Jupyter Notebook",
  "readme_contents": "# Kaggle titanic\n\nTrying out first [Kaggle](https://www.kaggle.com/) dataset exploration.\n\n# References\n\n* [feature engineering](http://ahmedbesbes.com/how-to-score-08134-in-titanic-kaggle-challenge.html) .\n* [Blending](https://github.com/emanuele/kaggle_pbr/blob/master/blend.py)\n* [Stacking](http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/)\n* [Ensembling](http://mlwave.com/kaggle-ensembling-guide/)\n* [Voting](https://www.kaggle.com/poonaml/titanic/titanic-survival-prediction-end-to-end-ml-pipeline)\n* [Stacking/Ensembling guide](https://www.kaggle.com/shivendra91/titanic/introduction-to-ensembling-stacking-in-python/editnb)\n* [Random Forest](https://www.kaggle.com/benhamner/titanic/random-forest-benchmark-r/code)\nLot more of them....\n\n# Problem Overview\nWe need to predict for each testing data whether the person was able to survive or not based on the model learnt from training data.\n\n## VARIABLE DESCRIPTIONS\n1. survival        Survival (0 = No; 1 = Yes)\n2. pclass          Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n3. name            Name\n4. sex             Sex\n5. age             Age\n6. sibsp           Number of Siblings/Spouses Aboard\n7. parch           Number of Parents/Children Aboard\n8. ticket          Ticket Number\n9. fare            Passenger Fare\n10. cabin           Cabin\n11. embarked        Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n\n## SPECIAL NOTES\n1. Pclass is a proxy for socio-economic status (SES)\n 1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower\n\n2. Age is in Years; Fractional if Age less than One (1)\n If the Age is Estimated, it is in the form xx.5\n\n3. With respect to the family relation variables (i.e. sibsp and parch)\nsome relations were ignored.  The following are the definitions used\nfor sibsp and parch.\n\n Sibling:  Brother, Sister, Stepbrother, or Stepsister of Passenger Aboard Titanic\n Spouse:   Husband or Wife of Passenger Aboard Titanic (Mistresses and Fiances Ignored)\n Parent:   Mother or Father of Passenger Aboard Titanic\n Child:    Son, Daughter, Stepson, or Stepdaughter of Passenger Aboard Titanic\n\n4. Other family relatives excluded from this study include cousins,\nnephews/nieces, aunts/uncles, and in-laws.  \n\n5. Some children travelled\nonly with a nanny, therefore parch=0 for them.  As well, some\ntravelled with very close friends or neighbors in a village, however,\nthe definitions do not support such relations.\n"
 },
 {
  "repo": "mathias-brandewinder/Kaggle-Titanic",
  "language": "F#",
  "readme_contents": "Kaggle-Titanic\n==============\n\nMy F# experimentations with the Kaggle Titanic \"Getting Started\" competition: \nhttp://www.kaggle.com/c/titanic-gettingStarted\n"
 },
 {
  "repo": "krishnamurthy-a/Titanic-Survival-Prediction",
  "language": "Jupyter Notebook",
  "readme_contents": "error: no README"
 },
 {
  "repo": "aanchan/Titanic-MTLDATA",
  "language": "Python",
  "readme_contents": "Titanic-MTLDATA\n===============\n\nThe goal of this IPython Notebook is to introduce some tools in Python for data-processing and machine learning. It is my no means exhaustive in any of the aspects of either Python, Machine Learning, Data-processing in Python or any of their permutations. The Machine Learning classifier used in this Notebook is one of the simplest classifiers called Logistic Regression models. The data-set on which the examples are run are taken from the Kaggle Titanic Challenge. This example was specifically chosen since there are many tutorials, IPython Notebooks, articles blogs and resources online on the Titanic Challenge that would help one get started. \n\nTo view this notebook online click on this [this link](http://nbviewer.ipython.org/github/aanchan/Titanic-MTLDATA/blob/master/TitanicPythonIntro.ipynb). This IPython notebook above assumes some facility in working with Python.\n\nInstalling tools required for this Notebook to run on your system.\n----\n\t1. Python\n\t2. IPython (Optional since you could run the Python commands from the IPython \n\t   \t   notebook on your native Python interpreter)\n\t3. Numpy\n\t4. Scipy\n\t5. Pandas\n\t6. Scikit-Learn\n\nInstallation methods vary depending on the Operating System. [Here](http://blog.yhathq.com/posts/setting-up-scientific-python.html) is a great link on completing a setup in Python for scientific purposes. \n\nBelow are pointers to some resources that might help one get started off. \n\n\nThe Kaggle Titanic Challenge\n----\nRead about it [here](https://www.kaggle.com/c/titanic-gettingStarted)\n\nIntroduction to Logistic Regression\n----\n[A Simple Explanation from Duke Medicine](https://www.youtube.com/watch?v=_Po-xZJflPM)\n\n[Logistic Regression for Classification](http://youtu.be/nMcxOiVj7oE)\n\nIntroduction to Python\n----\n[Course from Coursera](https://www.coursera.org/course/interactivepython). This does not require one to download and install Python. They have a version for the course that runs off the browser interactively.\n\n[The best intro I think, from Python Docs](https://docs.python.org/2/tutorial/introduction.html)\n\nIntroduction to the Numpy module in Python\n----\n[The Tentative Numpy Tutorial](http://wiki.scipy.org/Tentative_NumPy_Tutorial) is a good place to start.\n\nIntroduction to Scikit-Learn(referred to in this document also as SKLearn)\n----\nHere is a great introduction on Machine Learning with Scikit-Learn. Its a tutorial from [PyCon 2014](https://www.youtube.com/watch?v=HjAB45qsx_c).\n\nIntroduction to Pandas\n---\nThe [Python Pandas Cookbook Lecture Series](http://www.youtube.com/playlist?list=PLyBBc46Y6aAz54aOUgKXXyTcEmpMisAq3) on Youtube by Alfred Essa is a good place to start. Specifically to load our Titanic data set Alfred Essa talks about it [here](https://www.youtube.com/watch?v=lhkchS9gSYk#t=545) in Lesson 1.2.\n\nOther useful resources\n----\n[A tutorial from Kaggle on Python](https://www.kaggle.com/c/titanic-gettingStarted/details/getting-started-with-python)\n\n[A tutorial from Kaggle on Pandas](https://www.kaggle.com/c/titanic-gettingStarted/details/getting-started-with-python-ii)\n\n[A tutorial from Kaggle on SKLearn](https://www.kaggle.com/c/titanic-gettingStarted/details/getting-started-with-random-forests)\n\n[An SKLearn Notebook](http://nbviewer.ipython.org/github/cs109/content/blob/master/labs/lab4/Lab4full.ipynb)\n\n[A Fancy Notebook](http://nbviewer.ipython.org/github/agconti/kaggle-titanic/blob/master/Titanic.ipynb) showing off many aspects of the Titanic data problem"
 },
 {
  "repo": "gkcs/titanic",
  "language": "HTML",
  "readme_contents": "# titanic\n"
 },
 {
  "repo": "seosoft/Titanic_MachineLearningStudio",
  "language": null,
  "readme_contents": "# Azure Machine Learning Studio \u3067\u4e8c\u9805\u5206\u985e \uff5e \u30bf\u30a4\u30bf\u30cb\u30c3\u30af\u53f7\u304b\u3089\u8131\u51fa\u3067\u304d\u308b\u304b\uff1f\n\n\u3053\u306e\u30b3\u30f3\u30c6\u30f3\u30c4\u3067\u306f\u3001[**Azure Machine Learning Studio**](https://studio.azureml.net/) \u3092\u4f7f\u3063\u3066 **\u4e8c\u9805\u5206\u985e** \u3092\u884c\u3046\u624b\u9806\u3092\u7d39\u4ecb\u3057\u307e\u3059\u3002\n\n\u3053\u306e\u30b3\u30f3\u30c6\u30f3\u30c4\u306e\u30c6\u30fc\u30de\u306f\u3001[**\u30bf\u30a4\u30bf\u30cb\u30c3\u30af\u53f7\u306e\u4e57\u8239\u30ea\u30b9\u30c8**](https://www.kaggle.com/c/titanic/) \u304b\u3089\u3001\u3069\u306e\u3088\u3046\u306a\u5c5e\u6027\u306e\u4eba\u304c **\"\u52a9\u304b\u308b\" \u304b\u3092\u4e88\u6e2c** \u3059\u308b\u3053\u3068\u3067\u3059\u3002\n\n![Azure Machine Learning Studio](./images/msstudio_experiment.jpg)\n\n---\n\n\u30af\u30e9\u30a6\u30c9\u3084 PC \u306e\u6027\u80fd\u304c\u5411\u4e0a\u3057\u3066\u3001\u8a00\u8a9e\u3084\u30c4\u30fc\u30eb\u304c\u9032\u6b69\u3057\u305f\u3053\u3068\u304b\u3089\u3001**\u6df1\u5c64\u5b66\u7fd2** \u304c\u666e\u53ca\u3057\u3066\u304d\u307e\u3057\u305f\u3002\n\n\u3057\u304b\u3057\u3001**\u69cb\u9020\u5316\u30c7\u30fc\u30bf**\uff08Excel \u3084 CSV \u306e\u3088\u3046\u306a\u8868\u5f62\u5f0f\u306e\u69cb\u9020\u3092\u6301\u3064\u30c7\u30fc\u30bf\uff09\u3092\u5b66\u7fd2\u3059\u308b\u5834\u5408\u306f\u3001\u3080\u3057\u308d\u6df1\u5c64\u5b66\u7fd2\u3067\u306f\u306a\u3044 **\u6a5f\u68b0\u5b66\u7fd2** \u306e\u307b\u3046\u304c\u9069\u3057\u3066\u3044\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002  \nAI \u6280\u8853\u3092\u7406\u89e3\u3059\u308b\u4e0a\u3067\u3082\u3001\u30b7\u30f3\u30d7\u30eb\u306a\u6a5f\u68b0\u5b66\u7fd2\u306e\u307b\u3046\u304c\u76f4\u611f\u7684\u3067\u7406\u89e3\u3057\u3084\u3059\u3044\u3067\u3057\u3087\u3046\u3002\u521d\u5fc3\u8005\u30fb\u5165\u9580\u8005\u306f\u307e\u305a\u6a5f\u68b0\u5b66\u7fd2\u3067\u611f\u899a\u3092\u3064\u304b\u307f\u3001\u3042\u3068\u3067\u6df1\u5c64\u5b66\u7fd2\u306b\u9032\u3080\u3068\u30b9\u30e0\u30fc\u30ba\u306b\u9032\u3081\u3089\u308c\u305d\u3046\u3067\u3059\u3002\n\n---\n\n\u4eca\u56de\u306f\u3001\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u5358\u306b **\u5b66\u7fd2** \u3059\u308b\u3060\u3051\u3067\u306f\u306a\u304f\u3001\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092 **\u30af\u30e9\u30a6\u30c9\u306b\u767a\u884c** \u3057\u3066\u3001\u305d\u306e\u30b5\u30fc\u30d3\u30b9\u3092 **\u5229\u7528** \u3059\u308b\u624b\u9806\u307e\u3067\u7d39\u4ecb\u3057\u307e\u3059\u3002\n\n\u958b\u767a\u306b\u306f [Azure Machine Learning Studio](https://studio.azureml.net/) \u3092\u4f7f\u7528\u3057\u307e\u3059\u3002  \n\n\u6a5f\u68b0\u5b66\u7fd2\uff08\u6df1\u5c64\u5b66\u7fd2\u3067\u3082\u57fa\u672c\u7684\u306b\u306f\u540c\u69d8\uff09\u306f\u3001\u4ee5\u4e0b\u306e\u6d41\u308c\u3067\u9032\u3081\u307e\u3059\u3002  \n\n1. [\u30c7\u30fc\u30bf\u3092\u7528\u610f\u3059\u308b](./01_preparedata.md)\n2. [\u30c7\u30fc\u30bf\u3092\u5206\u6790\u3059\u308b](./02_dataanalyze.md)\n3. [\u30c7\u30fc\u30bf\u3092\u6574\u5f62\u3059\u308b](./03_dataformat.md)\n4. [\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3059\u308b](./04_createmodel.md)\n5. [\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092\u8a55\u4fa1\u3059\u308b](./05_evaluatemodel.md)\n6. [\u4e88\u6e2c\u30e2\u30c7\u30eb\uff08\u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\uff09\u3092\u767a\u884c\u3059\u308b](./06_deploymodel.md)\n7. [\u767a\u884c\u3057\u305f\u30b5\u30fc\u30d3\u30b9\u3092\u5229\u7528\u3059\u308b](./07_requestservice.md)\n\n> \u3053\u306e\u30b3\u30f3\u30c6\u30f3\u30c4\u3067\u306f\u3001Azure Machine Learning Studio \u306e **FREE \u30ec\u30d9\u30eb** \u3092\u5229\u7528\u3057\u307e\u3059\u3002  \nAzure Machine Learning Studio \u306e STANDARD \u30ec\u30d9\u30eb\uff08\u6709\u511f\uff09\u3092\u5229\u7528\u3059\u308b\u3068\u3001\u8907\u96d1\u306a\u30e2\u30c7\u30eb\u3092\u958b\u767a\u3057\u305f\u308a\u9ad8\u901f\u306b\u5b66\u7fd2\u3057\u305f\u308a\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002Azure Machine Learning Studio \u306e\u30ec\u30d9\u30eb\u306e\u5dee\u306f [\u3053\u3061\u3089](https://azure.microsoft.com/ja-jp/pricing/details/machine-learning-studio/) \u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002  \nSTANDARD \u30ec\u30d9\u30eb\u3067\u3053\u306e\u30b3\u30f3\u30c6\u30f3\u30c4\u3092\u5229\u7528\u3057\u305f\u3044\u5834\u5408\u306f\u3001 [STANDARD \u30ec\u30d9\u30eb\u306e Workspace \u4f5c\u6210\u624b\u9806](./a01_createworkspace.md) \u3067 Workspace \u3092\u4f5c\u6210\u3057\u3066\u304b\u3089\u3001\u4e0a\u8a18\u306e\u624b\u9806\u3067\u9032\u3081\u3066\u304f\u3060\u3055\u3044\u3002\n\n---\n\n\u9593\u9055\u3044\u3084\u66f4\u65b0\u306e\u63d0\u6848\u306f\u3001Issue\u3001Pull Request \u3067\u304a\u77e5\u3089\u305b\u304f\u3060\u3055\u3044\u3002\n"
 },
 {
  "repo": "niklaslong/titanic-data-viz",
  "language": "JavaScript",
  "readme_contents": "# _Titanic Data Visualization_\n\n#### _This app demonstrates simple d3 javascript data visualizations using graphical visualizations of information with 2.5D presentations, Wednesday, June 20, 2017_\n\n#### By _**Niklas Long, James Higgins, Nick Powell, Sara Schultz**_\n\n## Description\n\n_This web app provides a user driven graphical representation of data about passengers who were on the sunken Titanic vessel, sunk in 1912. The data represents 1,310 passengers of the 3,335 total passengers and crew onboard._\n\n## Data Source\n\n_Download Titanic passenger dataset: http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.xls_\n\n## Setup/Installation Requirements\n\n### Access Atom Files\n\n* _Open terminal_\n* _Once in terminal enter the following commands to clone the file to your desktop and open the repository:_\n```\n$ cd desktop\n$ git clone https://github.com/n-powell/titanic-data-viz\n$ cd desktop/titanic-data-viz\n$ npm install\n$ atom .\n$ touch src/app/api-keys.ts\n```\n* add project to Firebase https://firebase.google.com/\n* select 'add Firebase to your web app' to generate API info\n* in api-keys.ts write:\n```\n export var masterFirebaseConfig = {\n   apiKey: \"Your API info\",\n   authDomain: \"Your API info\",\n   databaseURL: \"Your API info\",\n   projectId: \"Your API info\",\n   storageBucket: \"Your API info\",\n   messagingSenderId: \"Your API info\"\n };\n ```\n ```\n$ ng serve\n$ open https://localhost:4200\n```\n\n# Specifications\n\n| Behavior | Input | Output |\n|----------|:-----:|:------:|\n| The application can display raw data | - | age: 22 |\n| The application can display manipulated data | - | age: 22 count: 6 |\n| | | |\n\n# User Stories\n* The site will allow the user to see a graph displaying the amount of passengers for each age\n* The site will allow the user to see a graph displaying the amount of passengers for each gender\n* The site will allow the user to see a graph displaying the ticket fare each passenger paid verses their age\n* The site will allow the user to see how many people survived on the graphs displaying passengers by gender and ticket fare by age\n\n## Known Bugs\n\n_No known bugs._\n\n## Support and contact details\n\n_Please comment on Github with any questions_\n\n## Technologies Used\n\n* HTML\n* CSS\n* D3 library v4\n* JavaScript\n* Angular 4\n* TypeScript\n* Node package manager\n* Firebase\n\n### License\n\n*This software is licensed under MIT license.*\n\nCopyright (c) 2017 **Niklas Long, James Higgins, Nick Powell, Sara Schultz**\n\n![Cluster](src/assets/cluster.png)\n![homepage logged in](src/assets/fare.png)\n"
 },
 {
  "repo": "saiabhishekgv/titanic_survival_exploration",
  "language": "HTML",
  "readme_contents": "## Project: Titanic Survival Exploration\n\n### Install\n\nThis project requires **Python 2.7** and the following Python libraries installed:\n\n- [NumPy](http://www.numpy.org/)\n- [Pandas](http://pandas.pydata.org)\n- [matplotlib](http://matplotlib.org/)\n- [scikit-learn](http://scikit-learn.org/stable/)\n\nYou will also need to have software installed to run and execute a [Jupyter Notebook](http://ipython.org/notebook.html)\n\nIf you do not have Python installed yet, it is highly recommended that you install the [Anaconda](http://continuum.io/downloads) distribution of Python, which already has the above packages and more included. Make sure that you select the Python 2.7 installer and not the Python 3.x installer.\n\n### Code\n\nTemplate code is provided in the notebook `titanic_survival_exploration.ipynb` notebook file. Additional supporting code can be found in `visuals.py`. While some code has already been implemented to get you started, you will need to implement additional functionality when requested to successfully complete the project. Note that the code included in `visuals.py` is meant to be used out-of-the-box and not intended for students to manipulate. If you are interested in how the visualizations are created in the notebook, please feel free to explore this Python file.\n\n### Run\n\nIn a terminal or command window, navigate to the top-level project directory `titanic_survival_exploration/` (that contains this README) and run one of the following commands:\n\n```bash\njupyter notebook titanic_survival_exploration.ipynb\n```\nor\n```bash\nipython notebook titanic_survival_exploration.ipynb\n```\n\nThis will open the Jupyter Notebook software and project file in your web browser.\n\n### Data\n\nThe dataset used in this project is included as `titanic_data.csv`. This dataset is provided by Udacity and contains the following attributes:\n\n**Features**\n- `pclass` : Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n- `name` : Name\n- `sex` : Sex\n- `age` : Age\n- `sibsp` : Number of Siblings/Spouses Aboard\n- `parch` : Number of Parents/Children Aboard\n- `ticket` : Ticket Number\n- `fare` : Passenger Fare\n- `cabin` : Cabin\n- `embarked` : Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n\n**Target Variable**\n- `survival` : Survival (0 = No; 1 = Yes)\n\n**Description**\n\nIn this introductory project, I have explored a subset of the RMS Titanic passenger manifest to determine which features best predict whether someone survived or did not survive. \nTo complete this project, I have implemented several conditional predictions.\nThe technique applied in this project is a manual implementation of a simple machine learning model, the decision tree.\nA decision tree splits a set of data into smaller and smaller groups (called nodes), by one feature at a time. Each time a subset of the data is split,\nour predictions become more accurate if each of the resulting subgroups are more homogeneous (contain similar labels) than before. \nThe advantage of having a computer do things for us is that it will be more exhaustive and more precise than our manual exploration above.\n\n## Conclusion \nAfter several iterations of exploring and conditioning on the data, you have built a useful algorithm for predicting the survival of each passenger aboard the RMS Titanic. \nThe technique applied in this project is a manual implementation of a simple machine learning model, the decision tree. \nA decision tree splits a set of data into smaller and smaller groups (called nodes), by one feature at a time. \nEach time a subset of the data is split, our predictions become more accurate if each of the resulting subgroups are more homogeneous (contain similar labels) than before. \nThe advantage of having a computer do things for us is that it will be more exhaustive and more precise than our manual exploration above. \nThis link provides another introduction into machine learning using a decision tree.\nA decision tree is just one of many models that come from supervised learning. In supervised learning, we attempt to use features of the data to predict or model things with objective outcome labels. \nThat is to say, each of our data points has a known outcome value, such as a categorical, discrete label like 'Survived', or a numerical, continuous value like predicting the price of a house."
 },
 {
  "repo": "wblakecannon/titanic-investigation",
  "language": "Jupyter Notebook",
  "readme_contents": "# Analysis of Titanic Passenger Data\n\nblake@caprinomics.com\nhttp://www.caprinomics.com/projects/titanic/\n\nIn this project, I analyze a dataset of Titanic passengers. I used a Jupyter Notebook with 2.7 Python. The external libraries used are NumPy, Pandas, Seaborn, and Matplotlib.\n\nThe Anaconda (https://www.anaconda.com) environment I used to create this is included in the repository (DAND.yml)\n\nThis study is an exercise to show how to use foundations of Data Science in order to import, study, visualize, and present the raw data in a method that is easy for any user to digest and understand.\n\nThis study uses passenger data from the ill-fated maiden voyage of the RMS Titanic (1912). The data (and explanation of the data) can be obtained from https://www.kaggle.com/c/titanic/data\n\nFirst, the raw comma separated values (.cvs) data will be loaded into a Python (Pandas) dataframe.\n\nSecond, there will be some data exploration. This will be completed mostly by loading plots of different data slices in order to better understand the data with visualization. Visualizing the data makes generating a hypothesis easier.\n\nThird, the data will be analyzed.\n\nLastly, a function has been created where a user can input their personal information to see their probability of surviving the Titanic disaster.\n\n## Files\n* DAND.yml (Anaconda/Python environment)\n* titanic-data.csv (the dataset used for the analysis from Kaggle)\n* titanic-investigation.html (html version of the analysis for passive viewing)\n* titanic-investigation.ipynb (Jupyter Notebook analysis to active viewing, use, and editing)\n\n"
 },
 {
  "repo": "kta/kaggle_titanic",
  "language": "Jupyter Notebook",
  "readme_contents": "# kaggle_titanic\nhttps://qiita.com/teru855/items/02bd885179bd8e39ba43\n"
 },
 {
  "repo": "PrzemekPobrotyn/Titanic-Kaggle",
  "language": "Jupyter Notebook",
  "readme_contents": "# Titanic-Kaggle\nA solution of Kaggle's Titanic competition (https://www.kaggle.com/c/titanic) using a number of classifiers and a stacking approach.\n\nIn the notebook we perform exploratory data analysis, feature engineering, fit a number of models and finally stack them with xgboost as 2nd level mode. \n\nRepository contains a jupyter notebook and two csv files with training and testing data.\n"
 },
 {
  "repo": "rutujar/kaggle-Titanic",
  "language": "Jupyter Notebook",
  "readme_contents": "### Kaggle-titanic\nThis is a tutorial in an IPython Notebook for the Kaggle competition, Titanic Machine Learning From Disaster. The goal of this repository is to provide an example of a competitive analysis for those interested in getting into the field of data analytics or using python for Kaggle's Data Science competitions .\n\n\n### Installation:\n\nTo run this notebook interactively:\n\n* Download this repository in a zip file by clicking on this [link](https://github.com/rutujar/kaggle-Titanic.git) \n\n#### Dependencies:\n* [NumPy](http://www.numpy.org/)\n* [IPython](http://ipython.org/)\n* [Pandas](http://pandas.pydata.org/)\n* [SciKit-Learn](http://scikit-learn.org/stable/)\n* [SciPy](http://www.scipy.org/)\n* [StatsModels](http://statsmodels.sourceforge.net/)\n* [Patsy](http://patsy.readthedocs.org/en/latest/)\n* [Matplotlib](http://matplotlib.org/)\n\n\n### Kaggle Competition | Titanic Machine Learning from Disaster\n\n>The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew.  This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\n>One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.  Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\n>In this contest, we ask you to complete the analysis of what sorts of people were likely to survive.  In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n\n>This Kaggle Getting Started Competition provides an ideal starting place for people who may not have a lot of experience in data science and machine learning.\"\n\nFrom the competition [homepage](http://www.kaggle.com/c/titanic-gettingStarted).\n\n### Goal for this Notebook:\nShow a simple example of an analysis of the Titanic disaster in Python using a full complement of PyData utilities. This is aimed for those looking to get into the field or those who are already in the field and looking to see an example of an analysis done with Python.\n\n#### This Notebook will show basic examples of:\n#### Data Handling\n*   Importing Data with Pandas\n*   Cleaning Data\n*   Exploring Data through Visualizations with Matplotlib\n\n#### Data Analysis\n*    Supervised Machine learning Techniques:\n    +   Logit Regression Model\n    +   Plotting results\n    +   Support Vector Machine (SVM) using 3 kernels\n    +   Basic Random Forest\n    +   Plotting results\n\n#### Valuation of the Analysis\n*   K-folds cross validation to valuate results locally\n*   Output the results from the IPython Notebook to Kaggle\n\n\n### Benchmark Scripts\nTo find the basic scripts for the competition benchmarks look in the \"titanic-solution\" folder. These scripts are based on the originals provided by Astro Dave but have been reworked so that they are easier to understand for new comers.\n\nCompetition Website: http://www.kaggle.com/c/titanic-gettingStarteddocs.org/en/latest/)\n* [Matplotlib](http://matplotlib.org/)\n\n### Kaggle Competition | Titanic Machine Learning from Disaster\n\n>The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew.  This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\n>One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.  Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\n>In this contest, we ask you to complete the analysis of what sorts of people were likely to survive.  In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n\n>This Kaggle Getting Started Competition provides an ideal starting place for people who may not have a lot of experience in data science and machine learning.\"\n\nFrom the competition [homepage](http://www.kaggle.com/c/titanic-gettingStarted).\n\n### Goal for this Notebook:\nShow a simple example of an analysis of the Titanic disaster in Python using a full complement of PyData utilities. This is aimed for those looking to get into the field or those who are already in the field and looking to see an example of an analysis done with Python.\n\n#### This Notebook will show basic examples of:\n#### Data Handling\n*   Importing Data with Pandas\n*   Cleaning Data\n*   Exploring Data through Visualizations with Matplotlib\n\n#### Data Analysis\n*    Supervised Machine learning Techniques:\n    +   Logit Regression Model\n    +   Plotting results\n    +   Support Vector Machine (SVM) using 3 kernels\n    +   Basic Random Forest\n    +   Plotting results\n\n#### Valuation of the Analysis\n*   K-folds cross validation to valuate results locally\n*   Output the results from the IPython Notebook to Kaggle\n\n\n### Benchmark Scripts\nTo find the basic scripts for the competition benchmarks look in the \"Python Examples\" folder. These scripts are based on the originals provided by Astro Dave but have been reworked so that they are easier to understand for new comers.\n\nCompetition Website: http://www.kaggle.com/c/titanic-gettingStarted\n"
 },
 {
  "repo": "upul/kaggle_titanic",
  "language": "Jupyter Notebook",
  "readme_contents": "Getting Started with Kaggle\n"
 },
 {
  "repo": "prachi1210/titanic-survival",
  "language": "Jupyter Notebook",
  "readme_contents": "# Machine Learning Project : Titanic Survival Exploration \nThis project contains create decision functions that attempt to predict survival outcomes from the 1912 Titanic disaster based on each passenger\u2019s features, such as sex, age, sibsp, embarrked and cabin class which accurately predict the outcomes for at least 80% of the passengers in the provided data.\n\n[View features of dataset on Kaggle](https://www.kaggle.com/c/titanic/data)\n\n**Software and Libraries**\n\nThis project uses the following software and Python libraries:\n\n- Python 2.7\n- NumPy\n- pandas\n- matplotlib\n- iPython Notebook\n\n**Introduction**:\n\nIn 1912, the ship RMS Titanic struck an iceberg on its maiden voyage and sank, resulting in the deaths of most of its passengers and crew. This project explores a subset of the RMS Titanic passenger manifest to determine which features best predict whether someone survived or did not survive.\n\nFrom a sample of the RMS Titanic data, we can see the various features present for each passenger on the ship:\n- **Survived**: Outcome of survival (0 = No; 1 = Yes)\n- **Pclass**: Socio-economic class (1 = Upper class; 2 = Middle class; 3 = Lower class)\n- **Name**: Name of passenger\n- **Sex**: Sex of the passenger\n- **Age**: Age of the passenger (Some entries contain `NaN`)\n- **SibSp**: Number of siblings and spouses of the passenger aboard\n- **Parch**: Number of parents and children of the passenger aboard\n- **Ticket**: Ticket number of the passenger\n- **Fare**: Fare paid by the passenger\n- **Cabin** Cabin number of the passenger (Some entries contain `NaN`)\n- **Embarked**: Port of embarkation of the passenger (C = Cherbourg; Q = Queenstown; S = Southampton)\n\nSince we're interested in the outcome of survival for each passenger or crew member, we can remove the **Survived** feature from this dataset and store it as its own separate variable `outcomes`. We will use these outcomes as our prediction targets. \n\n**Decision Functions**\n- Check for the sex of survived passengers\n```python\nsurvival_stats(data, outcomes, 'Sex')\n```\n![png](extras/output_13_0.png)\n- Check for the age of survived passengers\n```python\nsurvival_stats(data, outcomes, 'Age', [\"Sex == 'male'\"])\n```\n![png](extras/output_20_0.png)\n- Building a more complex decision tree using multiple filter conditions\n```python\nsurvival_stats(data, outcomes, 'SibSp', [\"Sex == 'female'\" or \"Age <15 \" and \"Pclass=3\" and \"Embarked != 'S' \"])\n```\n![png](extras/output_27_0.png)\n\n**Accuracy**\n```python\nprint accuracy_score(outcomes, predictions)\n```\nPredictions have an accuracy of **82.49%**\n"
 },
 {
  "repo": "KirovVerst/tensorflow",
  "language": "Python",
  "readme_contents": "# tensorflow\nTensorflow model for Kaggle Titanic Competition\n[![Screen Shot 2016-12-05 at 19.46.38.png](https://s17.postimg.org/ccj0vyhkf/Screen_Shot_2016_12_05_at_19_46_38.png)](https://postimg.org/image/mmlfv77fv/)\n## Requirements\nPython 3.5.2\n```\n$ pip install -r requirements.txt\n```\n## Run tensorboard\n```\n$ tensorboard --logdir=tensorflow_logs  \n```\n## Kaggle competition\n[Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic)\n\n## Docker container on Docker Hub\nhttps://hub.docker.com/r/kirovverst/tensorflow/\n"
 },
 {
  "repo": "navaneethbv/Kaggle_Titanic-Survivors",
  "language": "R",
  "readme_contents": "# Kaggle_Titanic-Survivors\nPractice of Kaggle's Titanic Survivors Challenge using R\n\nCredit goes to David Langer's Video on Youtube.\n"
 },
 {
  "repo": "rahuljadli/Titanic-Survival-Prediction",
  "language": "Jupyter Notebook",
  "readme_contents": "# Titanic Survival Prediction\n\n\n## Imported all the required library\n```\nimport pandas as pd\nimport numpy as np\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\n\n```\n### Loading and Viewing the data\n\n~~~\ndata=pd.read_csv('titanic_data.csv')\ndata.head()\n~~~\n# Data Visualisation\n\n### Ploting the survival rate\n\n![alt Survived](https://github.com/rahuljadli/Titanic-Survival-Prediction/blob/master/screen_shots/Survival.png)\n\n### Ploting Age graph\n\n![alt Age](https://github.com/rahuljadli/Titanic-Survival-Prediction/blob/master/screen_shots/Age.png)\n\n### Ploting Survival Based on Sex\n\n![alt Sex Survival ](https://github.com/rahuljadli/Titanic-Survival-Prediction/blob/master/screen_shots/Survival_gender.png)\n\n### Ploting Survival Based on Passenger Class\n\n![alt Sex Survival ](https://github.com/rahuljadli/Titanic-Survival-Prediction/blob/master/screen_shots/Survival_Pclass.png)\n\n### Ploting Survival Based on Sibling\n\n![alt Sex Survival ](https://github.com/rahuljadli/Titanic-Survival-Prediction/blob/master/screen_shots/Survival_sibling.png)\n\n### Ploting Survival Based on Parents\n\n![alt Sex Survival ](https://github.com/rahuljadli/Titanic-Survival-Prediction/blob/master/screen_shots/Survival_parch.png)\n\n## Data filling\n\n~~~\nclean_test.Age = clean_test.Age.fillna(titanic_data['Age'].mean())\ntesting_data.Fare=testing_data.Fare.fillna(data.Fare.mean())\n~~~\n\n# Using Different Model's \n\n## Creating Training and Testing Data set\n\n~~~\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=42)\n\n~~~\n# Training the model\n\n~~~\nmodel=LogisticRegression()\nmodel.fit(x_train,y_train)\n~~~\n# Making the prediction\n\n~~~\nnew_prediction=model.predict(testing_data)\n~~~\n## Getting the accuracy score\n\n~~~\nfrom sklearn.metrics import accuracy_score\n\n\nacc_logreg = round(accuracy_score(prediction, y_test) * 100, 2)\nprint(acc_logreg)\n~~~\n"
 },
 {
  "repo": "gauravmodi/Titanic-Flask-App",
  "language": "Jupyter Notebook",
  "readme_contents": "# Flask app for Titanic: Machine Learning from Disaster\n**- Gaurav Modi**<br>\n_(last edit: 04/15/2018)_\n<p>This is a flask app to predict whether a particular passenger would survive. Along with the label, it shows the Survival probability too.\n\n## Packages Used:\n```\n+ Json\n+ Pandas\n+ Numpy\n+ Flask\n+ WTForms\n+ Scikit-Learn\n```\n\n## Steps:\n### Data preperation and model building\n+ Reading data\n+ Splitting the data into training and testing data\n+ Imputing missing values from training dataset.\n+ Using values imputed from training data to fill missing values in testing dataset to avoid data leakage.\n+ Encoding Categorical predictors using sklearn LabelEncoder\n+ Fitting RandomForestClassifier with Training data\n+ Serializing the model to disk\n\n### Prediction on flask app\n```\n+ Navigate to prediction page - http://localhost:5000/predict.\n+ User fill the form in the browser with required predictors value and then submit the form.\n+ Values provided by users are put into model to get predictions.\n+ The result from the model is showed to user on browser.\n```\n\n## To run the app\n+ Open terminal in main directory '/Titanic Flask App'.\n+ Then run app.py from terminal\n\n```\npython app.py\n```\nThis will start flask app server. Now can open the site by opening http://localhost:5000<br>\nNote: Right now, app runs in debug mode\n\n## Future work\n+ Improve the model\n+ Provide drop down for user input\n+ Take user input as string for categorical data, instead of current method i.e. encoded numeric values.\n+ Improve the UI and UX."
 },
 {
  "repo": "martinbaroody77/Titanic-Survival-Predictor-App",
  "language": "R",
  "readme_contents": "A web app made using R and Shiny that allows the user to customize a person using Shiny widgets. The app can predict whether this person would have survived the titanic disaster using machine learning. \n\nNOTE: requires \"shiny\", \"mice\", and \"ggplot2\" libraries to be installed. Use install.packages in the R console to install them.\n\nI used logistic regression to train the model and make the predictions. \n\nTo run the app, type store these files in a directory called \"Titanic-App\", use the setwd() function in the R console to change the working directory to the folder containing \"Titanic-App\", type library(shiny), then type runApp(\"Titanic-App\").\n"
 },
 {
  "repo": "tabblogs/TableauPrepPythonTitanic",
  "language": "Jupyter Notebook",
  "readme_contents": "# TableauPrepPythonTitanic\nThe classic Titanic data science problem solved using Tableau Prep running Python scripts.\n"
 },
 {
  "repo": "jzpang/Kaggle-Titanic-HadoopKNN-xgboost",
  "language": "Jupyter Notebook",
  "readme_contents": "# Kaggle-Titanic-HadoopKNN-xgboost\n\nIntroduction\n\nThis project contains three parts:\n1. performed feature engineering on Kaggle Titanic dataset\n2. built up a KNN Classifier on Hadoop Mapreduce from scratch\n3. built up Gradient Bossted Trees Classifier with xgboost, grid search for parameter optimizing was performed for model improvement\n\n\nRun KNN classifier:\n1. First run fe.ipynb on Jupyter notebook for feature engineering\n2. hadoop jar KNNClassifier.jar KNNDriver [Training_Data_Path] [Test_Data_Path] [Output_Path] [k] [continuous_feature_index]\nNote: where continuous_feature_index is a list of feature index that you want to use as continuous variables. It there is no continuous variables, input \u201cNULL\u201d.\n      [k] is the number of neiborhoods used in KNN\n3. python cal.py gender_submission.csv [your_prediction_file] \nAnalyze your prediction and the true lables for accuracy and confusion matrix, etc\nNote: [your_prediction_file] is a part-r-00000 file\n\n"
 }
]